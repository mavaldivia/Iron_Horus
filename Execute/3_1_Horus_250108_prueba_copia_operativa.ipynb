{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este es el formal (ocupo solo enero 2025 por ahora) pero aquí se está consruyendo el modelo de optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 240108: inclusion de modo_tactico, y opción de ejecución con periodo mensual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opción de particionar DATE en Semana o Mes (aún no, primero integar las restricciones propias del escenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mip\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t_0 = time.time()\n",
    "t_inicio_0 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit('En df_R3c, en rango 1, tengo A > 0...eso no puede ser...A = 0, B >= 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "escenario = 'Base MX' #'Base MX operativo'\n",
    "modo_tactico = True\n",
    "\n",
    "refrescar_lectura = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'PC'\n",
    "# paises = ['MX'] #['UY', 'CL', 'MX', 'CO', 'BR', 'AR'] # Paises a considerar ['CL']\n",
    "# modo_proyeccion = 'LIBRE' #['PRESUPUESTO', 'LIBRE'] #['LIBRE', 'PRESUPUESTO']\n",
    "ajuste_metricas = True # Ajuste a versión de presupuesto (True)\n",
    "\n",
    "grano = 'F-LT'\n",
    "lista_duplicidades = ['F-LT', 'F', 'LT', 'TOTALES']  #, 'F-SF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "escribir_modelo = True, 9999\n",
    "activar_holguras = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "campo_last_touch = 'TIPO_MEDIO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_simplificada = False # Activar para buscar factibilidad. Elimina las restricciones: 8 - Todas  las restricciones propias del escenario\n",
    "# Además, permite un solo rango de inversión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones Transversales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carpeta_modulos = '../../Modulos/'  #/home/data/python/performance_automation/Modulos\n",
    "if mode == 'VM': # Esto tiene que ir en funcion generar dag\n",
    "    carpeta_modulos = '/home/data/python/performance_automation/Modulos/'\n",
    "\n",
    "sys.path.insert(0, carpeta_modulos)\n",
    "import funciones_transversales as tf\n",
    "client = tf.generar_cliente()\n",
    "\n",
    "dir_histogramas = '../Data/histogramas/'\n",
    "carpeta_input = '../Inputs/'\n",
    "dir_data = '../Data/'\n",
    "actualizar = False\n",
    "\n",
    "df_dias_eventos = tf.generar_calendario_eventos_new(actualizar = actualizar, carpeta_input = carpeta_input)\n",
    "df_dias_eventos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times = pd.DataFrame()\n",
    "t0 = time.time()\n",
    "\n",
    "def medir_tiempo(t0, texto, df_times, output = True):\n",
    "    \n",
    "    if output:\n",
    "        print(f'\\n\\n Medición de tiempo en {texto} \\n\\n\\n\\n')\n",
    "    t = (time.time() - t0) / 60\n",
    "    df = pd.DataFrame({'texto': [texto], 'tiempo': [t]})\n",
    "    df_times = pd.concat([df_times, df])\n",
    "    t0 = time.time()\n",
    "    return df_times, t0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Escenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_escenarios = tf.lectura_segura_Gsheets('Horus Escenarios', 'Escenarios', encabezado = 1, refrescar_lectura = refrescar_lectura)\n",
    "df_escenarios = df_escenarios[df_escenarios['ESCENARIO'] == escenario].reset_index(drop = True)\n",
    "display(df_escenarios)\n",
    "\n",
    "df_cols = tf.lectura_segura_Gsheets('Horus Escenarios', 'cols')\n",
    "df_escenarios.columns = df_cols.columns\n",
    "\n",
    "df_escenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medios = tf.lectura_segura_Gsheets('clacomizacion_corp_v0', 'DICCIONARIO_MEDIOS_CORP', refrescar_lectura = refrescar_lectura)\n",
    "#df_medios = df_medios[['TIPO_MEDIO', 'NATURALEZA_MEDIO']].drop_duplicates().reset_index(drop = True)\n",
    "df_medios['NATURALEZA_MEDIO'] = np.where(df_medios['NATURALEZA_MEDIO'] == 'Pago', 'PAGO', 'ORGANICO')\n",
    "df_medios = df_medios.drop(columns = 'LAST_TOUCH_CHANNEL')\n",
    "df_medios = df_medios.rename(columns = {'LAST_TOUCH_CHANNEL_CORP': 'LAST_TOUCH_CHANNEL'})\n",
    "df_medios = df_medios.drop_duplicates().reset_index(drop = True)\n",
    "df_medios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_medios = df_medios.sort_values(campo_last_touch).reset_index(drop = True)\n",
    "df_medios_org, df_medios_pago = df_medios[df_medios['NATURALEZA_MEDIO'] == 'ORGANICO'].reset_index(drop = True), df_medios[df_medios['NATURALEZA_MEDIO'] == 'PAGO'].reset_index(drop = True)\n",
    "\n",
    "lista_org, lista_pago = list(df_medios_org[campo_last_touch].unique()), list(df_medios_pago[campo_last_touch].unique())\n",
    "print(lista_org), print(len(lista_org))\n",
    "print(lista_pago), print(len(lista_pago))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_configuracion = df_escenarios[df_escenarios['TIPO'] == 'C'].reset_index(drop = True)\n",
    "df_configuracion\n",
    "\n",
    "k = 1\n",
    "# Temporalidad por defecto: Año actual + k Próximos años (Se puede cambiar)\n",
    "dias_proyeccion = [dt.datetime.today().replace(year = dt.datetime.today().year, month = 1, day = 1).date(), dt.datetime.today().replace(year = dt.datetime.today().year + k, month = 12, day = 31).date()] # declarar arriba, al inicio del código\n",
    "#dias_proyeccion = [pd.to_datetime(dias_proyeccion[0]), pd.to_datetime(dias_proyeccion[1])]\n",
    "\n",
    "paises = ['MX', 'CL', 'UY', 'CO', 'BR', 'AR', 'PE'] # Por defecto\n",
    "familias_ids = ['*'] # Todo\n",
    "last_touch_lista = ['*'] #lista_pago[:1] #+ lista_org[:1] #['SEM - Shopping', 'Typed', 'Afliliados', 'Internal', 'SEM - Non Brand', 'SEM - Brand'] # # Todo\n",
    "lista_fuentes, lista_canales = ['SODIMAC', 'ES', 'SIS'], ['WEB', 'APP']\n",
    "\n",
    "for c in df_configuracion.columns:\n",
    "    if 'Base + Config' in c:\n",
    "        name = '_'.join(c.split('_')[1:])\n",
    "        value = df_configuracion[c][0]\n",
    "        if name == 'MODO_PROYECCION': # Modo proyección\n",
    "            modo_proyeccion = value\n",
    "            if modo_proyeccion not in ['PRESUPUESTO', 'LIBRE']:\n",
    "                sys.exit(f'MODO_PROYECCION {modo_proyeccion} no es correcto')\n",
    "        \n",
    "        if (name == 'PAIS') and (value != ''):\n",
    "            paises = value.split(',')\n",
    "                        \n",
    "        if (name == 'DATE') and (value != ''):\n",
    "            dias_proyeccion_new = pd.to_datetime(value.split('>')[0]).date(), pd.to_datetime(value.split('>')[1]).date()\n",
    "            dias_proyeccion[0] = max(dias_proyeccion[0], dias_proyeccion_new[0])\n",
    "            dias_proyeccion[1] = min(dias_proyeccion[1], dias_proyeccion_new[1])\n",
    "        \n",
    "        if (name == 'PERIODO') and (value != ''):\n",
    "            periodo_0, periodo_1 = value.split('>')\n",
    "            dia0 = dt.datetime(int(periodo_0.split('-')[0]), int(periodo_0.split('-')[1]), 1).date()\n",
    "            dia1 = dt.datetime(int(periodo_1.split('-')[0]), int(periodo_1.split('-')[1]), 1).date()\n",
    "            dia1 = (dt.datetime(dia1.year, dia1.month, 1) + pd.DateOffset(months = 1) - pd.DateOffset(days = 1)).date()\n",
    "            dias_proyeccion[0] = max(dias_proyeccion[0], dia0)\n",
    "            dias_proyeccion[1] = min(dias_proyeccion[1], dia1)\n",
    "        \n",
    "        if (name == 'AÑO') and (value != ''):\n",
    "            year_0, year_1 = value.split('>')\n",
    "            dia0 = dt.datetime(int(year_0), 1, 1).date()\n",
    "            dia1 = dt.datetime(int(year_1), 12, 31).date()\n",
    "            dias_proyeccion[0] = max(dias_proyeccion[0], dia0)\n",
    "            dias_proyeccion[1] = min(dias_proyeccion[1], dia1)\n",
    "        \n",
    "        if (name == 'FAMILIA') and (value != ''):\n",
    "            familias_lista = value.split(',')\n",
    "            familias_ids = [int(f) for f in familias_lista]\n",
    "        \n",
    "        if (name == campo_last_touch) and (value != ''):\n",
    "            last_touch_lista = value.split(',')\n",
    "        \n",
    "        if (name == 'FUENTE') and (value != ''):\n",
    "            lista_fuentes = value.split(',')\n",
    "        \n",
    "        if (name == 'CANAL') and (value != ''):\n",
    "            lista_canales = value\n",
    "        \n",
    "        if (name == 'CANAL_BASE') and (value != ''):\n",
    "            sys.exit('CANAL_BASE en configuración debe ir vacío')\n",
    "            \n",
    "\n",
    "dias_proyeccion = [pd.to_datetime(dias_proyeccion[0]), pd.to_datetime(dias_proyeccion[1])]\n",
    "dias_proyeccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lectura de Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ias_proyeccion = [dt.datetime.today().replace(year = dt.datetime.today().year, month = 1, day = 1).date(), dt.datetime.today().replace(year = dt.datetime.today().year + 1, month = 12, day = 31).date()] # declarar arriba, al inicio del código\n",
    "dias_proyeccion = [pd.to_datetime(dias_proyeccion[0]), pd.to_datetime(dias_proyeccion[1])]\n",
    "dias_proyeccion # Desde el primer día del año actual, hasta el último día del próximo año (por defecto)\n",
    "\n",
    "print('Small Batch [un mes](eliminar)')\n",
    "print('Small Batch [un día](eliminar)')\n",
    "dias_proyeccion = [dt.datetime(2025, 1, 1).date(), dt.datetime(2025, 2, 28).date()] # declarar arriba, al inicio del código\n",
    "dias_proyeccion = [pd.to_datetime(dias_proyeccion[0]), pd.to_datetime(dias_proyeccion[1])]\n",
    "dias_proyeccion\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Conjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_dimensiones = {'TOTALES': '', 'F': 'FAMILIA', 'LT': campo_last_touch, 'SF': 'SUBFAMILIA', 'F1': 'F1', 'G1': 'G1'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campos_grano = []\n",
    "for d in grano.split('-'):\n",
    "    campos_grano.append(diccionario_dimensiones[d])\n",
    "campos_grano # campos asociados al grano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P2 = tf.parquet_act(f'{dir_data}Parametros/P2')\n",
    "df_P2['DATE'] = pd.to_datetime(df_P2['DATE'])\n",
    "df_P2 = df_P2[df_P2['PAIS'].isin(paises)]\n",
    "df_alpha_grano_pago = df_P2[campos_grano].drop_duplicates()\n",
    "df_alpha_grano_pago = df_alpha_grano_pago.sort_values(campos_grano).reset_index(drop = True)\n",
    "df_alpha_grano_pago = df_alpha_grano_pago[df_alpha_grano_pago[campos_grano].notna().all(axis = 1)].reset_index(drop = True) # alpha_tongo (alpha_grano)\n",
    "df_alpha_grano_pago # alpha_tongo (alpha_grano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_familias = df_alpha_grano_pago['FAMILIA'].unique()\n",
    "\n",
    "familia_sm = ['']\n",
    "for f in lista_familias:\n",
    "    \n",
    "    if '*' in familias_ids:\n",
    "        familia_sm = lista_familias[:]\n",
    "        break\n",
    "    \n",
    "    for s in familias_ids:\n",
    "        formatted_number = str(s).zfill(2)\n",
    "        if formatted_number in f:\n",
    "            familia_sm.append(f)\n",
    "            continue\n",
    "\n",
    "familia_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_LT_pago = df_alpha_grano_pago[campo_last_touch].unique()\n",
    "\n",
    "if '*' in last_touch_lista:\n",
    "    last_touch_lista_pago = lista_LT_pago\n",
    "else:\n",
    "    last_touch_lista_pago = []\n",
    "    for lt in lista_LT_pago:\n",
    "        if lt in last_touch_lista:\n",
    "            last_touch_lista_pago.append(lt)\n",
    "\n",
    "last_touch_lista_pago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_grano_pago = df_alpha_grano_pago[df_alpha_grano_pago['FAMILIA'].isin(familia_sm)].reset_index(drop = True) # Reducción de df_alpha_grano_pago\n",
    "df_alpha_grano_pago = df_alpha_grano_pago[df_alpha_grano_pago['TIPO_MEDIO'].isin(last_touch_lista_pago)].reset_index(drop = True)\n",
    "tm_seleccion = list(df_alpha_grano_pago['TIPO_MEDIO'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_P3 = tf.parquet_act(f'{dir_data}Parametros/P3')\n",
    "df_P3['DATE'] = pd.to_datetime(df_P3['DATE'])\n",
    "df_P3 = df_P3[df_P3['PAIS'].isin(paises)]\n",
    "df_alpha_grano_organico = df_P3[campos_grano].drop_duplicates()\n",
    "df_alpha_grano_organico = df_alpha_grano_organico.sort_values(campos_grano).reset_index(drop = True)\n",
    "df_alpha_grano_organico = df_alpha_grano_organico[df_alpha_grano_organico[campos_grano].notna().all(axis = 1)].reset_index(drop = True) # alpha_tongo (alpha_grano)\n",
    "df_alpha_grano_organico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_grano_organico.FAMILIA.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_LT_organico = df_alpha_grano_organico[campo_last_touch].unique()\n",
    "\n",
    "if '*' in last_touch_lista:\n",
    "    last_touch_lista_organico = lista_LT_organico\n",
    "else:\n",
    "    last_touch_lista_organico = []\n",
    "    for lt in lista_LT_organico:\n",
    "        if lt in last_touch_lista:\n",
    "            last_touch_lista_organico.append(lt)\n",
    "\n",
    "last_touch_lista_organico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_grano_organico = df_alpha_grano_organico[df_alpha_grano_organico['FAMILIA'].isin(familia_sm)].reset_index(drop = True)\n",
    "df_alpha_grano_organico = df_alpha_grano_organico[df_alpha_grano_organico['TIPO_MEDIO'].isin(last_touch_lista_organico)].reset_index(drop = True)\n",
    "tm_seleccion += list(df_alpha_grano_organico['TIPO_MEDIO'].unique())\n",
    "print(tm_seleccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta = df_P2[['PAIS', 'DATE', 'CANAL', 'FUENTE']].drop_duplicates()\n",
    "\n",
    "# Filtros\n",
    "\n",
    "df_beta = df_beta[df_beta['PAIS'].isin(paises)]\n",
    "df_beta = df_beta[(df_beta['DATE'] >= dias_proyeccion[0]) & (df_beta['DATE'] <= dias_proyeccion[1])]\n",
    "df_beta = df_beta[df_beta['FUENTE'].isin(lista_fuentes)]\n",
    "df_beta = df_beta[df_beta['CANAL'].isin(lista_canales)]\n",
    "df_beta = df_beta.sort_values(['PAIS', 'DATE', 'CANAL', 'FUENTE']).reset_index(drop = True)\n",
    "df_beta # beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, '1.1 Conjuntos', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Data Histórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_gcp(lista):\n",
    "    s = '('\n",
    "    for i in lista:\n",
    "        s += f'\"{i}\",'\n",
    "    s = s[:-1] + ')'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_df_duplicidades(tf, client, tabla_data, duplicacion, paises, dia_desde, dia_hasta, diccionario_dimensiones, dia_0, df_factores_PE, co_sin_fcom = True):\n",
    "    \n",
    "    if dia_desde >= str(dt.datetime.today().date()): # >= hoy\n",
    "        return pd.DataFrame()\n",
    "    # CO sin FCOM\n",
    "    str_co = ''\n",
    "    if co_sin_fcom:\n",
    "        str_co = 'and not (PAIS = \"CO\" and FUENTE != \"SODIMAC\")'\n",
    "    \n",
    "    # otras dimensiones (que dependen de la duplicacion)\n",
    "    \n",
    "    str_dimensiones, str_numeros = 'PAIS, DATE, CANAL, FUENTE, ', '1, 2, 3, 4, '\n",
    "    for i, elemento in enumerate(duplicacion.split('-')):\n",
    "        if elemento == 'TOTALES':\n",
    "            break\n",
    "        str_dimensiones += diccionario_dimensiones[elemento] + ', '\n",
    "        str_numeros += f'{i + 5}, '\n",
    "    \n",
    "    str_dimensiones, str_numeros = str_dimensiones[:-2], str_numeros[:-2]\n",
    "    \n",
    "    \"\"\"\n",
    "    ejecutar_PE = False\n",
    "    if 'PE' in paises:\n",
    "        ejecutar_PE = True\n",
    "    \"\"\"\n",
    "        \n",
    "    #paises = list(set(paises) - {'PE'}) # paises sin PE, mientras que está parchada APP SODIMAC PE\n",
    "\n",
    "    # paises          \n",
    "    paises = list_to_gcp(paises)      \n",
    "    \n",
    "    # query   \n",
    "    # dia_0 se ocupa por defecto, en caso de que no exista df_representantes y quiera actualizarse por completo  \n",
    "    \"\"\"  \n",
    "    if ejecutar_PE:\n",
    "        \n",
    "        query = f'SELECT {str_dimensiones}, SUM(VENTA_COLOCADA) AS VENTA_COLOCADA, SUM(ORDENES) AS ORDENES, SUM(VISITAS) AS VISITAS FROM `{tabla_data}_{duplicacion}` where PAIS in {paises} {str_co} and FUENTE_DATOS = \"REAL\" and DATE >= \"{dia_0}\" and DATE >= \"{dia_desde}\" and DATE <= \"{dia_hasta}\" group by {str_numeros} order by 1, 2' # main request para entrenamiento\n",
    "        print(query)\n",
    "        \n",
    "        dia_hasta = pd.to_datetime(str(dia_hasta)[:10]).date()\n",
    "        dia_desde_definitivo = max(pd.to_datetime(str(dia_0)[:10]), pd.to_datetime(str(dia_desde)[:10])).date()\n",
    "        print('desde, hasta', dia_desde_definitivo, dia_hasta)\n",
    "        \n",
    "        if ((dt.datetime(2024, 5, 8).date() >= dia_desde_definitivo) and (dt.datetime(2024, 5, 8).date() <= dia_hasta)) or ((dt.datetime(2024, 6, 8).date() >= dia_desde_definitivo) and (dt.datetime(2024, 6, 8).date() <= dia_hasta)):\n",
    "            \n",
    "        #sys.exit('desarrollar request para PE')\n",
    "    \"\"\"\n",
    "                                                                                                                                                                                                                                                                    \n",
    "    query = f'SELECT {str_dimensiones}, SUM(VENTA_COLOCADA) AS VENTA_COLOCADA, SUM(ORDENES) AS ORDENES, SUM(VISITAS) AS VISITAS, SUM(INVERSION) AS INVERSION FROM `{tabla_data}_{duplicacion}` where PAIS in {paises} {str_co} and FUENTE_DATOS = \"REAL\" and DATE >= \"{dia_0}\" and DATE >= \"{dia_desde}\" and DATE <= \"{dia_hasta}\" group by {str_numeros} order by 1, 2' # main request para entrenamiento\n",
    "    \n",
    "    # request y df\n",
    "    df =  tf.request_GCP_vnew(\n",
    "            nombre_tabla = \"\",\n",
    "            specific_query = query,\n",
    "            client = client,\n",
    "            output = True, permitir_fallos = False)\n",
    "    \n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    \n",
    "    # Corrección con factores PE cuando corresponde\n",
    "    df = df.merge(df_factores_PE, on = ['PAIS', 'DATE', 'CANAL', 'FUENTE'], how = 'left')\n",
    "    df['FACTOR'] = df['FACTOR'].fillna(1)\n",
    "    df['VENTA_COLOCADA'] = df['VENTA_COLOCADA'] * df['FACTOR']\n",
    "    df = df.drop(columns = ['FACTOR'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_back = f'{dir_data}Back/'\n",
    "print(dir_data_back) # Donde almacenar\n",
    "\n",
    "df_representantes_base = pd.DataFrame(columns = ['DUPLICACION', 'PAIS', 'DATE'])\n",
    "if 'Horus_data.pkl' in os.listdir(dir_data_back):\n",
    "    print('SI')\n",
    "    df_representantes_base = tf.parquet_act(f'{dir_data_back}Horus_data')\n",
    "df_representantes_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_representantes = pd.DataFrame({'REPRESENTANTE': lista_duplicidades})\n",
    "df_representantes['DIA_DESDE'], df_representantes['DIA_HASTA'] = str(dias_proyeccion[0])[:10], str(dias_proyeccion[1])[:10]\n",
    "\n",
    "df_paises = pd.DataFrame({'PAIS': paises})\n",
    "df_paises['AUX'] = 'aux'\n",
    "\n",
    "co_sin_fcom = True\n",
    "paises_no_andes = ['BR', 'MX', 'AR', 'UY']\n",
    "paises_andes = ['CL', 'PE']\n",
    "if co_sin_fcom:\n",
    "    paises_no_andes.append('CO')\n",
    "\n",
    "df_representantes['AUX'] = 'aux'\n",
    "df_representantes = df_representantes.merge(df_paises, on = 'AUX')\n",
    "df_representantes = df_representantes.drop(columns = ['AUX'])\n",
    "df_representantes = df_representantes[['PAIS', 'REPRESENTANTE', 'DIA_DESDE', 'DIA_HASTA']]\n",
    "df_representantes = df_representantes[~((df_representantes['PAIS'].isin(paises_no_andes)) & (df_representantes['REPRESENTANTE'].isin(['F1', 'G1'])))].reset_index(drop = True)\n",
    "df_representantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "actualizar_data_historica = False # Parámetro\n",
    "tabla_data = 'sod-corp-plp-beta.ETL_main_2023.Nexus_Main_230829_0_v10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_factores_PE = tf.lectura_segura_Gsheets('Factor_correccion_app_PE', 'Factores', refrescar_lectura = refrescar_lectura)\n",
    "df_factores_PE['DATE'] = pd.to_datetime(df_factores_PE['DATE'])\n",
    "df_factores_PE['FACTOR'] = df_factores_PE['FACTOR'].astype(float)\n",
    "df_factores_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_representantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df_representantes_base.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alguna_ejecucion = False\n",
    "\n",
    "for i in range(len(df_representantes)):\n",
    "    representante, dia_desde, dia_hasta, pais = df_representantes.loc[i, 'REPRESENTANTE'], df_representantes.loc[i, 'DIA_DESDE'], df_representantes.loc[i, 'DIA_HASTA'], df_representantes.loc[i, 'PAIS']\n",
    "    print(representante, dia_desde, dia_hasta, pais)\n",
    "    \n",
    "    df_dates = pd.date_range(start = dia_desde, end = dia_hasta, freq = 'D')\n",
    "    df_dates = pd.DataFrame({'DATE': df_dates})\n",
    "    df_dates['AUX'] = 'aux'\n",
    "    \n",
    "    df_pais = pd.DataFrame({'PAIS': [pais]})\n",
    "    df_pais['AUX'] = 'aux'\n",
    "    \n",
    "    df_req = df_dates.merge(df_pais, on = 'AUX')\n",
    "    df_req = df_req.drop(columns = ['AUX'])\n",
    "    df_req['DUPLICACION'] = representante\n",
    "\n",
    "    if not actualizar_data_historica:\n",
    "        df_existente = df_representantes_base[['DUPLICACION', 'PAIS', 'DATE']].drop_duplicates().reset_index(drop = True)\n",
    "        df_existente['EXISTE'] = True\n",
    "        \n",
    "        df_req = df_req.merge(df_existente, on = ['DUPLICACION', 'PAIS', 'DATE'], how = 'left')\n",
    "        df_req['EXISTE'] = df_req['EXISTE'].fillna(False)\n",
    "    else:\n",
    "        df_req['EXISTE'] = False\n",
    "        \n",
    "    df_req = df_req[~df_req['EXISTE']]\n",
    "\n",
    "    if len(df_req) == 0: # Están todos los datos actualizados\n",
    "        continue\n",
    "        \n",
    "    min_date, max_date, paises_ejecucion = str(df_req['DATE'].min())[:10], str(df_req['DATE'].max())[:10], df_req['PAIS'].unique()\n",
    "    \n",
    "    df_ejecucion = request_df_duplicidades(tf, client, tabla_data, representante, paises_ejecucion, min_date, max_date, diccionario_dimensiones, dia_desde, df_factores_PE)\n",
    "    \n",
    "    if len(df_ejecucion) > 0:\n",
    "        alguna_ejecucion = True\n",
    "    df_ejecucion['DUPLICACION'] = representante\n",
    "    #display(df_ejecucion)\n",
    "    df_representantes_base = pd.concat([df_representantes_base, df_ejecucion])\n",
    "    df_representantes_base['DATE'] = pd.to_datetime(df_representantes_base['DATE'])\n",
    "    df_representantes_base = df_representantes_base.drop_duplicates(subset = list(set(df_representantes_base.columns) - {'VENTA_COLOCADA', 'ORDENES', 'VISITAS'}), keep = 'last').reset_index(drop = True) # Se mantienen con prioridad los nuevos datos\n",
    "\n",
    "if alguna_ejecucion:\n",
    "    df_representantes_base = df_representantes_base.fillna(\"\")\n",
    "    tf.parquet_act(f'{dir_data_back}Horus_data', variable = df_representantes_base, mode = 'save') # Respaldo de la nueva data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, '1.2 Data Hist', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura general de parámetros\n",
    "for archivo in os.listdir(f'{dir_data}Parametros/'):\n",
    "    break # Por ahora, esto es solo revisión\n",
    "    print(archivo)\n",
    "    df = tf.parquet_act(f'{dir_data}Parametros/{archivo[:-4]}')\n",
    "    display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las metas están disponibles en los archivos\n",
    "dir_metas = '../Data/metas/'\n",
    "\n",
    "for archivo in os.listdir(dir_metas):\n",
    "    print(archivo)\n",
    "    df = tf.parquet_act(f'{dir_metas}{archivo[:-4]}')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "campos_beta = ['PAIS', 'DATE', 'CANAL', 'FUENTE']\n",
    "dic_parametros = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campos_beta_def = campos_beta\n",
    "if modo_tactico:\n",
    "    campos_beta_def = campos_beta + ['PERIODO']\n",
    "    campos_beta_def.remove('DATE')\n",
    "\n",
    "\n",
    "df_beta_def = df_beta.copy()\n",
    "if modo_tactico:\n",
    "    df_beta_def['PERIODO'] = df_beta_def['DATE'].astype(str).str[:7]\n",
    "    #df_beta_def['PERIODO'] = np.where(df_beta_def['PERIODO'] == str(ultimo_dia_real)[:7], np.where(df_beta_def['DATE'] <= ultimo_dia_real, df_beta_def['PERIODO'] + '_REAL', df_beta_def['PERIODO'] + '_PREDICT'), df_beta_def['PERIODO'])\n",
    "    df_beta_def = df_beta_def[campos_beta_def].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "df_beta_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Parámetros TC & TP [1-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw parámetros 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = df_representantes_base[df_representantes_base['DUPLICACION'] == grano]\n",
    "df_real['TC'], df_real['TP'] = df_real['ORDENES'] / df_real['VISITAS'], df_real['VENTA_COLOCADA'] / df_real['ORDENES']\n",
    "df_real = df_real[campos_beta + campos_grano + ['VISITAS', 'TC', 'TP']]\n",
    "df_real[['VISITAS', 'TC', 'TP']] = df_real[['VISITAS', 'TC', 'TP']].fillna(0)\n",
    "df_real\n",
    "\n",
    "df_real_visitas = df_real[campos_beta + campos_grano + ['VISITAS']]\n",
    "df_real_visitas['METRICA'] = 'VISITAS'\n",
    "df_real_visitas = df_real_visitas.rename(columns = {'VISITAS': 'VALOR_REAL'})\n",
    "\n",
    "df_real_tc = df_real[campos_beta + campos_grano + ['TC']]\n",
    "df_real_tc['METRICA'] = 'TC'\n",
    "df_real_tc = df_real_tc.rename(columns = {'TC': 'VALOR_REAL'})\n",
    "\n",
    "df_real_tp = df_real[campos_beta + campos_grano + ['TP']]\n",
    "df_real_tp['METRICA'] = 'TP'\n",
    "df_real_tp = df_real_tp.rename(columns = {'TP': 'VALOR_REAL'})\n",
    "\n",
    "df_real = pd.concat([df_real_visitas, df_real_tc, df_real_tp])\n",
    "df_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ultimo_dia_real = df_real.DATE.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tactico(df, ajustar_ratios, modo_tactico):\n",
    "    \n",
    "    if not modo_tactico:\n",
    "        df['y'] = df['y'].fillna(0)\n",
    "        return df\n",
    "    \n",
    "    if ajustar_ratios[0]:\n",
    "        existen_visitas = True\n",
    "        \n",
    "        columns_dims = set(df.columns)\n",
    "        columns_dims = columns_dims - set([ajustar_ratios[1]]) - set(['y'])\n",
    "        print(columns_dims)\n",
    "\n",
    "        df = df.pivot_table(index = list(columns_dims), columns = ajustar_ratios[1], values = ajustar_ratios[2]).reset_index()\n",
    "        if 'VISITAS' not in df.columns:\n",
    "            df['VISITAS'] = 1\n",
    "            existen_visitas = False\n",
    "        \n",
    "        df['ORDENES'] = df['TC'] * df['VISITAS']\n",
    "        df['VENTA_COLOCADA'] = df['TP'] * df['ORDENES']\n",
    "        df = df[list(columns_dims) + ['VISITAS', 'ORDENES', 'VENTA_COLOCADA']]\n",
    "        df[['VISITAS', 'ORDENES', 'VENTA_COLOCADA']] = df[['VISITAS', 'ORDENES', 'VENTA_COLOCADA']].fillna(0)\n",
    "        df['PERIODO'] = df['DATE'].astype(str).str[:7]\n",
    "        \n",
    "        #periodo_actual = str(ultimo_dia_real)[:7]\n",
    "        #df['PERIODO'] = np.where(df['PERIODO'] == periodo_actual, np.where(df['DATE'] <= ultimo_dia_real, df['PERIODO'] + '_REAL', df['PERIODO'] + '_PREDICT'), df['PERIODO'])\n",
    "        \n",
    "        columns_dims_periodo = list(columns_dims) + ['PERIODO']\n",
    "        columns_dims_periodo.remove('DATE')\n",
    "        \n",
    "        df = df[columns_dims_periodo + ['VISITAS', 'ORDENES', 'VENTA_COLOCADA']].groupby(columns_dims_periodo, as_index = False).sum()\n",
    "        \n",
    "        df['TC'] = df['ORDENES'] / df['VISITAS']\n",
    "        df['TP'] = df['VENTA_COLOCADA'] / df['ORDENES']\n",
    "        df = df.drop(columns = ['ORDENES', 'VENTA_COLOCADA'])\n",
    "        df = df.melt(id_vars = columns_dims_periodo, var_name = 'METRICA', value_name = 'y')\n",
    "        \n",
    "        if not existen_visitas:\n",
    "            df = df[df['METRICA'] != 'VISITAS'].reset_index(drop = True)\n",
    "        df['y'] = df['y'].fillna(0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_desde, dia_hasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_presupuesto = {True: '_ajustado', False: ''}\n",
    "\n",
    "# Orgánicos\n",
    "dfP3 = tf.parquet_act(f'{dir_data}Parametros/P3{dic_presupuesto[ajuste_metricas]}')\n",
    "dfP3 = dfP3[dfP3['PAIS'].isin(paises)].reset_index(drop = True)\n",
    "\n",
    "dfP3 = dfP3[(dfP3['DATE'] >= dias_proyeccion[0]) & (dfP3['DATE'] <= dias_proyeccion[1])].reset_index(drop = True)\n",
    "dfP3 = dfP3[dfP3['MODO_PROYECCION'] == modo_proyeccion].reset_index(drop = True) # Presupuesto o libre, según ajustar métricas\n",
    "dfP3 = dfP3.merge(df_real, how = 'left', on = campos_beta + campos_grano + ['METRICA'])\n",
    "\n",
    "dfP3['y2'] = dfP3['real'].fillna(dfP3['predict'])\n",
    "dfP3['y'] = dfP3['VALOR_REAL'].fillna(dfP3['y2'])\n",
    "dfP3 = dfP3[campos_beta + campos_grano + ['METRICA', 'y']]\n",
    "dfP3 = tactico(dfP3, [True, 'METRICA', 'y'], modo_tactico)\n",
    "display(dfP3)\n",
    "#sys.exit('Crear función f: tactico, que permite mensualizar los registros diarios')\n",
    "\n",
    "dic_parametros['TCO'] = dfP3[dfP3['METRICA'] == 'TC'][campos_beta_def + campos_grano + ['y']] # P1\n",
    "dic_parametros['TPO'] = dfP3[dfP3['METRICA'] == 'TP'][campos_beta_def + campos_grano + ['y']] # P3\n",
    "dic_parametros['VO'] = dfP3[dfP3['METRICA'] == 'VISITAS'][campos_beta_def + campos_grano + ['y']] # P5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagos\n",
    "\n",
    "dfP2 = tf.parquet_act(f'{dir_data}Parametros/P2')\n",
    "dfP2 = dfP2[dfP2['PAIS'].isin(paises)].reset_index(drop = True)\n",
    "dfP2 = dfP2[(dfP2['DATE'] >= dias_proyeccion[0]) & (dfP2['DATE'] <= dias_proyeccion[1])].reset_index(drop = True)\n",
    "dfP2 = dfP2.merge(df_real, how = 'left', on = campos_beta + campos_grano + ['METRICA'])\n",
    "dfP2['y2'] = dfP2['REAL'].fillna(dfP2['PREDICCION'])\n",
    "dfP2['y'] = dfP2['VALOR_REAL'].fillna(dfP2['y2'])\n",
    "dfP2 = dfP2[campos_beta + campos_grano + ['METRICA', 'y']]\n",
    "dfP2 = tactico(dfP2, [True, 'METRICA', 'y'], modo_tactico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros['TCP'] = dfP2[dfP2['METRICA'] == 'TC'][campos_beta_def + campos_grano + ['y']] # P2\n",
    "dic_parametros['TPP'] = dfP2[dfP2['METRICA'] == 'TP'][campos_beta_def + campos_grano + ['y']] # P4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deben existir para los conjuntos alpha grano y beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_gcp(lista):\n",
    "    s = '('\n",
    "    for i in lista:\n",
    "        s += f'\"{i}\",'\n",
    "    s = s[:-1] + ')'\n",
    "    return s\n",
    "\n",
    "\n",
    "str_paises = list_to_gcp(paises)\n",
    "\n",
    "tabla_data_all = tabla_data[:-2] + '9'\n",
    "tabla_data_all\n",
    "\n",
    "query = f'select distinct DUPLICACION, PAIS, CANAL, FUENTE, FAMILIA, SUBFAMILIA, TIPO_MEDIO, NATURALEZA_MEDIO from {tabla_data_all} where FUENTE_DATOS = \"REAL\" and pais in {str_paises}'\n",
    "\n",
    "df_alpha_beta_duplicacion = tf.request_GCP_vnew(\n",
    "                            nombre_tabla = \"\",\n",
    "                            specific_query = query,\n",
    "                            client = client,\n",
    "                            output = False, permitir_fallos = False)\n",
    "\n",
    "df_alpha_beta_duplicacion['AUX'] = 'aux'\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion.merge(df_dates, on = 'AUX')\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion.drop(columns = ['AUX'])\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion[df_alpha_beta_duplicacion['DUPLICACION'].isin(lista_duplicidades)].reset_index(drop = True)\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion.sort_values(['DUPLICACION', 'PAIS', 'DATE', 'CANAL', 'FUENTE', 'FAMILIA', 'SUBFAMILIA', 'TIPO_MEDIO', 'NATURALEZA_MEDIO']).reset_index(drop = True)\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion[['DUPLICACION', 'PAIS', 'DATE', 'CANAL', 'FUENTE', 'FAMILIA', 'SUBFAMILIA', 'TIPO_MEDIO', 'NATURALEZA_MEDIO']]\n",
    "\n",
    "print(df_alpha_beta_duplicacion.DUPLICACION.unique())\n",
    "if modo_tactico:\n",
    "    df_alpha_beta_duplicacion['PERIODO'] = df_alpha_beta_duplicacion['DATE'].astype(str).str[:7]\n",
    "    df_alpha_beta_duplicacion = df_alpha_beta_duplicacion.drop(columns = ['DATE']).drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "# Limpieza\n",
    "print(df_alpha_beta_duplicacion.DUPLICACION.unique())\n",
    "# Si duplicacion = F y FAMILIA = \"\", excluir\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion[~((df_alpha_beta_duplicacion['DUPLICACION'] == 'F') & (df_alpha_beta_duplicacion['FAMILIA'] == ''))].reset_index(drop = True)\n",
    "# Si duplicacion = F-LT y (familia = \"\" o tipo_medio = \"\") excluir\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion[~((df_alpha_beta_duplicacion['DUPLICACION'] == 'F-LT') & ((df_alpha_beta_duplicacion['FAMILIA'] == '') | (df_alpha_beta_duplicacion['TIPO_MEDIO'] == '')))].reset_index(drop = True)\n",
    "# Si duplicacion = F-SF y (familia = \"\" o subfamilia = \"\") excluir\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion[~((df_alpha_beta_duplicacion['DUPLICACION'] == 'F-SF') & ((df_alpha_beta_duplicacion['FAMILIA'] == '') | (df_alpha_beta_duplicacion['SUBFAMILIA'] == '')))].reset_index(drop = True)\n",
    "# Si duplicacion = LT y tipo_medio = \"\" excluir\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion[~((df_alpha_beta_duplicacion['DUPLICACION'] == 'LT') & (df_alpha_beta_duplicacion['TIPO_MEDIO'] == ''))].reset_index(drop = True)\n",
    "print(df_alpha_beta_duplicacion.DUPLICACION.unique())\n",
    "df_alpha_beta_duplicacion = df_alpha_beta_duplicacion[(df_alpha_beta_duplicacion['FAMILIA'].isin(list(familia_sm) + [''])) & (df_alpha_beta_duplicacion['TIPO_MEDIO'].isin(list(tm_seleccion) + ['']))].reset_index(drop = True)\n",
    "df_alpha_beta_duplicacion = df_beta_def.merge(df_alpha_beta_duplicacion, on = campos_beta_def, how = 'left')\n",
    "df_alpha_beta_duplicacion\n",
    "print(df_alpha_beta_duplicacion.DUPLICACION.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_beta_duplicacion_grano = df_alpha_beta_duplicacion[df_alpha_beta_duplicacion['DUPLICACION'] == grano].reset_index(drop = True)\n",
    "df_alpha_beta_duplicacion_grano = df_alpha_beta_duplicacion_grano[campos_beta_def + campos_grano]\n",
    "df_alpha_beta_duplicacion_grano['IN'] = True\n",
    "df_alpha_beta_duplicacion_grano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_alpha_beta_duplicacion_grano_def = df_alpha_beta_duplicacion_grano.copy()\n",
    "if modo_tactico:\n",
    "    df_alpha_beta_duplicacion_grano_def['PERIODO'] = df_alpha_beta_duplicacion_grano_def['DATE'].astype(str).str[:7]\n",
    "    #df_alpha_beta_duplicacion_grano_def['PERIODO'] = np.where(df_alpha_beta_duplicacion_grano_def['PERIODO'] == str(ultimo_dia_real)[:7], np.where(df_alpha_beta_duplicacion_grano_def['DATE'] <= ultimo_dia_real, df_alpha_beta_duplicacion_grano_def['PERIODO'] + '_REAL', df_alpha_beta_duplicacion_grano_def['PERIODO'] + '_PREDICT'), df_alpha_beta_duplicacion_grano_def['PERIODO'])\n",
    "    df_alpha_beta_duplicacion_grano_def = df_alpha_beta_duplicacion_grano_def.drop(columns = ['DATE']).drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "df_alpha_beta_duplicacion_grano_def\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_def['AUX'] = 'aux'\n",
    "df_alpha_grano_organico['AUX'] = 'aux'\n",
    "df_alpha_grano_pago['AUX'] = 'aux'\n",
    "\n",
    "df_beta_alpha_grano_organico = df_beta_def.merge(df_alpha_grano_organico, on = 'AUX')\n",
    "df_beta_alpha_grano_pago = df_beta_def.merge(df_alpha_grano_pago, on = 'AUX')\n",
    "\n",
    "for df in [df_beta_def, df_alpha_grano_organico, df_alpha_grano_pago, df_beta_alpha_grano_organico, df_beta_alpha_grano_pago]:\n",
    "    df.drop(columns = ['AUX'], inplace = True) # Con inplace para que funcione, ya que df es variable iterativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_alpha_grano_organico.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo los casos que existen en df_alpha_beta_duplicacion_grano\n",
    "df_beta_alpha_grano_organico = df_beta_alpha_grano_organico.merge(df_alpha_beta_duplicacion_grano, on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_beta_alpha_grano_pago = df_beta_alpha_grano_pago.merge(df_alpha_beta_duplicacion_grano, on = campos_beta_def + campos_grano, how = 'left')\n",
    "\n",
    "df_beta_alpha_grano_organico['IN'] = df_beta_alpha_grano_organico['IN'].fillna(False)\n",
    "df_beta_alpha_grano_pago['IN'] = df_beta_alpha_grano_pago['IN'].fillna(False)\n",
    "\n",
    "df_beta_alpha_grano_organico = df_beta_alpha_grano_organico[df_beta_alpha_grano_organico['IN']].reset_index(drop = True)\n",
    "df_beta_alpha_grano_pago = df_beta_alpha_grano_pago[df_beta_alpha_grano_pago['IN']].reset_index(drop = True)\n",
    "\n",
    "df_beta_alpha_grano_organico = df_beta_alpha_grano_organico.drop(columns = ['IN'])\n",
    "df_beta_alpha_grano_pago = df_beta_alpha_grano_pago.drop(columns = ['IN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_alpha_grano_organico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ['TCO', 'TPO', 'VO', 'TCP', 'TPP']:\n",
    "    df = dic_parametros[m]\n",
    "    if m[-1:] == 'O':\n",
    "        df = df_beta_alpha_grano_organico.merge(df, on = campos_beta_def + campos_grano, how = 'left')\n",
    "    else:\n",
    "        df = df_beta_alpha_grano_pago.merge(df, on = campos_beta_def + campos_grano, how = 'left')\n",
    "    df['y'] = df['y'].fillna(0)\n",
    "    dic_parametros[m] = df\n",
    "    print(m,  m[-1:])\n",
    "    display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, '1.3.1 Pars 1-5', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Relaciones duplicidad [8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_proyeccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.parquet_act(f'{dir_data}Parametros/P1')\n",
    "df = df[(df['DATE'] >= dias_proyeccion[0]) & (df['DATE'] <= dias_proyeccion[1])].reset_index(drop = True)\n",
    "df = df[df['PAIS'].isin(paises)].reset_index(drop = True)\n",
    "df_arcos = df[['AGREGADO', 'DESAGREGADO']].drop_duplicates().reset_index(drop = True)\n",
    "df_arcos = df_arcos[(df_arcos['AGREGADO'].isin(lista_duplicidades)) & (df_arcos['DESAGREGADO'].isin(lista_duplicidades))]\n",
    "\n",
    "# Para evitar ciclos en el grafo (doble-dependencias)\n",
    "df_arcos =  df_arcos[df_arcos['AGREGADO'] != \"\"]\n",
    "df_arcos = df_arcos[~((df_arcos['AGREGADO'] == 'LT') & (df_arcos['DESAGREGADO'] == 'F-LT'))].reset_index(drop = True)\n",
    "df = df[~((df['AGREGADO'] == 'LT') & (df['DESAGREGADO'] == 'F-LT'))]\n",
    "display(df_arcos)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modo_tactico:\n",
    "    df['PERIODO'] = df['DATE'].astype(str).str[:7]\n",
    "\n",
    "df = df[['AGREGADO', 'DESAGREGADO'] + campos_beta_def + campos_grano + ['VENTA_COLOCADA_AGREGADO', 'VENTA_COLOCADA_DESAGREGADO', 'ORDENES_AGREGADO', 'ORDENES_DESAGREGADO', 'VISITAS_AGREGADO', 'VISITAS_DESAGREGADO']].groupby(['AGREGADO', 'DESAGREGADO'] + campos_beta_def + campos_grano, as_index = False).sum()\n",
    "\n",
    "for m in ['VENTA_COLOCADA', 'ORDENES', 'VISITAS']:\n",
    "    df[f'f_{m}'] = df[f'{m}_AGREGADO'] / df[f'{m}_DESAGREGADO']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear un grafo dirigido\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Agregar los nodos y las aristas al grafo\n",
    "for agregado, desagregado in zip(df_arcos[\"AGREGADO\"], df_arcos[\"DESAGREGADO\"]):\n",
    "    G.add_edge(desagregado, agregado)\n",
    "\n",
    "# Dibujar el grafo\n",
    "plt.figure(figsize = (8, 6))\n",
    "pos = nx.spring_layout(G)  # Posiciones de los nodos para un diseño agradable\n",
    "nx.draw_networkx_nodes(G, pos, node_size = 500, node_color = \"lightblue\")\n",
    "nx.draw_networkx_edges(G, pos, arrowstyle = \"->\", arrowsize = 20, edge_color = \"gray\")\n",
    "nx.draw_networkx_labels(G, pos, font_size = 10, font_color = \"black\")\n",
    "\n",
    "# Mostrar el grafo\n",
    "plt.title(\"Grafo: AGREGADO y DESAGREGADO\", fontsize = 14)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros['RELACIONES DUPLICIDAD'] = df\n",
    "df_times, t0 = medir_tiempo(t0, '1.3.3 Pars 8', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Metas [9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relaciones_canal_fuente = pd.DataFrame({'CANAL_BASE': ['WEB', 'APP', 'FCOM', 'FCOM', 'FCOM', 'FCOM'],\n",
    "                              'CANAL': ['WEB', 'APP', 'WEB', 'APP', 'WEB', 'APP'],\n",
    "                              'FUENTE': ['SODIMAC', 'SODIMAC', 'SIS', 'SIS', 'ES', 'ES']})\n",
    "df_relaciones_canal_fuente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0_df_resumen_metas_general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.parquet_act(f'{dir_metas}0_df_resumen_metas_general')\n",
    "\n",
    "if not modo_tactico:\n",
    "    df_pais_t = df_beta_def[['DATE', 'PAIS']].drop_duplicates().reset_index(drop = True)\n",
    "    df_pais_t['PERIODO'] = df_pais_t['DATE'].astype(str).str[:7]\n",
    "    \n",
    "else:\n",
    "    df_pais_t = df_beta_def[['PERIODO', 'PAIS']].drop_duplicates().reset_index(drop = True)\n",
    "    \n",
    "df_pais_t['IN'] = True\n",
    "df = df.merge(df_pais_t, on = ['PERIODO', 'PAIS'], how = 'left')\n",
    "\n",
    "df['IN'] = df['IN'].fillna(False)\n",
    "df = df[df['IN']].reset_index(drop = True)\n",
    "df = df.drop(columns = ['IN'])\n",
    "\n",
    "df['RATIO_NETA_COLOCADA'] = df['VENTA_NETA'] / df['VENTA_COLOCADA']\n",
    "df['TP'] = df['VENTA_COLOCADA'] / df['ORDENES']\n",
    "df['TC'] = df['ORDENES'] / df['VISITAS']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros['METAS'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ratio VN / VC por df_beta\n",
    "\n",
    "df_vnvc = df[['PAIS', 'CANAL', 'PERIODO', 'RATIO_NETA_COLOCADA']]\n",
    "\n",
    "#year0, yearF = int(df['PERIODO'].min()[:4]), int(df['PERIODO'].max()[:4])\n",
    "# dataframe con todas las fechas en estos años\n",
    "\n",
    "#df_dates = pd.date_range(start = f'{year0}-01-01', end = f'{yearF}-12-31', freq = 'D')\n",
    "#df_dates = pd.DataFrame({'DATE': df_dates})\n",
    "#df_dates['PERIODO'] = df_dates['DATE'].astype(str).str[:7]\n",
    "#df_dates\n",
    "\n",
    "df_fcom = df_vnvc[df_vnvc['CANAL'] == 'FCOM'][['PAIS', 'PERIODO', 'RATIO_NETA_COLOCADA']]\n",
    "df_sodimac = df_vnvc[df_vnvc['CANAL'] != 'FCOM']\n",
    "\n",
    "# fcom\n",
    "if len(df_fcom) > 0:\n",
    "    df_fcom['AUX'] = 'aux'\n",
    "    df_fuente_canal_fcom = pd.DataFrame({'CANAL': ['APP', 'APP', 'WEB', 'WEB'], 'FUENTE': ['ES', 'SIS', 'ES', 'SIS']})\n",
    "    df_fuente_canal_fcom['AUX'] = 'aux'\n",
    "    df_fcom = df_fcom.merge(df_fuente_canal_fcom, on = 'AUX')\n",
    "    df_fcom = df_fcom.drop(columns = ['AUX'])\n",
    "    #df_fcom = df_dates.merge(df_fcom, on = ['PERIODO'], how = 'left')\n",
    "    df_fcom = df_fcom[['PAIS', 'PERIODO', 'CANAL', 'FUENTE', 'RATIO_NETA_COLOCADA']]\n",
    "\n",
    "# sodimac (azul)\n",
    "df_sodimac['FUENTE'] = 'SODIMAC'\n",
    "#df_sodimac = df_dates.merge(df_sodimac, on = ['PERIODO'], how = 'left')\n",
    "df_sodimac = df_sodimac[['PAIS', 'PERIODO', 'CANAL', 'FUENTE', 'RATIO_NETA_COLOCADA']]\n",
    "\n",
    "# concat\n",
    "df_ratio_neta_colocada = pd.concat([df_fcom, df_sodimac])[['PAIS', 'PERIODO', 'CANAL', 'FUENTE', 'RATIO_NETA_COLOCADA']].reset_index(drop = True)\n",
    "df_ratio_neta_colocada.columns.name = None\n",
    "dic_parametros['METAS']['RATIO NETA COLOCADA'] = df_ratio_neta_colocada\n",
    "df_ratio_neta_colocada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordenes & visitas\n",
    "for metrica in ['VENTA_NETA', 'VENTA_COLOCADA', 'ORDENES', 'VISITAS', 'TP', 'TC']:\n",
    "    df_metrica = df[['PAIS', 'CANAL', 'PERIODO', metrica]].rename(columns = {'CANAL': 'CANAL_BASE'})\n",
    "    \n",
    "    metrica0 = metrica\n",
    "    if metrica == 'VENTA_NETA':\n",
    "        metrica0 = 'VENTA_NETA_BASE'\n",
    "    dic_parametros['METAS'][metrica0] = df_metrica\n",
    "    display(df_metrica.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1_df_resumen_metas_otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.parquet_act(f'{dir_metas}1_df_resumen_metas_otros')\n",
    "df = df.merge(df_pais_t, on = ['PERIODO', 'PAIS'], how = 'left')\n",
    "df['IN'] = df['IN'].fillna(False)\n",
    "df = df[df['IN']].reset_index(drop = True)\n",
    "df = df.drop(columns = ['IN'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impuesto\n",
    "\n",
    "df_impuesto = df[['PAIS', 'PERIODO', 'IMPUESTO']].reset_index(drop = True)\n",
    "df_impuesto.columns.name = None\n",
    "dic_parametros['METAS']['IMPUESTO'] = df_impuesto\n",
    "df_impuesto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversion\n",
    "df_inversion = df[['PAIS', 'PERIODO', 'INVERSION']].reset_index(drop = True)\n",
    "df_inversion.columns.name = None\n",
    "dic_parametros['METAS']['INVERSION'] = df_inversion\n",
    "df_inversion.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# % Tráfico Orgánico\n",
    "df_trafico_organico = df[['PAIS', 'PERIODO', 'PORCENTAJE_TRAFICO_ORGANICO']].reset_index(drop = True)\n",
    "df_trafico_organico['PORCENTAJE_TRAFICO_ORGANICO'] *= (1 / 100)\n",
    "df_trafico_organico.columns.name = None\n",
    "dic_parametros['METAS']['PORCENTAJE_TRAFICO_ORGANICO'] = df_trafico_organico\n",
    "df_trafico_organico.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_df_resumen_metas_detalle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.parquet_act(f'{dir_metas}2_df_resumen_metas_detalle')\n",
    "df = df.merge(df_pais_t, on = ['PERIODO', 'PAIS'], how = 'left')\n",
    "df['IN'] = df['IN'].fillna(False)\n",
    "df = df[df['IN']].reset_index(drop = True)\n",
    "df_metas_venta = df[['PAIS', 'FAMILIA', 'SUBFAMILIA', 'PERIODO', 'VENTA_NETA']].reset_index(drop = True)\n",
    "dic_parametros['METAS']['VENTA_NETA'] = df_metas_venta\n",
    "df_metas_venta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros['METAS'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo revisión\n",
    "for m in dic_parametros['METAS'].keys():\n",
    "    print('m', m)\n",
    "    df = dic_parametros['METAS'][m]\n",
    "    df = df[df['PERIODO'] >= \"2025-01\"]\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, '1.3.4 Pars 9', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.5. Matriz de relación tráfico IN - tráfico Total [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data visitas IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = '../Data/'\n",
    "dir_data_horus = '../Data/Horus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_visitas = pd.DataFrame()\n",
    "if 'df_data_visitas.pkl' in os.listdir(dir_data_horus):\n",
    "    df_data_visitas = tf.parquet_act(f'{dir_data_horus}df_data_visitas')\n",
    "df_data_visitas\n",
    "\n",
    "if len(df_data_visitas) > 0:\n",
    "    df_data_visitas_existentes = df_data_visitas[['DATE', 'PAIS']].drop_duplicates().reset_index(drop = True)\n",
    "    df_data_visitas_existentes['DATE'] = pd.to_datetime(df_data_visitas_existentes['DATE'])\n",
    "    df_data_visitas_existentes['EXISTE'] = True\n",
    "\n",
    "df_data_visitas_existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '')\n",
    "from dias_comparacion import dias_comparacion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_max_date = f'select max(DATE) as DATE from `{tabla_data}_{grano}` where FUENTE_DATOS = \"REAL\"'\n",
    "\n",
    "df_max_date = tf.request_GCP_vnew(\n",
    "    nombre_tabla = \"\",\n",
    "    specific_query = query_max_date,\n",
    "    client = client,\n",
    "    output = False, permitir_fallos = False)\n",
    "\n",
    "df_max_date['DATE'] = pd.to_datetime(df_max_date['DATE'])\n",
    "\n",
    "dia_hasta = df_max_date['DATE'][0].date()\n",
    "\n",
    "dia_desde = (dia_hasta + dt.timedelta(days = 1)).replace(year = dia_hasta.year - 1)#.strftime('%Y-%m-%d')\n",
    "dia_desde, dia_hasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dias_proyeccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dias_predecir = dias_comparacion(paises, [dias_proyeccion[0].date(), dias_proyeccion[1].date()], df_dias_eventos, [dia_desde, dia_hasta])\n",
    "df_dias_predecir = df_dias_predecir[(df_dias_predecir['DATE'] >= dias_proyeccion[0]) & (df_dias_predecir['DATE'] <= dias_proyeccion[1])].reset_index(drop = True)\n",
    "df_dias_predecir # Asigna a cada día de la proyección, uno que ya exista (histórico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_desde, dia_hasta = df_dias_predecir['DATE_COMPARACION'].min(), df_dias_predecir['DATE_COMPARACION'].max()\n",
    "dia_hasta = dt.datetime.now().date() - dt.timedelta(days = 1)\n",
    "dia_desde, dia_hasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = pd.date_range(start = dia_desde, end = dia_hasta, freq = 'D')\n",
    "df_dates = pd.DataFrame({'DATE': df_dates})\n",
    "df_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_data_visitas) > 0:\n",
    "    df_data_visitas['DATE'] = pd.to_datetime(df_data_visitas['DATE'])\n",
    "\n",
    "for pais in paises:\n",
    "    \n",
    "    df_dates_pais = df_dates.copy()\n",
    "    df_dates_pais['PAIS'] = pais\n",
    "    if len(df_data_visitas) > 0:\n",
    "        df_dates_pais = df_dates_pais.merge(df_data_visitas_existentes, how = 'left', on = ['DATE', 'PAIS'])\n",
    "        df_dates_pais['EXISTE'] = df_dates_pais['EXISTE'].fillna(False)\n",
    "        df_dates_pais = df_dates_pais[df_dates_pais['EXISTE'] != True]\n",
    "    \n",
    "    if len(df_dates_pais) == 0: # Si no hay nada, el país está completamente actualizado\n",
    "        continue\n",
    "    \n",
    "    min_date, max_date = str(df_dates_pais['DATE'].min())[:10], str(df_dates_pais['DATE'].max())[:10]\n",
    "    \n",
    "    query_detalle = f'PAIS = \"{pais}\" and DATE >= \"{min_date}\" and DATE <= \"{max_date}\"'\n",
    "    \n",
    "    # VISITAS IN\n",
    "    query = f'SELECT * FROM `sod-corp-plp-beta.ETL_main_2023.VisitasIN_241122_0_v2` where {query_detalle} and duplicacion = \"{grano}\"'\n",
    "\n",
    "    # request y df\n",
    "    df_visitas_in =  tf.request_GCP_vnew(\n",
    "            nombre_tabla = \"\",\n",
    "            specific_query = query,\n",
    "            client = client,\n",
    "            output = True, permitir_fallos = False)\n",
    "\n",
    "    df_visitas_in['DATE'] = pd.to_datetime(df_visitas_in['DATE'])\n",
    "    #df_visitas_in.to_csv(f'{dir_data_horus}1df_visitas_in.csv', index = False, sep = ';', decimal = ',')\n",
    "    #df_visitas = df_visitas.merge(df_medios, on = 'LAST_TOUCH_CHANNEL', how = 'left')\n",
    "    #df_visitas = df_visitas[campos_beta + campos_grano + ['VISITAS_IN']].groupby(campos_beta + campos_grano, as_index = False).sum()\n",
    "    \n",
    "    df_visitas_in = df_visitas_in.merge(df_medios, on = 'LAST_TOUCH_CHANNEL', how = 'left')\n",
    "    df_visitas_in = df_visitas_in[campos_beta + campos_grano + ['VISITAS_IN']].groupby(campos_beta + campos_grano, as_index = False).sum()    \n",
    "    \n",
    "    # VISITAS\n",
    "    query = f'SELECT * FROM `sod-corp-plp-beta.ETL_main_2023.Nexus_Main_230829_0_v3` where {query_detalle} and duplicacion = \"{grano}\"'\n",
    "\n",
    "    # request y df\n",
    "    df_visitas =  tf.request_GCP_vnew(\n",
    "            nombre_tabla = \"\",\n",
    "            specific_query = query,\n",
    "            client = client,\n",
    "            output = True, permitir_fallos = False)\n",
    "\n",
    "    df_visitas['DATE'] = pd.to_datetime(df_visitas['DATE'])\n",
    "    #df_visitas.to_csv(f'{dir_data_horus}2df_visitas.csv', index = False, sep = ';', decimal = ',')\n",
    "    \n",
    "    df_visitas = df_visitas.merge(df_medios, on = 'LAST_TOUCH_CHANNEL', how = 'left')\n",
    "    df_visitas = df_visitas[campos_beta + campos_grano + ['VISITAS']].groupby(campos_beta + campos_grano, as_index = False).sum()\n",
    "    #df_visitas.to_csv(f'{dir_data_horus}2b_df_visitas.csv', index = False, sep = ';', decimal = ',')\n",
    "    \n",
    "    # RATIO\n",
    "    df_ratio = df_visitas_in.merge(df_visitas, on = campos_beta + campos_grano, how = 'outer')\n",
    "    df_ratio = df_ratio.fillna(0)\n",
    "    #df_ratio.to_csv(f'{dir_data_horus}df_ratio.csv', index = False, sep = ';', decimal = ',')\n",
    "    df_ratio['RATIO'] = np.where(df_ratio['VISITAS'] > 0, df_ratio['VISITAS_IN'] / df_ratio['VISITAS'], 1)\n",
    "    df_ratio = df_ratio[campos_beta + campos_grano + ['RATIO']]\n",
    "    df_ratio\n",
    "\n",
    "    # VISITAS ALL\n",
    "    query = f'SELECT * FROM `sod-corp-plp-beta.ETL_main_2023.Nexus_Main_230829_0_v10_{grano}` where {query_detalle} and duplicacion = \"{grano}\" and FUENTE_DATOS = \"REAL\"'\n",
    "\n",
    "    # request y df\n",
    "    df_visitas_all =  tf.request_GCP_vnew(\n",
    "            nombre_tabla = \"\",\n",
    "            specific_query = query,\n",
    "            client = client,\n",
    "            output = True, permitir_fallos = False)\n",
    "\n",
    "    df_visitas_all['DATE'] = pd.to_datetime(df_visitas_all['DATE'])\n",
    "\n",
    "    df_visitas_all = df_visitas_all[campos_beta + campos_grano + ['VISITAS']].groupby(campos_beta + campos_grano, as_index = False).sum()\n",
    "\n",
    "    df_visitas_all_final = df_visitas_all.merge(df_ratio, on = campos_beta + campos_grano, how = 'left')\n",
    "    #df_visitas_all_final.to_csv(f'{dir_data_horus}df_visitas_all_final.csv', index = False, sep = ';', decimal = ',')\n",
    "    df_visitas_all_final['RATIO'] = df_visitas_all_final['RATIO'].fillna(1)\n",
    "    df_visitas_all_final['VISITAS_IN'] = df_visitas_all_final['VISITAS'] * df_visitas_all_final['RATIO']\n",
    "    df_visitas_all_final = df_visitas_all_final.drop(columns = ['RATIO'])\n",
    "    \n",
    "    df_visitas_all_final['DATE'] = pd.to_datetime(df_visitas_all_final['DATE'])\n",
    "    df_data_visitas = pd.concat([df_visitas_all_final, df_data_visitas]).reset_index(drop = True) # Prioridad al nuevo registro\n",
    "    \n",
    "    df_data_visitas = df_data_visitas.drop_duplicates(subset = campos_beta + campos_grano, keep = 'first').reset_index(drop = True)\n",
    "\n",
    "tf.parquet_act(f'{dir_data_horus}df_data_visitas', df_data_visitas, 'save')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chatgpt.com/c/673e3b77-464c-8000-a9d7-5bb500da438d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_desde, dia_hasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz entropía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_visitas = tf.parquet_act(f'{dir_data_horus}df_data_visitas') # Se lee y se filtra df_data_visitas # este para inversion 250109\n",
    "df_data_visitas['DATE'] = pd.to_datetime(df_data_visitas['DATE'])\n",
    "df_data_visitas = df_data_visitas[df_data_visitas['PAIS'].isin(paises)]\n",
    "df_data_visitas = df_data_visitas[(df_data_visitas['DATE'] >= str(dia_desde)) & (df_data_visitas['DATE'] <= str(dia_hasta))].reset_index(drop = True)\n",
    "df_data_visitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodo_temporal = 'DATE'\n",
    "if modo_tactico:\n",
    "    periodo_temporal = 'PERIODO'\n",
    "    \n",
    "df_beta_alpha_grano_pago_visitas = df_beta_alpha_grano_pago.drop(columns = periodo_temporal).drop_duplicates().reset_index(drop = True)\n",
    "df_beta_alpha_grano_pago_visitas['AUX'] = 'aux'\n",
    "\n",
    "df_dates_comp = df_dias_predecir[['DATE_COMPARACION']].drop_duplicates().reset_index(drop = True).rename(columns = {'DATE_COMPARACION': 'DATE'})\n",
    "df_dates_comp['AUX'] = 'aux'\n",
    "\n",
    "df_beta_alpha_grano_pago_visitas = df_beta_alpha_grano_pago_visitas.merge(df_dates_comp, on = 'AUX')\n",
    "df_beta_alpha_grano_pago_visitas = df_beta_alpha_grano_pago_visitas.drop(columns = ['AUX'])\n",
    "\n",
    "df_beta_alpha_grano_pago_visitas['IN'] = True\n",
    "\n",
    "df_data_visitas['DATE'], df_beta_alpha_grano_pago_visitas['DATE'] = pd.to_datetime(df_data_visitas['DATE']), pd.to_datetime(df_beta_alpha_grano_pago_visitas['DATE'])\n",
    "df_data_visitas = df_data_visitas.merge(df_beta_alpha_grano_pago_visitas, on = campos_beta + campos_grano, how = 'left')\n",
    "df_data_visitas['IN'] = df_data_visitas['IN'].fillna(False)\n",
    "df_data_visitas = df_data_visitas[df_data_visitas['IN']].reset_index(drop = True)\n",
    "df_data_visitas = df_data_visitas.drop(columns = ['IN'])\n",
    "df_data_visitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfa = df_data_visitas[df_data_visitas['DATE'] == \"2024-02-02\"]\n",
    "#dfa[dfa.TIPO_MEDIO == 'Reference Domain'].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterador = campos_beta + [campo_last_touch]\n",
    "lista_clacom = list(set(campos_grano) - {campo_last_touch})\n",
    "\n",
    "print(iterador)\n",
    "print(lista_clacom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de entropía a maximizar\n",
    "def entropy(M):\n",
    "    M = M.reshape(n, n)\n",
    "    return -np.sum(M[M > 0] * np.log(M[M > 0]))\n",
    "\n",
    "# Restricción M * a = b\n",
    "def matrix_vector_constraint(M):\n",
    "    M = M.reshape(n, n)\n",
    "    return np.dot(M, a) - b\n",
    "\n",
    "# Restricción de diagonal fija = 1\n",
    "def diagonal_constraint(M):\n",
    "    M = M.reshape(n, n)\n",
    "    return [M[i, i] - 1 for i in range(n)]\n",
    "\n",
    "def matriz_max_entropia(df_data_visitas_i, lista_clacom):\n",
    "    # Configuración para n x n\n",
    "    global n, a, b\n",
    "    n = len(df_data_visitas_i['VISITAS_IN'])  # Tamaño de la matriz\n",
    "    a = np.array(df_data_visitas_i['VISITAS_IN'])\n",
    "    b = np.array(df_data_visitas_i['VISITAS'])\n",
    "    \n",
    "    #a = np.array(a, dtype=float)\n",
    "    #b = np.array(b, dtype=float)\n",
    "    \n",
    "    #print(type(a), a.dtype, type(b), b.dtype)\n",
    "\n",
    "    # Restricciones\n",
    "    constraints = [\n",
    "        {\"type\": \"eq\", \"fun\": diagonal_constraint},  # Diagonal fija\n",
    "        {\"type\": \"eq\", \"fun\": matrix_vector_constraint},  # M * a = b\n",
    "    ]\n",
    "\n",
    "    # Límites: Mij >= 0\n",
    "    bounds = [(0, None)] * (n * n)\n",
    "\n",
    "    # Inicialización de la matriz (puede ser identidad)\n",
    "    M_initial = np.eye(n).flatten()\n",
    "\n",
    "    # Optimización\n",
    "    result = minimize(\n",
    "        entropy,\n",
    "        M_initial,\n",
    "        bounds = bounds,\n",
    "        constraints = constraints,\n",
    "        method = 'SLSQP'\n",
    "    )\n",
    "\n",
    "    # Resultado\n",
    "    M_optimized = result.x.reshape(n, n)\n",
    "    \n",
    "    # Ajusta el resultado a un dataframe Familia 1, familia 2, resultado M\n",
    "    \n",
    "    df_clacom = df_data_visitas_i[lista_clacom]\n",
    "    for j in ['INICIAL', 'FINAL']:\n",
    "        df_clacom_j = df_clacom.copy()\n",
    "        for c in lista_clacom:\n",
    "            df_clacom_j = df_clacom_j.rename(columns = {c: f'{c}_{j}'})\n",
    "        df_clacom_j['AUX'] = 'aux'\n",
    "        if j == 'INICIAL':\n",
    "            df_clacom_all = df_clacom_j.copy()\n",
    "        else:\n",
    "            df_clacom_all = df_clacom_all.merge(df_clacom_j, on = 'AUX', how = 'left')\n",
    "    df_clacom_all = df_clacom_all.drop(columns = ['AUX'])\n",
    "    df_clacom_all['M'] = M_optimized.flatten()\n",
    "    \n",
    "    # comprueba las restricciones\n",
    "    df_clacom_all_igual = df_clacom_all[(df_clacom_all['FAMILIA_INICIAL'] == df_clacom_all['FAMILIA_FINAL']) & (df_clacom_all['M'] != 1)]\n",
    "    if len(df_clacom_all_igual) > 0:\n",
    "        print('ERROR')\n",
    "        display(df_clacom_all_igual)\n",
    "        sys.exit('Restricciones 1...diagonal != 1')\n",
    "    \n",
    "    if df_clacom_all['M'].min() < 0:\n",
    "        print('ERROR')\n",
    "        display(df_clacom_all[df_clacom_all['M'] < 0])\n",
    "        sys.exit('Restricciones 2...Mij < 0')\n",
    "\n",
    "    return df_clacom_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de entropía para casos no existentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iterador = df_data_visitas[iterador].drop_duplicates().reset_index(drop = True) # Lo que necesito\n",
    "df_iterador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matriz_entropia = pd.DataFrame()\n",
    "if 'df_matriz_entropia.pkl' in os.listdir(dir_data_horus):\n",
    "    df_matriz_entropia = tf.parquet_act(f'{dir_data_horus}df_matriz_entropia')\n",
    "\n",
    "if len(df_matriz_entropia) > 0:\n",
    "    df_matriz_entropia_existente = df_matriz_entropia[iterador].drop_duplicates().reset_index(drop = True)\n",
    "    df_matriz_entropia_existente['EXISTE'] = True\n",
    "\n",
    "df_matriz_entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_matriz_entropia) > 0: # Solo si existe algo\n",
    "    df_iterador = df_iterador.merge(df_matriz_entropia_existente, on = iterador, how = 'left')\n",
    "    df_iterador['EXISTE'] = df_iterador['EXISTE'].fillna(False)\n",
    "    df_iterador = df_iterador[~df_iterador['EXISTE']].reset_index(drop = True) # Se procesará solo lo que no existe\n",
    "    df_iterador = df_iterador.drop(columns = ['EXISTE']) \n",
    "df_iterador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "paso = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = campos_beta + [campo_last_touch] + [f'{j}_INICIAL' for j in lista_clacom] + [f'{j}_FINAL' for j in lista_clacom]\n",
    "\n",
    "if len(df_matriz_entropia) > 0:\n",
    "    df_matriz_entropia['DATE'] = pd.to_datetime(df_matriz_entropia['DATE'])\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_iterador))):\n",
    "    \n",
    "    #print('ELIMINAR')\n",
    "    df_i = df_iterador.iloc[i: i + 1]\n",
    "    df_i['INCLUIR'] = True\n",
    "    df_data_visitas_i = df_data_visitas.merge(df_i, on = iterador, how = 'left')\n",
    "    df_data_visitas_i['INCLUIR'] = df_data_visitas_i['INCLUIR'].fillna(False)\n",
    "    df_data_visitas_i = df_data_visitas_i[df_data_visitas_i['INCLUIR']].reset_index(drop = True)\n",
    "    df_data_visitas_i['VISITAS_IN'] = np.minimum(df_data_visitas_i['VISITAS_IN'], df_data_visitas_i['VISITAS']) # Casos inconsistentes donde visitas in puede ser > visitas\n",
    "    df_clacom_all = matriz_max_entropia(df_data_visitas_i, lista_clacom)\n",
    "    \n",
    "    df_clacom_all['INCLUIR'] = True\n",
    "    df_clacom_all = df_clacom_all.merge(df_i, on = 'INCLUIR', how = 'left')\n",
    "    \n",
    "    df_clacom_all = df_clacom_all[df_clacom_all['M'] > 0]\n",
    "    df_clacom_all = df_clacom_all[campos_beta + [campo_last_touch] + [f'{j}_INICIAL' for j in lista_clacom] + [f'{j}_FINAL' for j in lista_clacom] + ['M']]\n",
    "    df_clacom_all['DATE'] = pd.to_datetime(df_clacom_all['DATE'])\n",
    "    \n",
    "    df_matriz_entropia = pd.concat([df_clacom_all, df_matriz_entropia]).reset_index(drop = True) # Prioridad al nuevo registro\n",
    "    df_matriz_entropia = df_matriz_entropia.drop_duplicates(subset = subset, keep = 'first').reset_index(drop = True) # Todo actualizado. guardar según paso\n",
    "    \n",
    "    if ((i + 1) % paso == 0) or (i == len(df_iterador) - 1):\n",
    "        print('Guardar')\n",
    "        tf.parquet_act(f'{dir_data_horus}df_matriz_entropia', df_matriz_entropia, 'save') # Se guarda el resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parámetro matriz entropía"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matriz_entropia = tf.parquet_act(f'{dir_data_horus}df_matriz_entropia')\n",
    "df_matriz_entropia\n",
    "\n",
    "df_dates['IN'] = True\n",
    "df_matriz_entropia['DATE'] = pd.to_datetime(df_matriz_entropia['DATE'])\n",
    "df_matriz_entropia = df_matriz_entropia.merge(df_dates, on = 'DATE', how = 'left')\n",
    "df_matriz_entropia['IN'] = df_matriz_entropia['IN'].fillna(False)\n",
    "df_matriz_entropia = df_matriz_entropia[df_matriz_entropia['IN']].reset_index(drop = True)\n",
    "df_matriz_entropia = df_matriz_entropia.drop(columns = ['IN'])\n",
    "df_matriz_entropia = df_matriz_entropia[df_matriz_entropia['PAIS'].isin(paises)]\n",
    "\n",
    "df_matriz_entropia = df_matriz_entropia.rename(columns = {'DATE': 'DATE_COMPARACION'})\n",
    "df_dias_predecir = df_dias_predecir[['DATE', 'PAIS', 'DATE_COMPARACION']]\n",
    "df_dias_predecir['DATE_COMPARACION'] = pd.to_datetime(df_dias_predecir['DATE_COMPARACION'])\n",
    "df_dias_predecir['DATE'] = pd.to_datetime(df_dias_predecir['DATE'])\n",
    "\n",
    "df_matriz_entropia = df_matriz_entropia.merge(df_dias_predecir, on = ['PAIS', 'DATE_COMPARACION'], how = 'left')\n",
    "df_matriz_entropia = df_matriz_entropia[df_matriz_entropia['DATE'].notna()].reset_index(drop = True) # No interesan los dates fuera del horizonte\n",
    "df_matriz_entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if modo_tactico:\n",
    "    df_matriz_entropia = df_matriz_entropia.rename(columns = {'FAMILIA_INICIAL': 'FAMILIA'})\n",
    "    df_matriz_entropia = df_matriz_entropia.merge(df_data_visitas.rename(columns = {'DATE': 'DATE_COMPARACION'}), how = 'left', on = ['PAIS', 'DATE_COMPARACION', 'CANAL', 'FUENTE'] + campos_grano)\n",
    "    df_matriz_entropia['M_VISITAS_IN'] = df_matriz_entropia['M'] * df_matriz_entropia['VISITAS']\n",
    "    df_matriz_entropia['PERIODO'] = df_matriz_entropia['DATE'].astype(str).str[:7]\n",
    "    df_matriz_entropia = df_matriz_entropia.rename(columns = {'FAMILIA': 'FAMILIA_INICIAL'})\n",
    "    df_matriz_entropia = df_matriz_entropia[campos_beta_def + [campo_last_touch] + ['FAMILIA_INICIAL', 'FAMILIA_FINAL', 'M_VISITAS_IN', 'VISITAS_IN']].groupby(campos_beta_def + [campo_last_touch] + ['FAMILIA_INICIAL', 'FAMILIA_FINAL'], as_index = False).sum()\n",
    "    df_matriz_entropia['M'] = df_matriz_entropia['M_VISITAS_IN'] / df_matriz_entropia['VISITAS_IN']\n",
    "    df_matriz_entropia['M'] = np.where(df_matriz_entropia['M'] > 1, 1, df_matriz_entropia['M'])\n",
    "    df_matriz_entropia = df_matriz_entropia.drop(columns = ['M_VISITAS_IN', 'VISITAS_IN'])\n",
    "df_matriz_entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros['RELACION_VISITAS'] = df_matriz_entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matriz_entropia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, '1.3.5 Pars 10', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Parámetros Visitas In Pago e Inversión [6-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_in = df_beta.copy()\n",
    "df_beta_in['IN'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_visitas = tf.parquet_act(f'{dir_data_horus}df_data_visitas') # (Histórica) Se lee y se filtra df_data_visitas # este para inversion 250109 (Histórica)\n",
    "df_data_visitas['DATE'] = pd.to_datetime(df_data_visitas['DATE'])\n",
    "#df_data_visitas = df_data_visitas[df_data_visitas['PAIS'].isin(paises)]\n",
    "#df_data_visitas = df_data_visitas[(df_data_visitas['DATE'] >= str(dia_desde)) & (df_data_visitas['DATE'] <= str(dia_hasta))].reset_index(drop = True)\n",
    "df_data_visitas = df_data_visitas.merge(df_beta_in, on = campos_beta, how = 'left')\n",
    "df_data_visitas['IN'] = df_data_visitas['IN'].fillna(False)\n",
    "df_data_visitas = df_data_visitas[df_data_visitas['IN']].reset_index(drop = True)\n",
    "df_data_visitas = df_data_visitas.drop(columns = ['IN'])\n",
    "df_data_visitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.parquet_act(f'{dir_data}Parametros/P4').DATE.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tf.parquet_act(f'{dir_data}Parametros/P4')\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.merge(df_beta_in, on = campos_beta, how = 'left')\n",
    "df['IN'] = df['IN'].fillna(False)\n",
    "df = df[df['IN']]\n",
    "df = df.drop(columns = ['IN'])\n",
    "if len(df_data_visitas) > 0:  # Solo si hay data histórica.Si el horizonte es futuro, entonces df_data_visitas = vacío\n",
    "    df = df[df['DATE'] > df_data_visitas.DATE.max()].reset_index(drop = True) # Solo para días que no existan en los reales\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit(\"Eliminar acá abajo df = df[df['RANGO'] <= 3].reset_index(drop = True)..small batch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['RANGO'] <= 2].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limite = 20\n",
    "if version_simplificada:\n",
    "    limite = 2\n",
    "\n",
    "df_rangos = pd.DataFrame({'RANGO': [i for i in range(1, limite + 1)]})\n",
    "df_rangos['AUX'] = 'aux'\n",
    "df_rangos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depende del caso la union del real con el proyectado\n",
    "\n",
    "if modo_tactico:\n",
    "    # SI es modo tactico, se agrupa por periodo y se le suma la data real a la misma base\n",
    "    df['PERIODO'] = df['DATE'].astype(str).str[:7]\n",
    "    df_data_visitas['PERIODO'] = df_data_visitas['DATE'].astype(str).str[:7]\n",
    "    df_data_visitas = df_data_visitas[campos_beta_def + campos_grano + ['VISITAS_IN']].groupby(campos_beta_def + campos_grano, as_index = False).sum()\n",
    "    df_representantes_base['PERIODO'] = df_representantes_base['DATE'].astype(str).str[:7]\n",
    "    df_inversion = df_representantes_base[campos_beta_def + campos_grano + ['INVERSION']].groupby(campos_beta_def + campos_grano, as_index = False).sum()\n",
    "    \n",
    "    df = df[campos_beta_def + campos_grano + ['RANGO', 'I_0', 'I_F'] + ['INVERSION', 'VISITAS_PAGO', 'VP0', 'VPF', 'A', 'B']].groupby(campos_beta_def + campos_grano + ['RANGO', 'I_0', 'I_F'], as_index = False).sum()\n",
    "\n",
    "    df_real = df_data_visitas.merge(df_inversion, on = campos_beta_def + campos_grano, how = 'left')\n",
    "    df_real = df_real.rename(columns = {'VISITAS_IN': 'VISITAS_0', 'INVERSION': 'INVERSION_0'})\n",
    "    \n",
    "    df = df.merge(df_real, on = campos_beta_def + campos_grano, how = 'left')\n",
    "    df[['INVERSION_0', 'VISITAS_0']] = df[['INVERSION_0', 'VISITAS_0']].fillna(0)\n",
    "\n",
    "    df['I_0'] += df['INVERSION_0']\n",
    "    df['I_F'] += df['INVERSION_0']\n",
    "    df['INVERSION'] += df['INVERSION_0']\n",
    "    df['VISITAS_PAGO'] += df['VISITAS_0']\n",
    "    df['VP0'] += df['VISITAS_0']\n",
    "    df['VPF'] += df['VISITAS_0']\n",
    "    df['B'] += (df['VISITAS_0'] - df['A'] * df['INVERSION_0'])\n",
    "\n",
    "    df['K0'] = abs(df['A'] * df['I_0'] + df['B'] - df['VP0'])\n",
    "    df['KF'] = abs(df['A'] * df['I_F'] + df['B'] - df['VPF'])\n",
    "    \n",
    "    df_valid = df[(df['K0'] >= 0.0001) | (df['KF'] >= 0.0001)]\n",
    "    if len(df_valid) > 0:\n",
    "        print('ERROR')\n",
    "        display(df_valid)\n",
    "        sys.exit('Error en la validación de los datos')\n",
    "    \n",
    "    df = df.drop(columns = ['K0', 'KF'])\n",
    "\n",
    "else:\n",
    "    df_inversion = df_representantes_base[campos_beta + campos_grano + ['INVERSION']].groupby(campos_beta + campos_grano, as_index = False).sum()\n",
    "    df_data_visitas = df_data_visitas[campos_beta + campos_grano + ['VISITAS_IN']].groupby(campos_beta + campos_grano, as_index = False).sum()\n",
    "    df_real = df_data_visitas.merge(df_inversion, on = campos_beta + campos_grano, how = 'left').rename(columns = {'VISITAS_IN': 'VISITAS_PAGO'})\n",
    "    df_real['I_0'], df_real['I_F'] = df_real['INVERSION'], df_real['INVERSION']\n",
    "    df_real['VP0'], df_real['VPF'] = df_real['VISITAS_PAGO'], df_real['VISITAS_PAGO']\n",
    "    df_real['A'] = 0\n",
    "    df_real['B'] = df_real['VISITAS_PAGO']\n",
    "    df_real['AUX'] = 'aux'\n",
    "    df_real = df_real.merge(df_rangos, on = 'AUX')\n",
    "    df_real = df_real.drop(columns = ['AUX'])\n",
    "    df_real # No importa el rango, ya está predefinida la inversión y las visitas (reales)\n",
    "    df = pd.concat([df, df_real]).reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[campos_beta_def + campos_grano + ['RANGO', 'I_0', 'I_F', 'VP0', 'VPF', 'A', 'B']]\n",
    "#df = df[(df['DATE'] >= dias_proyeccion[0]) & (df['DATE'] <= dias_proyeccion[1])].reset_index(drop = True)\n",
    "#if modo_tactico:\n",
    "#    df['PERIODO'] = df['DATE'].astype(str).str[:7]\n",
    "#df = df[campos_beta_def + campos_grano + ['RANGO', 'I_0', 'I_F', 'A', 'B']].groupby(campos_beta_def + campos_grano + ['RANGO', 'I_0', 'I_F'], as_index = False).sum()\n",
    "#df['VP0'], df['VPF'] = df['I_0'] * df['A'] + df['B'], df['I_F'] * df['A'] + df['B']\n",
    "\n",
    "df_rango = df[['RANGO']].drop_duplicates().reset_index(drop = True)\n",
    "df_rango['AUX'] = 'aux'\n",
    "df_beta_alpha_grano_pago['AUX'] = 'aux'\n",
    "df_beta_alpha_grano_pago_rango = df_beta_alpha_grano_pago.merge(df_rango, on = 'AUX')\n",
    "df_beta_alpha_grano_pago_rango = df_beta_alpha_grano_pago_rango.drop(columns = ['AUX'])\n",
    "df_beta_alpha_grano_pago = df_beta_alpha_grano_pago.drop(columns = ['AUX'])\n",
    "df = df_beta_alpha_grano_pago_rango.merge(df, on = campos_beta_def + campos_grano + ['RANGO'], how = 'left')\n",
    "df = df.reset_index(drop = True)  \n",
    "dic_parametros['VISITAS_PAGO'] = df\n",
    "df\n",
    "\n",
    "df_times, t0 = medir_tiempo(t0, '1.3.2 Pars 6-7', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_alpha_grano_pago_rango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit('Revisar tiempos')\n",
    "#sys.exit('Atención abajo en restricciones > declarar días historicos cuando sea requerido (como en inversion y visitas in), y no cuando ya esté integraddo en el parámetro')\n",
    "# como el caso de sumas organicas y pagas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mip.Model(\"Horus\", sense = mip.MAXIMIZE) # Se crea el modelo de optimización (maximización)\n",
    "\n",
    "# , log_level=2 muestra info en el model.optimize() ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_alpha_grano_organico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_alpha_grano_organico['NAME'] = ''\n",
    "df_beta_alpha_grano_pago['NAME'] = ''\n",
    "\n",
    "for c in campos_beta_def + campos_grano:\n",
    "    df_beta_alpha_grano_organico['NAME'] += (df_beta_alpha_grano_organico[c].astype(str) + '_')\n",
    "    df_beta_alpha_grano_pago['NAME'] += (df_beta_alpha_grano_pago[c].astype(str) + '_')\n",
    "\n",
    "df_beta_alpha_grano_organico['NAME'] = df_beta_alpha_grano_organico['NAME'].str[:-1]\n",
    "df_beta_alpha_grano_pago['NAME'] = df_beta_alpha_grano_pago['NAME'].str[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_beta_duplicacion['NAME'] = ''\n",
    "for c in ['DUPLICACION'] + campos_beta_def + ['FAMILIA', 'SUBFAMILIA'] + [campo_last_touch, 'NATURALEZA_MEDIO']:\n",
    "    df_alpha_beta_duplicacion['NAME'] += (df_alpha_beta_duplicacion[c].astype(str) + '_')\n",
    "df_alpha_beta_duplicacion['NAME'] = df_alpha_beta_duplicacion['NAME'].str[:-1]\n",
    "df_alpha_beta_duplicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_variables = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M Vars Ini', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Venta Colocada [1-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_beta_duplicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alpha_beta_duplicacion.DUPLICACION.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SO (Sales Orgánicas)\n",
    "\n",
    "# Crear variables para cada fila y almacenarlas en una nueva columna\n",
    "df_SO = df_beta_alpha_grano_organico.copy()\n",
    "df_SO['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'SO_{name}') for name in df_beta_alpha_grano_organico['NAME'].values]\n",
    "display(df_SO)\n",
    "\n",
    "# SP (Sales Pagas)\n",
    "df_SP = df_beta_alpha_grano_pago.copy()\n",
    "df_SP['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'SP_{name}') for name in df_beta_alpha_grano_pago['NAME'].values]\n",
    "display(df_SP)\n",
    "\n",
    "# ST (Sales Totales)\n",
    "df_ST = df_alpha_beta_duplicacion.copy()\n",
    "df_ST['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'ST_{name}') for name in df_alpha_beta_duplicacion['NAME'].values]\n",
    "display(df_ST)\n",
    "\n",
    "# Venta neta\n",
    "df_ST_net = df_alpha_beta_duplicacion.copy()\n",
    "df_ST_net['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'ST_net_{name}') for name in df_alpha_beta_duplicacion['NAME'].values]\n",
    "\n",
    "\n",
    "dic_variables['Venta'] = {'SO': df_SO, 'SP': df_SP, 'ST': df_ST, 'ST_net': df_ST_net}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M Vars VC', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Órdenes Colocadas [4-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OO (Ordenes Orgánicas)\n",
    "df_OO = df_beta_alpha_grano_organico.copy()\n",
    "df_OO['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'OO_{name}') for name in df_beta_alpha_grano_organico['NAME'].values]\n",
    "display(df_OO)\n",
    "\n",
    "# OP (Ordenes Pagas)\n",
    "df_OP = df_beta_alpha_grano_pago.copy()\n",
    "df_OP['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'OP_{name}') for name in df_beta_alpha_grano_pago['NAME'].values]\n",
    "display(df_OP)\n",
    "\n",
    "# OT (Ordenes Totales)\n",
    "df_OT = df_alpha_beta_duplicacion.copy()\n",
    "df_OT['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'OT_{name}') for name in df_alpha_beta_duplicacion['NAME'].values]\n",
    "display(df_OT)\n",
    "\n",
    "dic_variables['Ordenes'] = {'OO': df_OO, 'OP': df_OP, 'OT': df_OT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M Vars OC', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visitas [7-9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VO (Visitas Orgánicas)\n",
    "df_VO = df_beta_alpha_grano_organico.copy()\n",
    "df_VO['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'VO_{name}') for name in df_beta_alpha_grano_organico['NAME'].values]\n",
    "display(df_VO)\n",
    "\n",
    "# VP (Visitas Pagas)\n",
    "df_VP = df_beta_alpha_grano_pago.copy()\n",
    "df_VP['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'VP_{name}') for name in df_beta_alpha_grano_pago['NAME'].values]\n",
    "display(df_VP)\n",
    "\n",
    "# VP (Visitas Pagas IN)\n",
    "df_VP_in = df_beta_alpha_grano_pago.copy()\n",
    "df_VP_in['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'VP_in_{name}') for name in df_beta_alpha_grano_pago['NAME'].values]\n",
    "display(df_VP_in)\n",
    "\n",
    "# VT (Visitas Totales)\n",
    "df_VT = df_alpha_beta_duplicacion.copy()\n",
    "df_VT['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'VT_{name}') for name in df_alpha_beta_duplicacion['NAME'].values]\n",
    "display(df_VT)\n",
    "\n",
    "dic_variables['Visitas'] = {'VO': df_VO, 'VP': df_VP, 'VP_in': df_VP_in, 'VT': df_VT}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M Vars Vis', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversión [10-11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversion (para grano)\n",
    "df_Inv = df_beta_alpha_grano_pago.copy()\n",
    "df_Inv['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'Inv_{name}') for name in df_beta_alpha_grano_pago['NAME'].values]\n",
    "display(df_Inv)\n",
    "\n",
    "# Inv para todos los niveles de duplicación\n",
    "df_Inv_all = df_alpha_beta_duplicacion[df_alpha_beta_duplicacion['NATURALEZA_MEDIO'].isin(['Pago', '']).reset_index(drop = True)]\n",
    "df_Inv_all['X'] = [model.add_var(var_type = 'C', lb = 0, name = f'Inv_all_{name}') for name in df_Inv_all['NAME'].values]\n",
    "display(df_Inv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = dic_parametros['VISITAS_PAGO']\n",
    "df_y = df_y[campos_beta_def + campos_grano + ['RANGO']].drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "df_y['NAME'] = ''\n",
    "for c in campos_beta_def + campos_grano + ['RANGO']:\n",
    "    df_y['NAME'] += (df_y[c].astype(str) + '_')\n",
    "df_y['NAME'] = df_y['NAME'].str[:-1]\n",
    "df_y['X'] = [model.add_var(var_type = 'B', name = f'Y_{name}') for name in df_y['NAME'].values] # Binario del rango seleccionado\n",
    "df_y\n",
    "\n",
    "df_Inv_rango = df_y[campos_beta_def + campos_grano + ['RANGO', 'NAME']]\n",
    "df_Inv_rango['X'] = [model.add_var(var_type = 'C', name = f'Inv_rango_{name}') for name in df_Inv_rango['NAME'].values]\n",
    "\n",
    "dic_variables['Inversion'] = {'Inv': df_Inv, 'Inv_all': df_Inv_all, 'Y': df_y, 'Inv_rango': df_Inv_rango}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_variables['Inversion']['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M Vars Inv', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función Objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escribir_modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probar_factibilidad(model):\n",
    "    model.max_gap = 0.01\n",
    "    status = model.optimize(max_seconds = 300, relax = True, msg=True) # Primero prueba factibilidad\n",
    "\n",
    "    print('Relaxed')\n",
    "    display(status)\n",
    "    display(model.objective_value)\n",
    "    return None\n",
    "\n",
    "def escribir_modelo_opt(activar, archivo, modo, linea):\n",
    "    if activar:\n",
    "        with open(archivo, modo) as writefile:\n",
    "            writefile.write(linea)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasas de cambio\n",
    "\n",
    "query = \"SELECT * FROM `sod-corp-plp-beta.ETL_performance_2023.exrate` WHERE DATE = (select max(date) from `sod-corp-plp-beta.ETL_performance_2023.exrate`)\"\n",
    "\n",
    "df_exrate = tf.request_GCP_vnew(\n",
    "                            nombre_tabla = \"\",\n",
    "                            specific_query = query,\n",
    "                            client = client,\n",
    "                            output = False, permitir_fallos = False)\n",
    "\n",
    "df_exrate = df_exrate[['PAIS', 'EXRATE']]\n",
    "\n",
    "# Función objetivo\n",
    "\n",
    "# Venta Neta\n",
    "df_venta_neta = dic_variables['Venta']['ST_net']\n",
    "df_venta_neta_total = df_venta_neta[df_venta_neta['DUPLICACION'] == 'TOTALES'].reset_index(drop = True)\n",
    "df_venta_neta_total = df_venta_neta_total.merge(df_exrate, on = 'PAIS', how = 'left')\n",
    "df_venta_neta_total['EXRATE'] = round(df_venta_neta_total['EXRATE'], 3)\n",
    "df_venta_neta_total['VENTA_NETA_USD'] = df_venta_neta_total['X'] / df_venta_neta_total['EXRATE']\n",
    "VN_USD = sum(df_venta_neta_total['VENTA_NETA_USD'])\n",
    "\n",
    "\n",
    "df_inversion = dic_variables['Inversion']['Inv']\n",
    "df_inversion = df_inversion.merge(df_exrate, on = 'PAIS', how = 'left')\n",
    "df_inversion['EXRATE'] = round(df_inversion['EXRATE'], 3)\n",
    "df_inversion['INVERSION_USD'] = df_inversion['X'] / df_inversion['EXRATE']\n",
    "INV_USD = sum(df_inversion['INVERSION_USD'])\n",
    "\n",
    "Z = VN_USD - INV_USD\n",
    "\n",
    "model.objective = mip.maximize(Z)\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo, archivo = f'Modelo de optimizacion.txt', modo = 'w', linea = f'Función Objetivo \\n')\n",
    "escribir_modelo_opt(activar = escribir_modelo, archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'----------------  \\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "escribir_modelo_opt(activar = escribir_modelo, archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'Z: {Z}  \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M FO', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restricciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Grano orgánico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El grano orgánico, viene de los parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'Restricciones\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Visitas Orgánicas\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR1a\\n\\n')\n",
    "\n",
    "df_R1a = dic_variables['Visitas']['VO'].merge(dic_parametros['VO'], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R1a = df_R1a.rename(columns = {'y': 'VO'})\n",
    "df_R1a = df_R1a.fillna(0)\n",
    "df_R1a['VO'] = round(df_R1a['VO'], 3)\n",
    "\n",
    "if activar_holguras:\n",
    "    df_R1a['H1a_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H1a_p_{name}') for name in df_R1a['NAME'].values]\n",
    "    df_R1a['H1a_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H1a_n_{name}') for name in df_R1a['NAME'].values]\n",
    "    #H1a_p = model.add_var(var_type = 'C', lb = 0, name = f'H1a_p') # Holgura positiva\n",
    "    #H1a_n = model.add_var(var_type = 'C', lb = 0, name = f'H1a_p') # Holgura negativa\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R1a))):\n",
    "    \n",
    "    #if version_simplificada:\n",
    "    #    break\n",
    "    \n",
    "    name = df_R1a[\"NAME\"][i]\n",
    "\n",
    "    if activar_holguras:\n",
    "        #R1a = (df_R1a['X'][i] == df_R1a['VO'][i] + H1a_p - H1a_n)#, f\"R1_{df_R1['NAME'][i]}\"\n",
    "        R1a = (df_R1a['X'][i] == round(df_R1a['VO'][i], 3) + df_R1a['H1a_p'][i] - df_R1a['H1a_n'][i])\n",
    "    else:\n",
    "        R1a = (df_R1a['X'][i] == round(df_R1a['VO'][i], 3))\n",
    "        \n",
    "    model += R1a\n",
    "    \n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R1a({name}): {R1a}  \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Ordenes Orgánicas\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR1b\\n\\n')\n",
    "\n",
    "df_R1b = dic_variables['Ordenes']['OO'].merge(dic_parametros['VO'], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R1b = df_R1b.rename(columns = {'y': 'VO'})\n",
    "df_R1b = df_R1b.merge(dic_parametros['TCO'], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R1b = df_R1b.rename(columns = {'y': 'TCO'})\n",
    "df_R1b = df_R1b.fillna(0)\n",
    "df_R1b['OO'] = df_R1b['VO'] * df_R1b['TCO']\n",
    "df_R1b['OO'] = round(df_R1b['OO'], 3)\n",
    "\n",
    "if activar_holguras:\n",
    "\n",
    "    print('Si')\n",
    "    df_R1b['H1b_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H1b_p_{name}') for name in df_R1b['NAME'].values]\n",
    "    df_R1b['H1b_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H1b_n_{name}') for name in df_R1b['NAME'].values]\n",
    "    #H1b_p = model.add_var(var_type = 'C', lb = 0, name = f'H1b_p') # Holgura positiva\n",
    "    #H1b_n = model.add_var(var_type = 'C', lb = 0, name = f'H1b_n') # Holgura negativa\n",
    "    \n",
    "for i in tqdm.tqdm(range(len(df_R1b))):\n",
    "\n",
    "    #if version_simplificada:\n",
    "    #    break\n",
    "    \n",
    "    name = df_R1b[\"NAME\"][i]\n",
    "\n",
    "    if activar_holguras:\n",
    "        #R1b = (df_R1b['X'][i] == df_R1b['OO'][i] + H1b_p + H1b_n)#, f\"R1_{df_R1['NAME'][i]}\"\n",
    "        R1b = (df_R1b['X'][i] == df_R1b['OO'][i] + df_R1b['H1b_p'][i] - df_R1b['H1b_n'][i])\n",
    "    else: \n",
    "        R1b = (df_R1b['X'][i] == df_R1b['OO'][i])\n",
    "        \n",
    "    model += R1b\n",
    "    \n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R1b({name}): {R1b}  \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Ventas Orgánicas\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR1c\\n\\n')\n",
    "\n",
    "df_R1c = dic_variables['Venta']['SO'].merge(dic_parametros['VO'], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R1c = df_R1c.rename(columns = {'y': 'VO'})\n",
    "df_R1c = df_R1c.merge(dic_parametros['TCO'], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R1c = df_R1c.rename(columns = {'y': 'TCO'})\n",
    "df_R1c = df_R1c.merge(dic_parametros['TPO'], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R1c = df_R1c.rename(columns = {'y': 'TPO'})\n",
    "df_R1c = df_R1c.fillna(0)\n",
    "df_R1c['SO'] = df_R1c['VO'] * df_R1c['TCO'] * df_R1c['TPO']\n",
    "df_R1c['SO'] = round(df_R1c['SO'], 3)\n",
    "\n",
    "if activar_holguras:\n",
    "    #H1c_p = model.add_var(var_type = 'C', lb = 0, name = f'H1c_p') # Holgura positiva\n",
    "    #H1c_n = model.add_var(var_type = 'C', lb = 0, name = f'H1c_n') # Holgura negativa\n",
    "    df_R1c['H1c_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H1c_p_{name}') for name in df_R1c['NAME'].values]\n",
    "    df_R1c['H1c_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H1c_n_{name}') for name in df_R1c['NAME'].values]\n",
    "    \n",
    "    \n",
    "for i in tqdm.tqdm(range(len(df_R1c))):\n",
    "    \n",
    "    name = df_R1c[\"NAME\"][i]\n",
    "    \n",
    "    if activar_holguras:\n",
    "        #R1c = (df_R1c['X'][i] == df_R1c['SO'][i] + H1c_p - H1c_n)\n",
    "        R1c = (df_R1c['X'][i] == df_R1c['SO'][i] + df_R1c['H1c_p'][i] - df_R1c['H1c_n'][i])\n",
    "    else:\n",
    "        R1c = (df_R1c['X'][i] == df_R1c['SO'][i])\n",
    "    model += R1c\n",
    "    \n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R1c({name}): {R1c}  \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R1', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Grano pago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se incluyen visitas en este caso y, a diferencia de orgánico, no se generan las variables como producto de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Ordenes Pago\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR2a\\n\\n')\n",
    "\n",
    "df_R2a = dic_variables['Ordenes']['OP'].rename(columns = {'X': 'X_OP'})\n",
    "df_R2a = df_R2a.merge(dic_variables['Visitas']['VP'], on = campos_beta_def + campos_grano + ['NAME'], how = 'left').rename(columns = {'X': 'X_VP'})\n",
    "df_R2a = df_R2a.merge(dic_parametros['TCP'], on = campos_beta_def + campos_grano, how = 'left').rename(columns = {'y': 'TCP'})\n",
    "df_R2a['TCP'] = df_R2a['TCP'].fillna(0)\n",
    "df_R2a['TCP'] = round(df_R2a['TCP'], 3)\n",
    "\n",
    "if activar_holguras:\n",
    "    df_R2a['H2a_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H2a_p_{name}') for name in df_R2a['NAME'].values]\n",
    "    df_R2a['H2a_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H2a_n_{name}') for name in df_R2a['NAME'].values]\n",
    "                       \n",
    "    #H2a_p = model.add_var(var_type = 'C', lb = 0, name = f'H2a_p') # Holgura positiva\n",
    "    #H2a_n = model.add_var(var_type = 'C', lb = 0, name = f'H2a_n') # Holgura negativa\n",
    "    \n",
    "for i in tqdm.tqdm(range(len(df_R2a))):\n",
    "    \n",
    "    name = df_R2a[\"NAME\"][i]\n",
    "    \n",
    "    if activar_holguras:\n",
    "        #R2a = (df_R2a['X_OP'][i] == df_R2a['X_VP'][i] * df_R2a['TCP'][i] + H2a_p - H2a_n)\n",
    "        R2a = (df_R2a['X_OP'][i] == df_R2a['X_VP'][i] * df_R2a['TCP'][i] + df_R2a['H2a_p'][i] - df_R2a['H2a_n'][i])\n",
    "    else:\n",
    "        R2a = (df_R2a['X_OP'][i] == df_R2a['X_VP'][i] * df_R2a['TCP'][i])\n",
    "    model += R2a\n",
    "\n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R2a({name}): {R2a}  \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Venta Pago\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR2b\\n\\n')\n",
    "\n",
    "df_R2b = dic_variables['Venta']['SP'].rename(columns = {'X': 'X_SP'})\n",
    "df_R2b = df_R2b.merge(dic_variables['Ordenes']['OP'], on = campos_beta_def + campos_grano + ['NAME'], how = 'left').rename(columns = {'X': 'X_OP'})\n",
    "df_R2b = df_R2b.merge(dic_parametros['TPP'], on = campos_beta_def + campos_grano, how = 'left').rename(columns = {'y': 'TPP'})\n",
    "df_R2b['TPP'] = df_R2b['TPP'].fillna(0)\n",
    "df_R2b['TPP'] = round(df_R2b['TPP'], 3)\n",
    "\n",
    "if activar_holguras:\n",
    "    #H2b_p = model.add_var(var_type = 'C', lb = 0, name = f'H2b_p') # Holgura positiva\n",
    "    #H2b_n = model.add_var(var_type = 'C', lb = 0, name = f'H2b_n') # Holgura negativa\n",
    "    df_R2b['H2b_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H2b_p_{name}') for name in df_R2b['NAME'].values]\n",
    "    df_R2b['H2b_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H2b_n_{name}') for name in df_R2b['NAME'].values]\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R2b))):\n",
    "    \n",
    "    name = df_R2b[\"NAME\"][i]\n",
    "    \n",
    "    if activar_holguras:\n",
    "        #R2b = (df_R2b['X_SP'][i] == df_R2b['X_OP'][i] * df_R2b['TPP'][i] + H2b_p - H2b_n)\n",
    "        R2b = (df_R2b['X_SP'][i] == df_R2b['X_OP'][i] * df_R2b['TPP'][i] + df_R2b['H2b_p'][i] - df_R2b['H2b_n'][i])\n",
    "    else:\n",
    "        R2b = (df_R2b['X_SP'][i] == df_R2b['X_OP'][i] * df_R2b['TPP'][i])\n",
    "    model += R2b\n",
    "\n",
    "    if i < escribir_modelo[1]:\n",
    "        #print(str(R2b))\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R2b({name}): {R2b}  \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R2', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inversión & Tráfico Pago IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dic_parametros['VISITAS_PAGO']\n",
    "df = df.sort_values(by = campos_beta_def + campos_grano + ['RANGO']).reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MIN_VIS_IN'] = df['A'] * df['I_0'] + df['B']\n",
    "df['MAX_VIS_IN'] = df['A'] * df['I_F'] + df['B']\n",
    "\n",
    "# Vis = A * I + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR3a: Para cada combinación, la suma de y en los rangos tiene que ser = 1...esto es, y solo puede existir en uno y solo un rango\\n\\n')\n",
    "\n",
    "lista_rangos = list(df_rango.RANGO.unique())\n",
    "\n",
    "# Variable Y\n",
    "df_y = dic_variables['Inversion']['Y']\n",
    "\n",
    "# Para cada combinación, la suma de y en los rangos tiene que ser = 1...esto es, y solo puede existir en uno y solo un rango\n",
    "# Pivotear el DataFrame\n",
    "df_y = df_y.pivot_table(\n",
    "    index = campos_beta_def + campos_grano,  # Mantener estas columnas\n",
    "    columns = 'RANGO',  # RANGO como columnas\n",
    "    values = 'X',  # Usar X como valores\n",
    "    aggfunc = 'first'  # Tomar el primer valor en caso de duplicados\n",
    ").reset_index()\n",
    "\n",
    "df_y\n",
    "\n",
    "df_y['NAME'] = ''\n",
    "for c in campos_beta_def + campos_grano:\n",
    "    df_y['NAME'] += (df_y[c].astype(str) + '_')\n",
    "df_y['NAME'] = df_y['NAME'].str[:-1]\n",
    "\n",
    "if activar_holguras:\n",
    "    #df_R3b['H3b1'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3b1{name}') for name in df_R3b['NAME'].values]\n",
    "    df_y['H3a_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3a_p_{name}') for name in df_y['NAME'].values]\n",
    "    df_y['H3a_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3a_n_{name}') for name in df_y['NAME'].values]\n",
    "\n",
    "# Esta restricción, no tiene holguras\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_y))):\n",
    "    \n",
    "    name = df_y[\"NAME\"][i]\n",
    "    #R3a = ((df_R2b['X_SP'][i] == df_R2b['X_OP'][i] * df_R2b['TPP'][i]))\n",
    "    s = 0\n",
    "    for c in lista_rangos:\n",
    "        s += df_y[c][i]\n",
    "        \n",
    "    if activar_holguras:\n",
    "        R3a = (s == 1 + df_y['H3a_p'][i] - df_y['H3a_n'][i])\n",
    "    else:\n",
    "        R3a = s == 1\n",
    "    model += R3a\n",
    "\n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3a({name}): {R3a}  \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R3a', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R3b: El rango seleccionado depende de la inversión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Inv\n",
    "df_Inv = dic_variables['Inversion']['Inv_rango']\n",
    "df_Inv = df_Inv[campos_beta_def + campos_grano + ['RANGO', 'X']].rename(columns = {'X': 'Inv_rango'})\n",
    "display(df_Inv.head())\n",
    "\n",
    "df_y_inv = dic_variables['Inversion']['Y']\n",
    "df_y_inv = df_y_inv[campos_beta_def + campos_grano + ['RANGO', 'X']].rename(columns = {'X': 'X_Y'})\n",
    "display(df_y_inv.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R3b = df_y_inv.merge(df_Inv, on = campos_beta_def + campos_grano + ['RANGO'], how = 'left')\n",
    "df_R3b = df_R3b.merge(df[campos_beta_def + campos_grano + ['RANGO'] + ['I_0', 'I_F']], on = campos_beta_def + campos_grano + ['RANGO'], how = 'left')\n",
    "df_R3b\n",
    "\n",
    "df_R3b['NAME'] = ''\n",
    "for c in campos_beta_def + campos_grano + ['RANGO']:\n",
    "    df_R3b['NAME'] += (df_R3b[c].astype(str) + '_')\n",
    "df_R3b['NAME'] = df_R3b['NAME'].str[:-1]\n",
    "\n",
    "df_R3b = df_R3b.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b1 y 3b2: I >= min * y & I <= max *y\n",
    "\n",
    "if activar_holguras:\n",
    "    #H3b1 = model.add_var(var_type = 'C', lb = 0, name = f'H3b1') # Holgura positiva\n",
    "    #H3b2 = model.add_var(var_type = 'C', lb = 0, name = f'H3b2') # Holgura negativa\n",
    "    \n",
    "    df_R3b['H3b1'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3b1_{name}') for name in df_R3b['NAME'].values] # Holgura positiva\n",
    "    df_R3b['H3b2'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3b2_{name}') for name in df_R3b['NAME'].values] # Holgura negativa\n",
    "\n",
    "#df_R3b['RHS1'] = df_R3b['I_0'] * df_R3b['X_Y']\n",
    "#df_R3b['RHS2'] = df_R3b['I_F'] * df_R3b['X_Y']\n",
    "\n",
    "df_R3b['I_0'], df_R3b['I_F'] = round(df_R3b['I_0'], 3), round(df_R3b['I_F'], 3)\n",
    "\n",
    "df_R3b['RHS1'] = df_R3b['I_0'] * df_R3b['X_Y']\n",
    "if activar_holguras:\n",
    "    df_R3b['RHS1'] -= df_R3b['H3b1']\n",
    "\n",
    "df_R3b['RHS2'] = df_R3b['I_F'] * df_R3b['X_Y']\n",
    "if activar_holguras:\n",
    "    df_R3b['RHS2'] += df_R3b['H3b2']\n",
    "\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR3b\\n\\n')\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R3b))):\n",
    "    name = df_R3b[\"NAME\"][i]\n",
    "    \n",
    "    #if activar_holguras:\n",
    "    #    R3b1 = (df_R3b['Inv_rango'][i] >= df_R3b['RHS1'][i] - H3b1)   \n",
    "    #    R3b2 = (df_R3b['Inv_rango'][i] <= df_R3b['RHS2'][i] + H3b2)\n",
    "    #else:\n",
    "    #    R3b1 = (df_R3b['Inv_rango'][i] >= df_R3b['RHS1'][i])\n",
    "    #    R3b2 = (df_R3b['Inv_rango'][i] <= df_R3b['RHS2'][i])\n",
    " \n",
    "    R3b1 = (df_R3b['Inv_rango'][i] >= df_R3b['RHS1'][i])\n",
    "    R3b2 = (df_R3b['Inv_rango'][i] <= df_R3b['RHS2'][i])\n",
    "    model += R3b1\n",
    "    model += R3b2    \n",
    "    \n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3b1({name}): {R3b1}  \\n')\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3b2({name}): {R3b2}  \\n')\n",
    "\n",
    "\n",
    "\"\"\"escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR3b1\\n\\n')\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R3b))):\n",
    "    \n",
    "    name = df_R3b[\"NAME\"][i]\n",
    "    if activar_holguras:\n",
    "        R3b1 = (df_R3b['Inv_rango'][i] >= df_R3b['I_0'][i] * df_R3b['X_Y'][i] - df_R3b['H3b1'][i])\n",
    "    else:\n",
    "        R3b1 = (df_R3b['Inv_rango'][i] >= df_R3b['I_0'][i] * df_R3b['X_Y'][i])\n",
    "    model += R3b1\n",
    "\n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3b1({name}): {R3b1}  \\n')\n",
    "    \n",
    "# 3b2: I <= max * y\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR3b2\\n\\n')\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R3b))):\n",
    "    \n",
    "    name = df_R3b[\"NAME\"][i]\n",
    "    if activar_holguras:\n",
    "        R3b2 = (df_R3b['Inv_rango'][i] <= df_R3b['I_F'][i] * df_R3b['X_Y'][i] + df_R3b['H3b2'][i])\n",
    "    else:\n",
    "        R3b2 = (df_R3b['Inv_rango'][i] <= df_R3b['I_F'][i] * df_R3b['X_Y'][i])\n",
    "    model += R3b2\n",
    "\n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3b2({name}): {R3b2}  \\n')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R3b 1-2', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b3\n",
    "\n",
    "df_Inv = dic_variables['Inversion']['Inv_rango']\n",
    "\n",
    "# Eliminar este if / else: Dejar solo lo que está en el if\n",
    "if 'X' in df_Inv.columns:\n",
    "    df_Inv = df_Inv[campos_beta_def + campos_grano + ['RANGO', 'X']].rename(columns = {'X': 'Inv_rango'})\n",
    "else:\n",
    "    df_Inv = df_Inv[campos_beta_def + campos_grano + ['RANGO', 'Inv_rango']]\n",
    "    \n",
    "df_Inv_total = dic_variables['Inversion']['Inv']\n",
    "df_Inv_total = df_Inv_total[campos_beta_def + campos_grano + ['X']].rename(columns = {'X': 'Inv_total'})\n",
    "\n",
    "df_R3b3 = df_Inv.merge(df_Inv_total, how = 'left', on = campos_beta_def + campos_grano)\n",
    "#df_R3b3 = df_R3b3[campos_beta + campos_grano + ['Inv_total', 'Inv_rango']].groupby(campos_beta + campos_grano + ['Inv_total'], as_index = False).sum().reset_index(drop = True)\n",
    "\n",
    "#df_names = dic_variables['Inversion']['Inv'][campos_beta + campos_grano + ['NAME']]\n",
    "#df_R3b3 = df_R3b3.merge(df_names, how = 'left', on = campos_beta + campos_grano)\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR3b3\\n\\n')\n",
    "\n",
    "\n",
    "df_inv_total = df_R3b3[['Inv_total']] # 250116\n",
    "df_inv_total['Inv_total_NAME'] = df_inv_total['Inv_total'].astype(str)\n",
    "df_inv_total = df_inv_total.drop_duplicates().reset_index(drop = True)\n",
    "\n",
    "# Group by con name (en los campos del group by tiene no puede haver una var, por eso se hace la reversa después): Con esta solución, Tiempos bajan de 22 mins a 2 seg (99.85%)\n",
    "df_R3b3['Inv_total_NAME'] = df_R3b3['Inv_total'].astype(str)\n",
    "df_R3b3_agr = df_R3b3[campos_beta_def + campos_grano + ['Inv_rango', 'Inv_total_NAME']].groupby(campos_beta_def + campos_grano + ['Inv_total_NAME'], as_index = False).sum().reset_index(drop = True)\n",
    "df_R3b3_agr = df_R3b3_agr.merge(df_inv_total, on = 'Inv_total_NAME', how = 'left')\n",
    "df_R3b3_agr\n",
    "\n",
    "if activar_holguras:\n",
    "    df_R3b3_agr['H3b3_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3b3_p_{name}') for name in df_R3b3_agr['Inv_total_NAME'].values]\n",
    "    df_R3b3_agr['H3b3_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3b3_n_{name}') for name in df_R3b3_agr['Inv_total_NAME'].values]\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R3b3_agr))):\n",
    "    \n",
    "    if activar_holguras:\n",
    "        R3b3 = (df_R3b3_agr['Inv_total'][i] == df_R3b3_agr['Inv_rango'][i] + df_R3b3_agr['H3b3_p'][i] - df_R3b3_agr['H3b3_n'][i])\n",
    "    else:\n",
    "        R3b3 = (df_R3b3_agr['Inv_total'][i] == df_R3b3_agr['Inv_rango'][i])\n",
    "    model += R3b3\n",
    "    \n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3b3({name}): {R3b3}  \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R3b3', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_alpha_grano_pago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop_duplicates(subset = list(set(df.columns) - {'RANGO'})).reset_index(drop = True) # Casos con la inversión fija (A = 0)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c\n",
    "# A * I + B\n",
    "#Límites\n",
    "\n",
    "# df_L: VP_min - max (h) {a(h) + b(h) * I_max}\n",
    "\n",
    "df = df.fillna(0)\n",
    "df_max_I = df[campos_beta_def + campos_grano + ['RANGO', 'I_F']].groupby(campos_beta_def + campos_grano, as_index = False).max().rename(columns = {'I_F': 'I_MAX'})\n",
    "df_VP0_min = df[campos_beta_def + campos_grano + ['RANGO', 'VP0']].groupby(campos_beta_def + campos_grano, as_index = False).min().rename(columns = {'VP0': 'VP0_MIN'})\n",
    "df_L = df[campos_beta_def + campos_grano + ['RANGO', 'A', 'B']].merge(df_max_I[campos_beta_def + campos_grano + ['I_MAX']], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_L = df_L.merge(df_VP0_min[campos_beta_def + campos_grano + ['VP0_MIN']], on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_L['L'] = df_L['VP0_MIN'] - (df_L['A'] * df_L['I_MAX'] + df_L['B'])\n",
    "df_L = df_L[campos_beta_def + campos_grano + ['L']].groupby(campos_beta_def + campos_grano, as_index = False).min()\n",
    "df_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AQUI!!!\n",
    "df_VPF_MAX = df[campos_beta_def + campos_grano + ['VPF']].groupby(campos_beta_def + campos_grano, as_index = False).max().rename(columns = {'VPF': 'VPF_MAX'})\n",
    "df_B_MIN = df[campos_beta_def + campos_grano + ['B']].groupby(campos_beta_def + campos_grano, as_index = False).min().rename(columns = {'B': 'B_MIN'})\n",
    "\n",
    "df_U = df_VPF_MAX.merge(df_B_MIN, on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_U['U'] = df_U['VPF_MAX'] - df_U['B_MIN']\n",
    "df_U = df_U.drop(columns = ['VPF_MAX', 'B_MIN'])\n",
    "df_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R3c = df_R3b[campos_beta_def + campos_grano + ['NAME'] + ['RANGO', 'X_Y']]\n",
    "\n",
    "df_Inv_total = dic_variables['Inversion']['Inv']\n",
    "df_Inv_total = df_Inv_total[campos_beta_def + campos_grano + ['X']].rename(columns = {'X': 'X_Inv'})\n",
    "df_R3c = df_R3c.merge(df_Inv_total, on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R3c = df_R3c.merge(df_L, on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R3c = df_R3c.merge(df_U, on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R3c = df_R3c.merge(df[campos_beta_def + campos_grano + ['RANGO', 'A', 'B']], on = campos_beta_def + campos_grano + ['RANGO'], how = 'left')\n",
    "df_R3c = df_R3c.fillna(0)\n",
    "df_R3c\n",
    "\n",
    "df_VP_in = dic_variables['Visitas']['VP_in'][campos_beta_def + campos_grano + ['X']]\n",
    "df_VP_in = df_VP_in.rename(columns = {'X': 'X_VP_in'})\n",
    "df_R3c = df_R3c.merge(df_VP_in, on = campos_beta_def + campos_grano, how = 'left')\n",
    "df_R3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_R3c1: VP - [A + B*I] <= U * (1-y)\n",
    "df_R3c['A'], df_R3c['B'], df_R3c['U'], df_R3c['L'] = round(df_R3c['A'], 3), round(df_R3c['B'], 3), round(df_R3c['U'], 3), round(df_R3c['L'], 3)\n",
    "\n",
    "df_R3c['LHS'] = df_R3c['X_VP_in'] - (df_R3c['A'] * df_R3c['X_Inv'] + df_R3c['B'])\n",
    "df_R3c['RHSU'] = df_R3c['U'] * (1 - df_R3c['X_Y'])\n",
    "df_R3c['RHSL'] = df_R3c['L'] * (1 - df_R3c['X_Y'])\n",
    "\n",
    "\n",
    "if activar_holguras:\n",
    "    #H3c1 = model.add_var(var_type = 'C', lb = 0, name = f'H3c1') # Holgura positiva\n",
    "    #H3c2 = model.add_var(var_type = 'C', lb = 0, name = f'H3c2') # Holgura negativa\n",
    "    df_R3c['H3c1'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3c1_{name}') for name in df_R3c['NAME'].values] # Holgura positiva\n",
    "    df_R3c['H3c2'] = [model.add_var(var_type = 'C', lb = 0, name = f'H3c2_{name}') for name in df_R3c['NAME'].values] # Holgura negativa\n",
    "    df_R3c['RHSU'] += df_R3c['H3c1']\n",
    "    df_R3c['RHSL'] -= df_R3c['H3c2']\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR3c\\n\\n')\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R3c))):\n",
    "    \n",
    "    name = df_R3c[\"NAME\"][i]\n",
    "    \n",
    "    R3c1 = (df_R3c['LHS'][i] <= df_R3c['RHSU'][i])\n",
    "    R3c2 = (df_R3c['LHS'][i] >= df_R3c['RHSL'][i])\n",
    "    \n",
    "    \"\"\"\n",
    "    if activar_holguras:\n",
    "        #R3c1 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) <= df_R3c['U'][i] * (1 - df_R3c['X_Y'][i])) + H3c1\n",
    "        #R3c2 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) >= df_R3c['L'][i] * (1 - df_R3c['X_Y'][i])) - H3c2\n",
    "        R3c1 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) <= df_R3c['U'][i] * (1 - df_R3c['X_Y'][i])) + df_R3c['H3c1'][i]\n",
    "        R3c2 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) >= df_R3c['L'][i] * (1 - df_R3c['X_Y'][i])) - df_R3c['H3c2'][i]\n",
    "    else:\n",
    "        R3c1 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) <= df_R3c['U'][i] * (1 - df_R3c['X_Y'][i]))\n",
    "        R3c2 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) >= df_R3c['L'][i] * (1 - df_R3c['X_Y'][i]))\n",
    "    \"\"\"\n",
    "    model += R3c1\n",
    "    model += R3c2\n",
    "\n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3c1({name}): {R3c1}  \\n')\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3c2({name}): {R3c2}  \\n')\n",
    "    \n",
    "\"\"\"  \n",
    "#df_R3c2: VP - [A + B*I] >= L * (1-y)\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR3c2\\n\\n')\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_R3c))):\n",
    "    \n",
    "    name = df_R3c[\"NAME\"][i]\n",
    "    if activar_holguras:\n",
    "        R3c2 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) >= df_R3c['L'][i] * (1 - df_R3c['X_Y'][i])) - H3c2\n",
    "    else:\n",
    "        R3c2 = (df_R3c['X_VP_in'][i] - (df_R3c['A'][i] * df_R3c['X_Inv'][i] + df_R3c['B'][i]) >= df_R3c['L'][i] * (1 - df_R3c['X_Y'][i]))\n",
    "    model += R3c2\n",
    "\n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R3c2({name}): {R3c2}  \\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R3c', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Suma de variables aditivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "simbolos = {'Venta': 'S', 'Ordenes': 'O', 'Visitas': 'V', 'Inversion': 'I'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Relación de Inv con Inv all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desarrollo actual (reemplazar bloque de arriba)\n",
    "df_inv_grano_0 = dic_variables['Inversion']['Inv']\n",
    "df_inv_grano_0 = df_inv_grano_0.rename(columns = {'X': 'X_Inv_0'})\n",
    "df_inv_all = dic_variables['Inversion']['Inv_all'].rename(columns = {'X': 'X_Inv_1'})\n",
    "df_inv_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for duplicacion in df_inv_all['DUPLICACION'].unique():\n",
    "    \n",
    "    #if duplicacion != \"F-LT\":\n",
    "    #    continue\n",
    "    print(duplicacion)\n",
    "    df_inv_all_duplicacion = df_inv_all[df_inv_all['DUPLICACION'] == duplicacion].reset_index(drop = True)\n",
    "\n",
    "    campos_dim = campos_beta_def[:]\n",
    "\n",
    "    for d in duplicacion.split('-'):\n",
    "        dim_name = diccionario_dimensiones[d]\n",
    "        if dim_name != \"\":\n",
    "            campos_dim += [dim_name]\n",
    "    \n",
    "    campos_metricas = ['X_Inv_1']\n",
    "    df_inv_all_duplicacion = df_inv_all_duplicacion[campos_dim + campos_metricas].groupby(campos_dim, as_index = False).sum().reset_index(drop = True)\n",
    "    \n",
    "    # # Lo de arriba es solo reducir campos..no debería \"agruparse\" realmente...el len no debería bajar...a diferencia de abajo, en la agrupación de df_inv_grano_0\n",
    "    \n",
    "    campos_metricas = ['X_Inv_0']\n",
    "    df_inv_grano_0_agr = df_inv_grano_0[['NAME'] + campos_dim + campos_metricas].groupby(campos_dim, as_index = False).sum().reset_index(drop = True)\n",
    "    \n",
    "    df_R = df_inv_grano_0_agr.merge(df_inv_all_duplicacion, on = campos_dim, how = 'left')\n",
    "    display(df_R.head())\n",
    "\n",
    "    escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR40: Relación de inversión grano con inversión x {duplicacion} \\n\\n')\n",
    "    \n",
    "    #if activar_holguras:\n",
    "    #    df_R[f'H4a0_p_{duplicacion}'] = [model.add_var(var_type = 'C', lb = 0, name = f'H4a0_p{name}') for name in df_R['NAME'].values]\n",
    "    #    df_R[f'H4a0_n_{duplicacion}'] = [model.add_var(var_type = 'C', lb = 0, name = f'H4a0_n{name}') for name in df_R['NAME'].values]\n",
    "        \n",
    "    for i in tqdm.tqdm(range(len(df_R))):\n",
    "        name = df_R[\"NAME\"][i]\n",
    "        #if activar_holguras:\n",
    "        #    R4a0 = (df_R[f'X_Inv_0'][i] == df_R[f'X_Inv_1'][i] + df_R['H4a0_p'][i] - df_R['H4a0_n'][i])\n",
    "        #else:\n",
    "        R4a0 = (df_R[f'X_Inv_0'][i] == df_R[f'X_Inv_1'][i])\n",
    "            \n",
    "        #R4a0 = (df_R[f'X_Inv_0'][i] == df_R[f'X_Inv_1'][i])\n",
    "        model += R4a0\n",
    "\n",
    "        if i < escribir_modelo[1]:\n",
    "            escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R4a0({name}): {R4a0}  \\n')\n",
    "        \n",
    "    # sys.exit('Seguir revisión')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Otras métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in simbolos:\n",
    "    if m == 'Inversion':\n",
    "        continue\n",
    "    print(m)\n",
    "    m_s = simbolos[m]\n",
    "    df_o = dic_variables[m][f'{m_s}O'].rename(columns = {'X': f'X_{m_s}O'}) # grano\n",
    "    #display(df_o.head())\n",
    "    df_p = dic_variables[m][f'{m_s}P'].rename(columns = {'X': f'X_{m_s}P'}) # grano\n",
    "    #display(df_p.head())\n",
    "    df_t = dic_variables[m][f'{m_s}T'].rename(columns = {'X': f'X_{m_s}T'}) # todos\n",
    "    #display(df_t.head())\n",
    "    \n",
    "    # Relación de grano con total\n",
    "    \n",
    "    escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR4 {m}\\n\\n')\n",
    "    \n",
    "    # df_R4a: Orgánico\n",
    "    df_R4a = df_o.merge(df_t[campos_beta_def + campos_grano + [f'X_{m_s}T']], on = campos_beta_def + campos_grano, how = 'left')\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(df_R4a))):\n",
    "        R4a = (df_R4a[f'X_{m_s}O'][i] == df_R4a[f'X_{m_s}T'][i])\n",
    "        model += R4a\n",
    "\n",
    "        if i < escribir_modelo[1]:\n",
    "            escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R4a({df_R4a[\"NAME\"][i]}): {R4a}  \\n')\n",
    "    \n",
    "    # df_R4b: Pago\n",
    "    df_R4b = df_p.merge(df_t[campos_beta_def + campos_grano + [f'X_{m_s}T']], on = campos_beta_def + campos_grano, how = 'left')\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(df_R4b))):\n",
    "        R4b = (df_R4b[f'X_{m_s}P'][i] == df_R4b[f'X_{m_s}T'][i])\n",
    "        model += R4b\n",
    "\n",
    "        if i < escribir_modelo[1]:\n",
    "            escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R4b({df_R4b[\"NAME\"][i]}): {R4b}  \\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R4', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Relación de niveles de duplicidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_duplicidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relaciones_duplicidad = dic_parametros['RELACIONES DUPLICIDAD']\n",
    "df_relaciones_duplicidad = df_relaciones_duplicidad[(df_relaciones_duplicidad['FAMILIA'].isin(list(familia_sm) + [''])) & (df_relaciones_duplicidad['TIPO_MEDIO'].isin(list(tm_seleccion) + ['']))]\n",
    "df_relaciones_duplicidad = df_relaciones_duplicidad[(df_relaciones_duplicidad['AGREGADO'].isin(lista_duplicidades)) & (df_relaciones_duplicidad['DESAGREGADO'].isin(lista_duplicidades))].reset_index(drop = True)\n",
    "df_relaciones_duplicidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_relaciones = {'Venta': 'VENTA_COLOCADA', 'Ordenes': 'ORDENES', 'Visitas': 'VISITAS', 'Inversion': 'INVERSION'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_dimensiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_relaciones_duplicidad['NAME'] = df_relaciones_duplicidad.index.astype(str)\n",
    "\n",
    "H5_p, H5_n = {}, {}\n",
    "\n",
    "for m in simbolos:\n",
    "    if not activar_holguras:\n",
    "        break\n",
    "    \n",
    "    df_relaciones_duplicidad[f'H5_p_{m}'] = [model.add_var(var_type = 'C', lb = 0, name = f'H5_p_{m}_{name}') for name in df_relaciones_duplicidad['NAME'].values] # Holgura positiva\n",
    "    df_relaciones_duplicidad[f'H5_n_{m}'] = [model.add_var(var_type = 'C', lb = 0, name = f'H5_n_{m}_{name}') for name in df_relaciones_duplicidad['NAME'].values] # Holgura negativa\n",
    "    \n",
    "    #H5_p[m] = model.add_var(var_type = 'C', lb = 0, name = f'H5_p_{m}')\n",
    "    #H5_n[m] = model.add_var(var_type = 'C', lb = 0, name = f'H5_n_{m}')\n",
    "\"\"\"                           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_beta_alpha_all = pd.concat([df_beta_alpha_grano_organico, df_beta_alpha_grano_pago], axis = 0).reset_index(drop = True)\n",
    "df_beta_alpha_all = df_beta_alpha_all[campos_beta_def + ['FAMILIA']].drop_duplicates().reset_index(drop = True)\n",
    "df_beta_alpha_all\n",
    "\n",
    "df_beta_alpha_all_totales = df_beta_alpha_all[campos_beta_def].drop_duplicates().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simbolos)\n",
    "print(dic_relaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relaciones_duplicidad['f_INVERSION'] = 1 # new 241226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relaciones_duplicidad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relaciones_duplicidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = 0\n",
    "# Suma de holguras\n",
    "conjunto_holguras_contraste = set()\n",
    "\n",
    "df_relaciones_duplicidad = df_beta_def.merge(df_relaciones_duplicidad, how = 'left', on = list(df_beta_def.columns)).reset_index(drop = True)\n",
    "\n",
    "for m in simbolos:\n",
    "    \n",
    "    m_s = simbolos[m]\n",
    "    metrica_name = dic_relaciones[m]\n",
    "    \n",
    "    if m == 'Inversion': # new 241226\n",
    "        df_m = dic_variables[m]['Inv_all'].rename(columns = {'X': f'X_{m_s}T'}) # todos\n",
    "    else:\n",
    "        df_m = dic_variables[m][f'{m_s}T'].rename(columns = {'X': f'X_{m_s}T'}) # todos\n",
    "    \n",
    "    for i in range(len(df_arcos)):\n",
    "        \n",
    "        agr, desagr = df_arcos['AGREGADO'][i], df_arcos['DESAGREGADO'][i]\n",
    "        \n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'\\nR5 {m} {agr} | {desagr}\\n\\n')\n",
    "        \n",
    "        campos_extra = []\n",
    "        for c in agr.split('-'):\n",
    "            if c == 'TOTALES':\n",
    "                continue\n",
    "            campos_extra.append(diccionario_dimensiones[c])\n",
    "            \n",
    "        print(m, m_s, metrica_name)\n",
    "        print(agr, desagr)\n",
    "\n",
    "        # Caso específico agr &  desagr\n",
    "        df_relaciones_duplicidad_i = df_relaciones_duplicidad[(df_relaciones_duplicidad['AGREGADO'] == agr) & (df_relaciones_duplicidad['DESAGREGADO'] == desagr)].reset_index(drop = True)\n",
    "\n",
    "        \"\"\" Se agregan tuplas (de campos_beta_def) que podrían no existir. Son necesarias para el cumplimiento correcto de las restricciones\"\"\"\n",
    "        if agr == 'TOTALES':\n",
    "            df_relaciones_duplicidad_i_tuplas = df_relaciones_duplicidad_i[campos_beta_def].drop_duplicates().reset_index(drop = True)\n",
    "            df_relaciones_duplicidad_i_tuplas['IN'] = True\n",
    "            \n",
    "            #display('A')\n",
    "            #display(df_relaciones_duplicidad_i_tuplas)\n",
    "            df_beta_alpha_all_totales_aux = df_beta_alpha_all_totales.copy()\n",
    "            df_beta_alpha_all_totales_aux = df_beta_alpha_all_totales_aux.merge(df_relaciones_duplicidad_i_tuplas, on = campos_beta_def, how = 'left')\n",
    "            \n",
    "            #display('B')\n",
    "            #display(df_beta_alpha_all_totales_aux)\n",
    "            \n",
    "            df_beta_alpha_all_totales_aux['IN'] = df_beta_alpha_all_totales_aux['IN'].fillna(False)\n",
    "            df_beta_alpha_all_totales_aux = df_beta_alpha_all_totales_aux[~df_beta_alpha_all_totales_aux['IN']].reset_index(drop = True)\n",
    "            df_beta_alpha_all_totales_aux = df_beta_alpha_all_totales_aux.drop(columns = ['IN'])\n",
    "            df_relaciones_duplicidad_i = pd.concat([df_relaciones_duplicidad_i, df_beta_alpha_all_totales_aux], axis = 0).reset_index(drop = True)\n",
    "            \n",
    "        else:\n",
    "            df_relaciones_duplicidad_i_tuplas = df_relaciones_duplicidad_i[campos_beta_def + ['FAMILIA']].drop_duplicates().reset_index(drop = True)\n",
    "            df_relaciones_duplicidad_i_tuplas['IN'] = True\n",
    "            df_beta_alpha_all_aux = df_beta_alpha_all.copy()\n",
    "            df_beta_alpha_all_aux = df_beta_alpha_all_aux.merge(df_relaciones_duplicidad_i_tuplas, on = campos_beta_def + ['FAMILIA'], how = 'left')\n",
    "            df_beta_alpha_all_aux['IN'] = df_beta_alpha_all_aux['IN'].fillna(False)\n",
    "            df_beta_alpha_all_aux = df_beta_alpha_all_aux[~df_beta_alpha_all_aux['IN']].reset_index(drop = True)\n",
    "            df_beta_alpha_all_aux = df_beta_alpha_all_aux.drop(columns = ['IN'])\n",
    "\n",
    "            df_relaciones_duplicidad_i = pd.concat([df_relaciones_duplicidad_i, df_beta_alpha_all_aux], axis = 0).reset_index(drop = True)\n",
    "\n",
    "        # Fillna con el valor que esá arriba\n",
    "        df_relaciones_duplicidad_i[['AGREGADO', 'DESAGREGADO'] + campos_grano] = df_relaciones_duplicidad_i[['AGREGADO', 'DESAGREGADO'] + campos_grano].fillna(method = 'ffill')\n",
    "        df_relaciones_duplicidad_i = df_relaciones_duplicidad_i.fillna(0)\n",
    "        \n",
    "        \"\"\" Se itera en los distintos casos\"\"\"\n",
    "        \n",
    "        if 'DATE' in campos_beta_def:\n",
    "            df_relaciones_duplicidad_i['DATE'], df_m['DATE'] = pd.to_datetime(df_relaciones_duplicidad_i['DATE']), pd.to_datetime(df_m['DATE'])\n",
    "        \n",
    "        df_agr = df_m[['DUPLICACION'] + campos_beta_def + campos_grano + [f'X_{m_s}T']].rename(columns = {'DUPLICACION': 'AGREGADO', f'X_{m_s}T': f'X_{m_s}T_AGR'})\n",
    "        df_agr = df_agr[df_agr['AGREGADO'] == agr]\n",
    "        \n",
    "        df_desagr = df_m[['DUPLICACION'] + campos_beta_def + campos_grano + [f'X_{m_s}T']].rename(columns = {'DUPLICACION': 'DESAGREGADO', f'X_{m_s}T': f'X_{m_s}T_DESAGR'})\n",
    "        df_desagr = df_desagr[df_desagr['DESAGREGADO'] == desagr]\n",
    "\n",
    "        if list(df_relaciones_duplicidad_i['FAMILIA'].unique()) == ['']:\n",
    "            df_desagr['FAMILIA'] =  ''\n",
    "\n",
    "        if list(df_relaciones_duplicidad_i[campo_last_touch].unique()) == ['']:\n",
    "            df_desagr[campo_last_touch] =  ''\n",
    "\n",
    "        #print('A')\n",
    "        #display(df_relaciones_duplicidad_i)\n",
    "        #display(df_agr)\n",
    "        \n",
    "        df_relaciones_duplicidad_i = df_relaciones_duplicidad_i.merge(df_agr, on = ['AGREGADO'] + campos_beta_def + campos_grano, how = 'left')\n",
    "        df_relaciones_duplicidad_i = df_relaciones_duplicidad_i.merge(df_desagr, on = ['DESAGREGADO'] + campos_beta_def + campos_grano, how = 'left')\n",
    "\n",
    "        #print('A0') # cambio 250116 (drop duplicates se hace abajo, post declarar name como str)\n",
    "        df_relaciones_duplicidad_i_dict_nombres = df_relaciones_duplicidad_i[[f'X_{m_s}T_AGR']]#.drop_duplicates()\n",
    "        #display(df_relaciones_duplicidad_i_dict_nombres)\n",
    "        df_relaciones_duplicidad_i_dict_nombres[f'X_{m_s}T_AGR_NAME'] = df_relaciones_duplicidad_i_dict_nombres[f'X_{m_s}T_AGR'].astype(str)\n",
    "        df_relaciones_duplicidad_i_dict_nombres = df_relaciones_duplicidad_i_dict_nombres.drop_duplicates().reset_index(drop = True)\n",
    "        #display(df_relaciones_duplicidad_i_dict_nombres)\n",
    "    \n",
    "        #display('A2')  \n",
    "        #display(df_relaciones_duplicidad_i)\n",
    "        \n",
    "        df_relaciones_duplicidad_i[f'X_{m_s}T_AGR_NAME'] = df_relaciones_duplicidad_i[f'X_{m_s}T_AGR'].astype(str)\n",
    "        #display('A2.5')  \n",
    "        #display(df_relaciones_duplicidad_i)\n",
    "        \n",
    "        df_relaciones_duplicidad_i = df_relaciones_duplicidad_i[['AGREGADO', 'DESAGREGADO'] + campos_beta_def + campos_grano + [f'f_{metrica_name}', f'X_{m_s}T_AGR_NAME', f'X_{m_s}T_DESAGR']].groupby(['AGREGADO', 'DESAGREGADO'] + campos_beta_def + campos_grano + [f'f_{metrica_name}', f'X_{m_s}T_AGR_NAME'], as_index = False).sum().reset_index(drop = True)\n",
    "        \n",
    "        #display('A2.7')  \n",
    "        #display(df_relaciones_duplicidad_i)\n",
    "        #display(df_relaciones_duplicidad_i_dict_nombres)\n",
    "        \n",
    "        df_relaciones_duplicidad_i = df_relaciones_duplicidad_i.merge(df_relaciones_duplicidad_i_dict_nombres, on = f'X_{m_s}T_AGR_NAME', how = 'left')\n",
    "        \n",
    "        #display('A3')  \n",
    "        #display(df_relaciones_duplicidad_i)\n",
    "        \n",
    "        df_relaciones_duplicidad_i['NAME'] = ''\n",
    "        for c in ['AGREGADO', 'DESAGREGADO'] + campos_beta_def + campos_grano:\n",
    "            df_relaciones_duplicidad_i['NAME'] += (df_relaciones_duplicidad_i[c].astype(str) + '_')\n",
    "        df_relaciones_duplicidad_i['NAME'] = df_relaciones_duplicidad_i['NAME'].str[:-1]\n",
    "        \n",
    "        if activar_holguras:\n",
    "            df_relaciones_duplicidad_i[f'H5_p_{m}'] = [model.add_var(var_type = 'C', lb = 0, name = f'H5_p_{m}_{name}') for name in df_relaciones_duplicidad_i['NAME'].values]\n",
    "            df_relaciones_duplicidad_i[f'H5_n_{m}'] = [model.add_var(var_type = 'C', lb = 0, name = f'H5_n_{m}_{name}') for name in df_relaciones_duplicidad_i['NAME'].values]\n",
    "            conjunto_holguras_contraste.add(f'H5_p')\n",
    "            conjunto_holguras_contraste.add(f'H5_n')\n",
    "        \n",
    "        #display('A4')  \n",
    "        #display(df_relaciones_duplicidad_i)\n",
    "        \n",
    "        #sys.exit()\n",
    "\n",
    "        for j in tqdm.tqdm(range(len(df_relaciones_duplicidad_i))):\n",
    "            \n",
    "            name = df_relaciones_duplicidad_i[f'X_{m_s}T_AGR_NAME'][j]\n",
    "            \n",
    "            #print(name, (f'f_{metrica_name}', j), round(df_relaciones_duplicidad_i[f'f_{metrica_name}'][j], 3))\n",
    "            #display(df_relaciones_duplicidad_i)\n",
    "            \n",
    "            df_relaciones_duplicidad_i\n",
    "            \n",
    "            \n",
    "            if activar_holguras:\n",
    "                #R5 = (df_relaciones_duplicidad_i[f'X_{m_s}T_AGR'][j] == df_relaciones_duplicidad_i[f'f_{metrica_name}'][j] * df_relaciones_duplicidad_i[f'X_{m_s}T_DESAGR'][j] + H5_p[m] - H5_n[m])\n",
    "                R5 = (df_relaciones_duplicidad_i[f'X_{m_s}T_AGR'][j] == round(df_relaciones_duplicidad_i[f'f_{metrica_name}'][j], 3) * df_relaciones_duplicidad_i[f'X_{m_s}T_DESAGR'][j] + df_relaciones_duplicidad_i[f'H5_p_{m}'][j] - df_relaciones_duplicidad_i[f'H5_n_{m}'][j])\n",
    "                H += df_relaciones_duplicidad_i[f'H5_p_{m}'][j] + df_relaciones_duplicidad_i[f'H5_n_{m}'][j]\n",
    "            else:\n",
    "                R5 = (df_relaciones_duplicidad_i[f'X_{m_s}T_AGR'][j] == round(df_relaciones_duplicidad_i[f'f_{metrica_name}'][j], 3) * df_relaciones_duplicidad_i[f'X_{m_s}T_DESAGR'][j])\n",
    "            \n",
    "            model += R5\n",
    "\n",
    "            if j < escribir_modelo[1]:\n",
    "                escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R5({name}): {R5}  \\n') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R5', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit('M R3c estaba en 4.24 mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Relación de Visitas & Visitas IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "campos_clacom = list(set(campos_grano) - {campo_last_touch})\n",
    "campos_clacom\n",
    "\n",
    "campos_clacom_inicial = []\n",
    "for c in campos_clacom:\n",
    "    campos_clacom_inicial.append(f'{c}_INICIAL')\n",
    "    \n",
    "campos_clacom_final = []\n",
    "for c in campos_clacom:\n",
    "    campos_clacom_final.append(f'{c}_FINAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visitas_pago = dic_variables['Visitas']['VP'][campos_beta_def + campos_grano  + ['X', 'NAME']].rename(columns = {'X': 'X_VP'})\n",
    "df_visitas_pago_in = dic_variables['Visitas']['VP_in'][campos_beta_def + campos_grano  + ['X']].rename(columns = {'X': 'X_VP_in'})\n",
    "df_relacion_visitas = dic_parametros['RELACION_VISITAS']\n",
    "\n",
    "\n",
    "df_visitas_pago_final = df_visitas_pago.copy()\n",
    "for c in campos_clacom:\n",
    "    df_visitas_pago_final = df_visitas_pago_final.rename(columns = {c: c + '_FINAL'})\n",
    "df_visitas_pago_final\n",
    "df_visitas_pago_final = df_visitas_pago_final.merge(df_relacion_visitas, on = campos_beta_def + campos_clacom_final + [campo_last_touch], how = 'left')\n",
    "\n",
    "# Los casos sin familia inicial, no están en df_relacion_visitas...es decir, no hay flujo a estas familias finales -> FLujo = 0\n",
    "\n",
    "df_visitas_pago_final[['FAMILIA_INICIAL', 'M']] = df_visitas_pago_final[['FAMILIA_INICIAL', 'M']].fillna(0)\n",
    "df_visitas_pago_final['M'] = round(df_visitas_pago_final['M'], 3)\n",
    "\n",
    "for c in campos_clacom:\n",
    "    df_visitas_pago_in = df_visitas_pago_in.rename(columns = {c: c + '_INICIAL'})\n",
    "    \n",
    "df_visitas_pago_final = df_visitas_pago_final.merge(df_visitas_pago_in, on = campos_beta_def + campos_clacom_inicial + [campo_last_touch], how = 'left')\n",
    "\n",
    "df_X_VP_name = df_visitas_pago_final[['X_VP']] # 250116\n",
    "df_X_VP_name['X_VP_NAME'] = df_X_VP_name['X_VP'].astype(str)\n",
    "df_X_VP_name = df_X_VP_name.drop_duplicates()\n",
    "\n",
    "df_visitas_pago_final['X_VP_NAME'] = df_visitas_pago_final['X_VP'].astype(str)\n",
    "df_visitas_pago_final['M_X_VP_in'] = df_visitas_pago_final['M'] * df_visitas_pago_final['X_VP_in']\n",
    "df_visitas_pago_final = df_visitas_pago_final[campos_beta_def + campos_clacom_final + [campo_last_touch] + ['X_VP_NAME', 'M_X_VP_in']].groupby(campos_beta_def + campos_clacom_final + [campo_last_touch] + ['X_VP_NAME'], as_index = False).sum().reset_index(drop = True)\n",
    "df_visitas_pago_final = df_visitas_pago_final.merge(df_X_VP_name, on = 'X_VP_NAME', how = 'left')\n",
    "\n",
    "display(df_visitas_pago_final)\n",
    "\n",
    "df_visitas_pago_final['NAME'] = ''\n",
    "for c in campos_beta_def + campos_clacom_final + [campo_last_touch]:\n",
    "    df_visitas_pago_final['NAME'] += (df_visitas_pago_final[c].astype(str) + '_')\n",
    "df_visitas_pago_final['NAME'] = df_visitas_pago_final['NAME'].str[:-1]\n",
    "\n",
    "if activar_holguras:\n",
    "    #H6_p = model.add_var(var_type = 'C', lb = 0, name = f'H6_p') # Holgura positiva\n",
    "    #H6_n = model.add_var(var_type = 'C', lb = 0, name = f'H6_n') # Holgura neg\n",
    "    \n",
    "    df_visitas_pago_final['H6_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H6_p_{name}') for name in df_visitas_pago_final['NAME'].values] # Holgura positiva\n",
    "    df_visitas_pago_final['H6_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H6_n_{name}') for name in df_visitas_pago_final['NAME'].values] # Holgura negativa\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_visitas_pago_final))):\n",
    "    \n",
    "    name = df_visitas_pago_final['X_VP_NAME'][i]\n",
    "    \n",
    "    if activar_holguras:\n",
    "        #R6 = (df_visitas_pago_final['X_VP'][i] == df_visitas_pago_final['M_X_VP_in'][i] + H6_p - H6_n)\n",
    "        R6 = (df_visitas_pago_final['X_VP'][i] == df_visitas_pago_final['M_X_VP_in'][i] + df_visitas_pago_final['H6_p'][i] - df_visitas_pago_final['H6_n'][i])\n",
    "    else:\n",
    "        R6 = (df_visitas_pago_final['X_VP'][i] == df_visitas_pago_final['M_X_VP_in'][i])\n",
    "    model += R6\n",
    "    \n",
    "    if i <= escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R6({name}): {R6}  \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R6', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Relación de venta neta y colocada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_neta_colocada = dic_parametros['METAS']['RATIO NETA COLOCADA']\n",
    "ratio_neta_colocada.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R7 = dic_variables['Venta']['ST'].rename(columns = {'X': 'X_ST'})\n",
    "df_venta_neta = dic_variables['Venta']['ST_net'][['NAME', 'X']].rename(columns = {'X': 'X_ST_net'})\n",
    "\n",
    "df_R7 = df_R7.merge(df_venta_neta, on = 'NAME', how = 'left')\n",
    "\n",
    "if not modo_tactico:\n",
    "    df_R7['PERIODO'] = df_R7['DATE'].astype(str).str[:7]\n",
    "\n",
    "df_R7 = df_R7.merge(ratio_neta_colocada, on = ['PAIS', 'PERIODO', 'CANAL', 'FUENTE'], how = 'left')\n",
    "\n",
    "if activar_holguras:\n",
    "    H7_p = model.add_var(var_type = 'C', lb = 0, name = 'H7_p') # Holgura positiva\n",
    "    H7_n = model.add_var(var_type = 'C', lb = 0, name = 'H7_n') # Holgura negativa\n",
    "    \n",
    "    df_R7['H7_p'] = [model.add_var(var_type = 'C', lb = 0, name = f'H7_p_{name}') for name in df_R7['NAME'].values] # Holgura positiva\n",
    "    df_R7['H7_n'] = [model.add_var(var_type = 'C', lb = 0, name = f'H7_n_{name}') for name in df_R7['NAME'].values] # Holgura negativa\n",
    "\n",
    "df_R7['RATIO_NETA_COLOCADA'] = round(df_R7['RATIO_NETA_COLOCADA'], 3)\n",
    "\n",
    "#H2 = 0\n",
    "for i in tqdm.tqdm(range(len(df_R7))):\n",
    "    \n",
    "    name = df_R7[\"NAME\"][i]\n",
    "    \n",
    "    if activar_holguras:\n",
    "        R7 = (df_R7['X_ST'][i]  * df_R7['RATIO_NETA_COLOCADA'][i] == df_R7['X_ST_net'][i] + H7_p - H7_n)\n",
    "        R7 = (df_R7['X_ST'][i]  * df_R7['RATIO_NETA_COLOCADA'][i] == df_R7['X_ST_net'][i] + df_R7['H7_p'][i] - df_R7['H7_n'][i])\n",
    "    else:\n",
    "        R7 = (df_R7['X_ST'][i]  * df_R7['RATIO_NETA_COLOCADA'][i] == df_R7['X_ST_net'][i])\n",
    "    #R7 = (df_R7['X_ST'][i]  * df_R7['RATIO_NETA_COLOCADA'][i] == df_R7['X_ST_net'][i])\n",
    "    model += R7\n",
    "    #H2 += (df_R7['H7_p'][i] + df_R7['H7_n'][i])\n",
    "    \n",
    "    if i < escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R7({name}): {R7}  \\n')\n",
    "\n",
    "# Este bloque, antes de H2, estaba en 1.20 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R7', df_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Relaciones de inversión "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ?? Esto ya está cubierto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Sensibilizables y cumplimiento de metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros['METAS'].keys()\n",
    "\n",
    "# Esto es todo lo que está disponible para sensibilizar en GSheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restricciones = df_escenarios[df_escenarios['TIPO'] == 'R'].reset_index(drop = True)\n",
    "df_restricciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def depuracion_temporal(filtro, c):\n",
    "    if c == 'PERIODO':\n",
    "        periodo0, periodo1 = filtro[0].split('>')\n",
    "        date0 = pd.to_datetime(periodo0)\n",
    "        date1 = pd.to_datetime(periodo1)\n",
    "\n",
    "        df_dates = pd.DataFrame(pd.date_range(date0, date1), columns = ['DATE'])\n",
    "        df_dates['PERIODO'] = df_dates['DATE'].astype(str).str[:7]\n",
    "        return list(df_dates['PERIODO'].unique())\n",
    "\n",
    "    elif c == 'DATE':\n",
    "        date0, date1 = filtro[0].split('>')\n",
    "        date0 = pd.to_datetime(date0)\n",
    "        date1 = pd.to_datetime(date1)\n",
    "        \n",
    "        df_dates = pd.DataFrame(pd.date_range(date0, date1), columns = ['DATE'])\n",
    "        return list(df_dates['DATE'].unique())\n",
    "\n",
    "    elif c == 'AÑO':\n",
    "        año0, año1 = filtro[0].split('>')\n",
    "        año0 = int(año0)\n",
    "        año1 = int(año1)\n",
    "        \n",
    "        return list(range(año0, año1 + 1))\n",
    "\n",
    "    else:\n",
    "        return filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_restricciones)):\n",
    "    \n",
    "    if version_simplificada:\n",
    "        break\n",
    "    \n",
    "    #break\n",
    "\n",
    "    print('Si o si estas restrs tienen que ir con holguras. Aunque no exista ninguna otra holgura en el modelo')\n",
    "    \n",
    "    #print('ELIMINAR (2)')\n",
    "    \n",
    "    #if i > 4:\n",
    "    #    break\n",
    "    \n",
    "    #if i <= 4: # Hasta aquí están ok revisados\n",
    "    #    continue\n",
    "    \n",
    "    df_i = df_restricciones.iloc[i:i + 1].reset_index(drop = True)\n",
    "    \n",
    "    eliminar_cols = ['ESCENARIO', 'Descripción / Comentarios (Opcional)', 'TIPO']\n",
    "    for c in df_i.columns:\n",
    "        if df_i[c][0] == '':\n",
    "            eliminar_cols.append(c)\n",
    "    \n",
    "    descripcion = df_i['Descripción / Comentarios (Opcional)'][0]\n",
    "    \n",
    "    df_i = df_i.drop(columns = eliminar_cols)\n",
    "    \n",
    "    print(f'\\n\\n\\n\\n\\n i: {i}: {descripcion}')\n",
    "    display(df_i)\n",
    "    \n",
    "    # Primero: Determinar en que nivel de dupicidad / duplicación, aplicar la restricción\n",
    "    \n",
    "    lista_duplicacion = []\n",
    "    if 'Base + Config_FAMILIA' in df_i.columns:\n",
    "        lista_duplicacion.append('F') \n",
    "    if 'Base + Config_SUBFAMILIA' in df_i.columns:\n",
    "        lista_duplicacion.append('SF')\n",
    "    if f'Base + Config_{campo_last_touch}' in df_i.columns:\n",
    "        lista_duplicacion.append('LT')\n",
    "    \n",
    "    if lista_duplicacion == []:\n",
    "        duplicacion = 'TOTALES'\n",
    "    else:\n",
    "        duplicacion = '-'.join(lista_duplicacion)\n",
    "    \n",
    "    # Selección de variable para la restricción (Métrica)\n",
    "    \n",
    "    metrica = df_i['Condiciones de restricción_METRICA'][0]\n",
    "    ratio = False\n",
    "    \n",
    "    if metrica == 'NMV':\n",
    "        df_base = dic_variables['Venta']['ST_net']\n",
    "        df_X = df_base[df_base['DUPLICACION'] == duplicacion].reset_index(drop = True)\n",
    "        df_P_Base = dic_parametros['METAS']['VENTA_NETA_BASE']\n",
    "        df_P = dic_parametros['METAS']['VENTA_NETA']\n",
    "        metrica_name = 'VENTA_NETA'\n",
    "    elif metrica == 'GMV':\n",
    "        df_base = dic_variables['Venta']['ST']\n",
    "        df_X = df_base[df_base['DUPLICACION'] == duplicacion].reset_index(drop = True)\n",
    "        df_P = dic_parametros['METAS']['VENTA_COLOCADA']\n",
    "        metrica_name = 'VENTA_COLOCADA'\n",
    "    elif metrica == 'O':\n",
    "        df_base = dic_variables['Ordenes']['OT']\n",
    "        df_X = df_base[df_base['DUPLICACION'] == duplicacion].reset_index(drop = True)\n",
    "        df_P = dic_parametros['METAS']['ORDENES']\n",
    "        metrica_name = 'ORDENES'\n",
    "    elif metrica == 'V':\n",
    "        df_base = dic_variables['Visitas']['VT']\n",
    "        df_X = df_base[df_base['DUPLICACION'] == duplicacion].reset_index(drop = True)\n",
    "        df_P = dic_parametros['METAS']['VISITAS']\n",
    "        metrica_name = 'VISITAS'\n",
    "    elif metrica == 'TC':\n",
    "        df_base_ordenes = dic_variables['Ordenes']['OT']\n",
    "        df_base_ordenes = df_base_ordenes[df_base_ordenes['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_OT'})\n",
    "        df_base_visitas = dic_variables['Visitas']['VT']\n",
    "        df_base_visitas = df_base_visitas[df_base_visitas['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_VT'})\n",
    "        cols_merge = list(set(df_base_ordenes.columns) - {'NAME', 'X_OT'})\n",
    "        df_X = df_base_ordenes[cols_merge + ['X_OT']].merge(df_base_visitas[cols_merge + ['X_VT']], on = cols_merge, how = 'left')\n",
    "        \n",
    "        # Para ratios\n",
    "        df_ordenes = dic_parametros['METAS']['ORDENES']\n",
    "        df_visitas = dic_parametros['METAS']['VISITAS']\n",
    "        df_P = df_ordenes.merge(df_visitas, on = ['PAIS', 'PERIODO', 'CANAL_BASE'], how = 'left')\n",
    "        \n",
    "        metricas_aux = [['X_OT', 'X_VT'], ['ORDENES', 'VISITAS']]\n",
    "        metrica_name = 'TC'\n",
    "        ratio = True\n",
    "\n",
    "    elif metrica == 'I':\n",
    "        df_base = dic_variables['Inversion']['Inv_all']\n",
    "        df_X = df_base[df_base['DUPLICACION'] == duplicacion].reset_index(drop = True)\n",
    "        df_P = dic_parametros['METAS']['INVERSION']\n",
    "        metrica_name = 'INVERSION'\n",
    "        \n",
    "    elif metrica == 'TP':\n",
    "        df_base_venta_colocada = dic_variables['Venta']['ST']\n",
    "        df_base_venta_colocada = df_base_venta_colocada[df_base_venta_colocada['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_ST'})\n",
    "        df_base_ordenes = dic_variables['Ordenes']['OT']\n",
    "        df_base_ordenes = df_base_ordenes[df_base_ordenes['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_OT'})\n",
    "        cols_merge = list(set(df_base_venta_colocada.columns) - {'NAME', 'X_ST'})\n",
    "        df_X = df_base_venta_colocada[cols_merge + ['X_ST']].merge(df_base_ordenes[cols_merge + ['X_OT']], on = cols_merge, how = 'left')\n",
    "        df_P = dic_parametros['METAS']['TP']\n",
    "        metrica_name = 'TP'\n",
    "        sys.exit('Ver en TC, para ratios (# Para ratios) y replicar')\n",
    "        \n",
    "    elif metrica == 'CV':\n",
    "        df_base_inversion = dic_variables['Inversion']['Inv_all']\n",
    "        df_base_inversion = df_base_inversion[df_base_inversion['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_INV'})\n",
    "        df_base_venta_colocada = dic_variables['Venta']['ST']\n",
    "        df_base_venta_colocada = df_base_venta_colocada[df_base_venta_colocada['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_ST'})\n",
    "        cols_merge = list(set(df_base_inversion.columns) - {'NAME', 'X_INV'})\n",
    "        df_X = df_base_inversion[cols_merge + ['X_INV']].merge(df_base_venta_colocada[cols_merge + ['X_ST']], on = cols_merge, how = 'left')\n",
    "        df_P_VC = dic_parametros['METAS']['VENTA_COLOCADA']\n",
    "        df_P_VC = df_P_VC[['PAIS', 'PERIODO', 'VENTA_COLOCADA']].groupby(['PAIS', 'PERIODO'], as_index = False).sum()\n",
    "        df_P = df_P_VC.merge(dic_parametros['METAS']['INVERSION'], on = ['PAIS', 'PERIODO'], how = 'left')\n",
    "        df_P['CV'] = df_P['INVERSION'] / df_P['VENTA_COLOCADA']\n",
    "        df_P = df_P[['PAIS', 'PERIODO', 'CV']]\n",
    "        metrica_name = 'CV'\n",
    "        sys.exit('Ver en TC, para ratios (# Para ratios) y replicar')\n",
    "    elif metrica == 'CVN':\n",
    "        df_base_inversion = dic_variables['Inversion']['Inv_all']\n",
    "        df_base_inversion = df_base_inversion[df_base_inversion['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_INV'})\n",
    "        df_base_venta_neta = dic_variables['Venta']['ST_net']\n",
    "        df_base_venta_neta = df_base_venta_neta[df_base_venta_neta['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_ST_net'})\n",
    "        cols_merge = list(set(df_base_inversion.columns) - {'NAME', 'X_INV'})\n",
    "        df_X = df_base_inversion[cols_merge + ['X_INV']].merge(df_base_venta_neta[cols_merge + ['X_ST_net']], on = cols_merge, how = 'left')\n",
    "        df_P_VN = dic_parametros['METAS']['VENTA_NETA']\n",
    "        df_P_VN = df_P_VN[['PAIS', 'PERIODO', 'VENTA_NETA']].groupby(['PAIS', 'PERIODO'], as_index = False).sum()\n",
    "        df_P = df_P_VN.merge(dic_parametros['METAS']['INVERSION'], on = ['PAIS', 'PERIODO'], how = 'left')\n",
    "        df_P['CVN'] = df_P['INVERSION'] / df_P['VENTA_NETA']\n",
    "        df_P = df_P[['PAIS', 'PERIODO', 'CVN']]\n",
    "        metrica_name = 'CVN'\n",
    "        sys.exit('Ver en TC, para ratios (# Para ratios) y replicar')\n",
    "    elif metrica == 'CVis':\n",
    "        df_base_inversion = dic_variables['Inversion']['Inv_all']\n",
    "        df_base_inversion = df_base_inversion[df_base_inversion['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_INV'})\n",
    "        df_base_visitas = dic_variables['Visitas']['VT']\n",
    "        df_base_visitas = df_base_visitas[df_base_visitas['DUPLICACION'] == duplicacion].reset_index(drop = True).rename(columns = {'X': 'X_VT'})\n",
    "        cols_merge = list(set(df_base_inversion.columns) - {'NAME', 'X_INV'})\n",
    "        df_X = df_base_inversion[cols_merge + ['X_INV']].merge(df_base_visitas[cols_merge + ['X_VT']], on = cols_merge, how = 'left')\n",
    "        df_P_Vis = dic_parametros['METAS']['VISITAS']\n",
    "        df_P_Vis = df_P_Vis[['PAIS', 'PERIODO', 'VISITAS']].groupby(['PAIS', 'PERIODO'], as_index = False).sum()\n",
    "        df_P = df_P_Vis.merge(dic_parametros['METAS']['INVERSION'], on = ['PAIS', 'PERIODO'], how = 'left')\n",
    "        df_P['CVis'] = df_P['INVERSION'] / df_P['VISITAS']\n",
    "        df_P = df_P[['PAIS', 'PERIODO', 'CVis']]\n",
    "        metrica_name = 'CVis'\n",
    "        sys.exit('Ver en TC, para ratios (# Para ratios) y replicar')\n",
    "    else:\n",
    "        sys.exit(f'Metrica {metrica} no configurada')\n",
    "        \n",
    "    relacion = df_i['Condiciones de restricción_RELACION'][0]\n",
    "    contraste = df_i['Condiciones de restricción_CONTRASTE'][0]\n",
    "    para_cada = df_i['Condiciones de restricción_PARA_CADA'][0].split(',')\n",
    "    cond_share = df_i['Condiciones de restricción_SHARE'][0]\n",
    "    \n",
    "    ponderador_contraste = 1 # Por defecto\n",
    "    if 'Condiciones de restricción_PONDERADOR_CONTRASTE' in df_i.columns:\n",
    "        ponderador_contraste = float(df_i['Condiciones de restricción_PONDERADOR_CONTRASTE'][0])\n",
    "    \n",
    "    # if metrica = NMV, también puede influir df_P_Base\n",
    "    if ('F' in duplicacion) or (metrica != 'NMV'):\n",
    "        None\n",
    "    else:\n",
    "        df_P = df_P_Base.copy()\n",
    "        \n",
    "    df_X['CANAL_BASE'] = np.where(df_X['FUENTE'] == 'SODIMAC', df_X['CANAL'], 'FCOM')\n",
    "    \n",
    "    # Aquí deberían ir los filtros en casos puntuales\n",
    "    # ANCLA 0\n",
    "    campos_particion = ['PAIS', 'PERIODO', 'CANAL', 'FUENTE', 'FAMILIA', 'SUBFAMILIA', campo_last_touch, 'DATE', 'CANAL_BASE', 'NATURALEZA_MEDIO', 'AÑO']\n",
    "    \n",
    "    if 'PERIODO' in para_cada:\n",
    "        if ('DATE' in df_X.columns) and not ('PERIODO' in df_X.columns):\n",
    "            df_X['PERIODO'] = df_X['DATE'].astype(str).str[:7]\n",
    "        # lo mismo para df_P\n",
    "        if ('DATE' in df_P.columns) and not ('PERIODO' in df_P.columns):\n",
    "            df_P['PERIODO'] = df_P['DATE'].astype(str).str[:7]\n",
    "    \n",
    "    if 'AÑO' in para_cada:\n",
    "        if ('DATE' in df_X.columns) and not ('AÑO' in df_X.columns):\n",
    "            df_X['AÑO'] = df_X['DATE'].astype(str).str[:4].astype(int)\n",
    "        # lo mismo para df_P\n",
    "        if ('DATE' in df_P.columns) and not ('AÑO' in df_P.columns):\n",
    "            df_P['AÑO'] = df_P['DATE'].astype(str).str[:4].astype(int)\n",
    "\n",
    "    existen_filtros = False\n",
    "    for c in campos_particion:\n",
    "        if (f'Base + Config_{c}' in df_i.columns) or (f'Aux_{c}' in df_i.columns):\n",
    "            existen_filtros = True\n",
    "            break\n",
    "\n",
    "    if existen_filtros:\n",
    "        if cond_share == 'TRUE':\n",
    "\n",
    "            df_X_L, df_X_R = df_X.copy(), df_X.copy() #Left y Right\n",
    "            df_X_L = df_X_L.rename(columns = {'X': 'X_L'})\n",
    "            df_X_R = df_X_R.rename(columns = {'X': 'X_R'})\n",
    "            for c in campos_particion:\n",
    "                if (f'Base + Config_{c}' in df_i.columns):\n",
    "                    filtro = df_i[f'Base + Config_{c}'][0].split(',')\n",
    "                    filtro = depuracion_temporal(filtro, c)\n",
    "                    df_X_L = df_X_L[df_X_L[c].isin(filtro)]\n",
    "                    df_i = df_i.drop(columns = [f'Base + Config_{c}'])\n",
    "                if (f'Aux_{c}' in df_i.columns):\n",
    "                    filtro = df_i[f'Aux_{c}'][0].split(',')\n",
    "                    filtro = depuracion_temporal(filtro, c)\n",
    "                    df_X_R = df_X_R[df_X_R[c].isin(filtro)]\n",
    "                    df_i = df_i.drop(columns = [f'Aux_{c}'])\n",
    "        else:\n",
    "            for c in campos_particion:\n",
    "                if (f'Base + Config_{c}' in df_i.columns):\n",
    "                    filtro = df_i[f'Base + Config_{c}'][0].split(',')\n",
    "                    filtro = depuracion_temporal(filtro, c)\n",
    "                    df_X = df_X[df_X[c].isin(filtro)]\n",
    "                    df_P = df_P[df_P[c].isin(filtro)]\n",
    "                    df_i = df_i.drop(columns = [f'Base + Config_{c}'])\n",
    "\n",
    "\n",
    "    # Creación del campo periodo si es necesario\n",
    "    if ('PERIODO' in para_cada) and (cond_share == 'TRUE') and ('DATE' in df_X_L.columns) and ('DATE' in df_X_R.columns):\n",
    "        df_X_L['PERIODO'] = df_X_L['DATE'].astype(str).str[:7]\n",
    "        df_X_R['PERIODO'] = df_X_R['DATE'].astype(str).str[:7]\n",
    "    \n",
    "    if ('AÑO' in para_cada) and (cond_share == 'TRUE') and ('DATE' in df_X_L.columns) and ('DATE' in df_X_R.columns):\n",
    "        df_X_L['AÑO'] = df_X_L['DATE'].astype(str).str[:4].astype(int)\n",
    "        df_X_R['AÑO'] = df_X_R['DATE'].astype(str).str[:4].astype(int)\n",
    "        \n",
    "    # Si contraste == M, entonces se usa df_P, si no, no\n",
    "    if contraste == 'M':\n",
    "        if cond_share == 'TRUE':\n",
    "            sys.exit('No se acepta esta combinación')\n",
    "\n",
    "        #display('df_X')\n",
    "        #display(df_X)\n",
    "        #display('df_P')\n",
    "        #display(df_P)\n",
    "        \n",
    "        if ratio:\n",
    "            df_X = df_X[para_cada + metricas_aux[0]].groupby(para_cada, as_index = False).sum()\n",
    "            df_P = df_P[para_cada + metricas_aux[1]].groupby(para_cada, as_index = False).sum()\n",
    "            #df_X['X'] = df_X[metricas_aux[0][0]] / df_X[metricas_aux[0][1]]\n",
    "            df_P[metrica_name] = df_P[metricas_aux[1][0]] / df_P[metricas_aux[1][1]]\n",
    "            #df_X = df_X.drop(columns = metricas_aux[0])\n",
    "            df_P = df_P.drop(columns = metricas_aux[1])\n",
    "        else:\n",
    "            df_X = df_X[para_cada + ['X']].groupby(para_cada, as_index = False).sum()\n",
    "            df_P = df_P[para_cada + [metrica_name]].groupby(para_cada, as_index = False).sum()\n",
    "\n",
    "        \n",
    "        #display('df_X')\n",
    "        #display(df_X)\n",
    "        #display('df_P')\n",
    "        #display(df_P)\n",
    "            \n",
    "        df_R = df_X.merge(df_P, on = para_cada, how = 'left').reset_index(drop = True)\n",
    "\n",
    "        #display('df_R')\n",
    "        #display(df_R)\n",
    "\n",
    "        #if para_cada != ['PERIODO']:\n",
    "        #    sys.exit(f'Revisar bien aqui {duplicacion}')\n",
    "        \n",
    "        #if metrica == 'TC':\n",
    "        #    sys.exit('dfP debe estar construido desde ordenes y visitas')\n",
    "        \n",
    "    if cond_share == 'TRUE':\n",
    "        df_X_L = df_X_L[para_cada + ['X_L']].groupby(para_cada, as_index = False).sum()\n",
    "        df_X_R = df_X_R[para_cada + ['X_R']].groupby(para_cada, as_index = False).sum()\n",
    "        df_R = df_X_L.merge(df_X_R, on = para_cada, how = 'left').reset_index(drop = True)\n",
    "        \n",
    "        \n",
    "    df_i = df_i.drop(columns = ['Condiciones de restricción_METRICA', 'Condiciones de restricción_RELACION', 'Condiciones de restricción_CONTRASTE', 'Condiciones de restricción_PARA_CADA', 'Condiciones de restricción_SHARE'])\n",
    "    if 'Condiciones de restricción_PONDERADOR_CONTRASTE' in df_i.columns:\n",
    "        df_i = df_i.drop(columns = ['Condiciones de restricción_PONDERADOR_CONTRASTE'])\n",
    "        \n",
    "    print(duplicacion, metrica, relacion, contraste, para_cada, cond_share, metrica_name, ponderador_contraste)\n",
    "    \n",
    "    #display('df_i')\n",
    "    #display(df_i)\n",
    "    \n",
    "    # Restricción\n",
    "\n",
    "    #display(df_R)\n",
    "    \n",
    "    if ratio:\n",
    "        df_R['X_R'] = df_R[metrica_name] * df_R[metricas_aux[0][0]]\n",
    "        df_R['X_L'] = df_R[metricas_aux[0][1]]\n",
    "    \n",
    "    x_l, x_r = 'X', metrica_name\n",
    "    if contraste != 'M':\n",
    "        ponderador_contraste *= float(contraste) # Para casos shares por ejem\n",
    "        x_l, x_r = 'X_L', 'X_R'\n",
    "    if ratio:\n",
    "        x_l, x_r = 'X_L', 'X_R'\n",
    "    \n",
    "    #print(x_l, x_r)\n",
    "    #display(df_R)\n",
    "    \n",
    "    print('PC2', ponderador_contraste, type(ponderador_contraste ))\n",
    "    #sys.exit()\n",
    "\n",
    "    \n",
    "    for j in range(len(df_R)):\n",
    "        if relacion == '==':\n",
    "            R = (df_R[x_l][j] == ponderador_contraste * df_R[x_r][j])\n",
    "        elif relacion == '>=':\n",
    "            R = (df_R[x_l][j] >= ponderador_contraste * df_R[x_r][j])\n",
    "        elif relacion == '<=':\n",
    "            R = (df_R[x_l][j] <= ponderador_contraste * df_R[x_r][j])\n",
    "        else:\n",
    "            sys.exit(f'Relación {relacion} no configurada') \n",
    "\n",
    "        model += R\n",
    "        \n",
    "        print(str(R))\n",
    "        \n",
    "        if j < escribir_modelo[1]:\n",
    "            escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R8({i}-{j}): {R}  \\n')\n",
    "\n",
    "    #if i == 1:\n",
    "    #    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_configuracion = df_escenarios[df_escenarios['TIPO'] == 'C'].reset_index(drop = True)\n",
    "df_configuracion\n",
    "\n",
    "k = 1\n",
    "# Temporalidad por defecto: Año actual + k Próximos años (Se puede cambiar)\n",
    "dias_proyeccion = [dt.datetime.today().replace(year = dt.datetime.today().year, month = 1, day = 1).date(), dt.datetime.today().replace(year = dt.datetime.today().year + k, month = 12, day = 31).date()] # declarar arriba, al inicio del código\n",
    "#dias_proyeccion = [pd.to_datetime(dias_proyeccion[0]), pd.to_datetime(dias_proyeccion[1])]\n",
    "\n",
    "paises = ['MX', 'CL', 'UY', 'CO', 'BR', 'AR', 'PE'] # Por defecto\n",
    "familias_ids = ['*'] # Todo\n",
    "last_touch_lista = ['*'] # Todo\n",
    "lista_fuentes, lista_canales = ['SODIMAC', 'ES', 'SIS'], ['WEB', 'APP']\n",
    "\n",
    "for c in df_configuracion.columns:\n",
    "    if 'Base + Config' in c:\n",
    "        name = '_'.join(c.split('_')[1:])\n",
    "        value = df_configuracion[c][0]\n",
    "        if name == 'MODO_PROYECCION': # Modo proyección\n",
    "            modo_proyeccion = value\n",
    "            if modo_proyeccion not in ['PRESUPUESTO', 'LIBRE']:\n",
    "                sys.exit(f'MODO_PROYECCION {modo_proyeccion} no es correcto')\n",
    "        \n",
    "        if (name == 'PAIS') and (value != ''):\n",
    "            paises = value.split(',')\n",
    "                        \n",
    "        if (name == 'DATE') and (value != ''):\n",
    "            dias_proyeccion_new = pd.to_datetime(value.split('>')[0]).date(), pd.to_datetime(value.split('>')[1]).date()\n",
    "            dias_proyeccion[0] = max(dias_proyeccion[0], dias_proyeccion_new[0])\n",
    "            dias_proyeccion[1] = min(dias_proyeccion[1], dias_proyeccion_new[1])\n",
    "        \n",
    "        if (name == 'PERIODO') and (value != ''):\n",
    "            periodo_0, periodo_1 = value.split('>')\n",
    "            dia0 = dt.datetime(int(periodo_0.split('-')[0]), int(periodo_0.split('-')[1]), 1).date()\n",
    "            dia1 = dt.datetime(int(periodo_1.split('-')[0]), int(periodo_1.split('-')[1]), 1).date()\n",
    "            dia1 = (dt.datetime(dia1.year, dia1.month, 1) + pd.DateOffset(months = 1) - pd.DateOffset(days = 1)).date()\n",
    "            dias_proyeccion[0] = max(dias_proyeccion[0], dia0)\n",
    "            dias_proyeccion[1] = min(dias_proyeccion[1], dia1)\n",
    "        \n",
    "        if (name == 'AÑO') and (value != ''):\n",
    "            año_0, año_1 = value.split('>')\n",
    "            dia0 = dt.datetime(int(año_0), 1, 1).date()\n",
    "            dia1 = dt.datetime(int(año_1), 12, 31).date()\n",
    "            dias_proyeccion[0] = max(dias_proyeccion[0], dia0)\n",
    "            dias_proyeccion[1] = min(dias_proyeccion[1], dia1)\n",
    "        \n",
    "        if (name == 'FAMILIA') and (value != ''):\n",
    "            familias_lista = value.split(',')\n",
    "            familias_ids = [int(f) for f in familias_lista]\n",
    "        \n",
    "        if (name == campo_last_touch) and (value != ''):\n",
    "            last_touch_lista = value.split(',')\n",
    "        \n",
    "        if (name == 'FUENTE') and (value != ''):\n",
    "            lista_fuentes = value.split(',')\n",
    "        \n",
    "        if (name == 'CANAL') and (value != ''):\n",
    "            lista_canales = value\n",
    "        \n",
    "        if (name == 'CANAL_BASE') and (value != ''):\n",
    "            sys.exit('CANAL_BASE en configuración debe ir vacío')\n",
    "            \n",
    "\n",
    "dias_proyeccion = [pd.to_datetime(dias_proyeccion[0]), pd.to_datetime(dias_proyeccion[1])]\n",
    "dias_proyeccion\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit('Continuar aqui con restricciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tiempo hasta ahora [MIN]')\n",
    "print((time.time() - t_0) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M R8', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raux = (VN_USD >= 4000)\n",
    "#model += Raux\n",
    "\n",
    "#escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'Raux: {Raux}  \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_relaciones_duplicidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de variables\n",
    "str(list(model.vars)[0])\n",
    "conjunto_holguras = set()\n",
    "\n",
    "lista_vars = list(model.vars)\n",
    "for i in lista_vars:\n",
    "    #print(str(i))\n",
    "    if str(i)[:1] == 'H':\n",
    "        a, b = str(i).split('_')[0], str(i).split('_')[1]\n",
    "        if b in ['p', 'n']:\n",
    "            k = f'{a}_{b}'\n",
    "        else:\n",
    "            k = a\n",
    "        \n",
    "        conjunto_holguras.add(k)\n",
    "    \n",
    "conjunto_holguras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if activar_holguras:\n",
    "    \"\"\"\n",
    "    H = 0\n",
    "    \n",
    "    df_R1a['H'] = df_R1a['H1a_p'] + df_R1a['H1a_n']\n",
    "    for i in tqdm.tqdm(range(len(df_R1a))):\n",
    "        H += df_R1a['H'][i]\n",
    "    \n",
    "    df_R1b['H'] = df_R1b['H1b_p'] + df_R1b['H1b_n']\n",
    "    for i in tqdm.tqdm(range(len(df_R1b))):\n",
    "        H += df_R1b['H'][i]\n",
    "    \n",
    "    df_R1c['H'] = df_R1c['H1c_p'] + df_R1c['H1c_n']\n",
    "    for i in tqdm.tqdm(range(len(df_R1c))):\n",
    "        H += df_R1c['H'][i]\n",
    "    \n",
    "    df_R2a['H'] = df_R2a['H2a_p'] + df_R2a['H2a_n']\n",
    "    for i in tqdm.tqdm(range(len(df_R2a))):\n",
    "        H += df_R2a['H'][i]\n",
    "    \n",
    "    df_R2b['H'] = df_R2b['H2b_p'] + df_R2b['H2b_n']\n",
    "    for i in tqdm.tqdm(range(len(df_R2b))):\n",
    "        H += df_R2b['H'][i]\n",
    "    \n",
    "    df_R3b['H'] = df_R3b['H3b1'] + df_R3b['H3b2']\n",
    "    for i in tqdm.tqdm(range(len(df_R3b))):\n",
    "        H += df_R3b['H'][i]\n",
    "    \n",
    "    df_relaciones_duplicidad['H'] = df_relaciones_duplicidad['H5_p_Venta'] + df_relaciones_duplicidad['H5_n_Venta'] + df_relaciones_duplicidad['H5_p_Ordenes'] + df_relaciones_duplicidad['H5_n_Ordenes'] + df_relaciones_duplicidad['H5_p_Visitas'] + df_relaciones_duplicidad['H5_n_Visitas']\n",
    "    for i in tqdm.tqdm(range(len(df_relaciones_duplicidad))):\n",
    "        H += df_relaciones_duplicidad['H'][i]\n",
    "    \n",
    "    \n",
    "    df_visitas_pago['H'] = df_visitas_pago['H6_p'] + df_visitas_pago['H6_n']\n",
    "    for i in tqdm.tqdm(range(len(df_visitas_pago))):\n",
    "        H += df_visitas_pago['H'][i]\n",
    "    \n",
    "    df_R7['H'] = df_R7['H7_p'] + df_R7['H7_n']\n",
    "    for i in tqdm.tqdm(range(len(df_R7))):\n",
    "        H += df_R7['H'][i]\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    #H = 0\n",
    "\n",
    "    print('A')\n",
    "    t00 = time.time()\n",
    "    for c in ['H1a_p', 'H1a_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R1a[c])\n",
    "\n",
    "    print('B', time.time() - t00, 2 * len(df_R1b))\n",
    "    for c in ['H1b_p', 'H1b_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R1b[c])\n",
    "        \n",
    "    print('C', time.time() - t00, 2 * len(df_R1c))\n",
    "    for c in ['H1c_p', 'H1c_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R1c[c])\n",
    "\n",
    "    print('D', time.time() - t00, 2 * len(df_R2a))\n",
    "    for c in ['H2a_p', 'H2a_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R2a[c])\n",
    "\n",
    "    print('E', time.time() - t00, 2 * len(df_R2b))\n",
    "    for c in ['H2b_p', 'H2b_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R2b[c])\n",
    "        \n",
    "    print('E2', time.time() - t00, 2 * len(df_y))\n",
    "    for c in ['H3a_p', 'H3a_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_y[c])\n",
    "\n",
    "    #H += H2\n",
    "    print('F', time.time() - t00, 2 * len(df_R3b))\n",
    "    #sys.exit('Ver tiempos cuando se genera H2')\n",
    "    for c in ['H3b1', 'H3b2']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R3b[c])\n",
    "        \n",
    "    print('F2', time.time() - t00, 2 * len(df_R3b3_agr))\n",
    "    for c in ['H3b3_p', 'H3b3_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R3b3_agr[c])\n",
    "        \n",
    "    print('F3', time.time() - t00, 2 * len(df_R3c))\n",
    "    for c in ['H3c1', 'H3c2']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R3c[c])\n",
    "\n",
    "    # H5 se declara todo arriba\n",
    "    #for c in ['H5_p_Venta', 'H5_n_Venta', 'H5_p_Ordenes', 'H5_n_Ordenes', 'H5_p_Visitas', 'H5_n_Visitas']:\n",
    "    #    conjunto_holguras_contraste.add(c)\n",
    "    #    H += sum(df_relaciones_duplicidad[c])\n",
    "    \n",
    "    print('G', time.time() - t00, 2 * len(df_visitas_pago_final))\n",
    "    for c in ['H6_p', 'H6_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_visitas_pago_final[c])\n",
    "        \n",
    "    print('H', time.time() - t00, 2 * len(df_R7))\n",
    "    for c in ['H7_p', 'H7_n']:\n",
    "        conjunto_holguras_contraste.add(c)\n",
    "        H += sum(df_R7[c])\n",
    "        \n",
    "    #H = H1a_p + H1a_n + H1b_p + H1b_n + H1c_p + H1c_n + H2a_p + H2a_n + H2b_p + H2b_n + H3b1 + H3b2 + H6_p + H6_n + H7_p + H7_n\n",
    "    \n",
    "    #H = H1a_p + H1a_n + H1b_p + H1b_n + H1c_p + H1c_n + H2a_p + H2a_n + H2b_p + H2b_n + H3b1 + H3b2 + H6_p + H6_n + H7_p + H7_n\n",
    "    #for m in ['Venta', 'Ordenes', 'Visitas']:\n",
    "    #    H += H5_p[m] + H5_n[m]\n",
    "\n",
    "    Z = VN_USD - INV_USD - (10 ** 10) * H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Holguras que se crean pero que no se penalizan')\n",
    "print(conjunto_holguras - conjunto_holguras_contraste)\n",
    "\n",
    "print('Holguras que se penalizan pero que no se crean')\n",
    "print(conjunto_holguras_contraste - conjunto_holguras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.objective = mip.maximize(Z)\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo, archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'Función Objetivo \\n')\n",
    "escribir_modelo_opt(activar = escribir_modelo, archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'----------------  \\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "escribir_modelo_opt(activar = escribir_modelo, archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'Z: {Z}  \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RS = (S == 0)\n",
    "model += RS\n",
    "\n",
    "escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'RS: {RS}  \\n')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tiempo hasta ahora [MIN]')\n",
    "print((time.time() - t_0) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.vars), len(model.constrs) #cantidad de variables y restricciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.constrs) # Cantidad de restricciones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'SOL Prev', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.exit('Medir memorias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Obtener todas las variables almacenadas en la sesión\n",
    "def get_memory_usage():\n",
    "    # Diccionario para almacenar el nombre de la variable y su tamaño\n",
    "    memory_data = []\n",
    "    for var_name, var_value in globals().items():\n",
    "        try:\n",
    "            # Calcula el tamaño en bytes\n",
    "            size_bytes = sys.getsizeof(var_value)\n",
    "            memory_data.append({'Variable': var_name, 'Size (MB)': size_bytes / (1024**2)})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Crear un DataFrame ordenado por tamaño\n",
    "    df_memory = pd.DataFrame(memory_data).sort_values(by = 'Size (MB)', ascending = False).reset_index(drop=True)\n",
    "    return df_memory\n",
    "\n",
    "if False:\n",
    "    df_memory = get_memory_usage()\n",
    "    display(df_memory) #['Size (MB)'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tiempo hast ahora esde inicio (mins)')\n",
    "round((time.time() - t_inicio_0) / 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "ejecutar_modelo_relajado = True\n",
    "resistencia = 0.01 # Parámetro\n",
    "optimo_encontrado = False\n",
    "previo = 0\n",
    "\n",
    "if ejecutar_modelo_relajado:\n",
    "    for k in range(30): # Fix-and-Optimize\n",
    "        print(f'Ejecución modelo relajado {k + 1}')\n",
    "        #status = model.optimize(max_seconds = 300, relax = True) # Primero prueba factibilidad\n",
    "        status = model.optimize(relax = True) # Primero prueba factibilidad\n",
    "\n",
    "        print('Relaxed')\n",
    "        display(status)\n",
    "        display(model.objective_value)\n",
    "\n",
    "        print(f'Tiempo en modelo relajado [MIN]: {(time.time() - t1) / 60}')\n",
    "        df_times, t0 = medir_tiempo(t0, 'SOL Relaxed', df_times)\n",
    "        \n",
    "        if k == 0:\n",
    "            df_Y_aux = dic_variables['Inversion']['Y'].copy()\n",
    "            df_Y_aux['NUEVO'] = False\n",
    "            \n",
    "        df_Y_aux['Y'] = [df_Y_aux['X'][i].x for i in range(len(df_Y_aux))] \n",
    "        df_Y_aux['Y'] = np.where(df_Y_aux['Y'] < resistencia, 0, df_Y_aux['Y'])\n",
    "        df_Y_aux['Y'] = np.where(df_Y_aux['Y'] > 1 - resistencia, 1, df_Y_aux['Y'])\n",
    "        \n",
    "        df_Y_aux['BINARIOS'] = np.where(df_Y_aux['Y'].isin([0, 1]), 1, 0)\n",
    "        df_Y_aux['NUEVO'] = np.where((df_Y_aux['Y'].isin([0, 1])) & (df_Y_aux['NUEVO'] == False), True, False)\n",
    "        \n",
    "        proporcion = df_Y_aux['BINARIOS'].sum() / len(df_Y_aux)\n",
    "        print(f'Proporcion: {100 * proporcion} %') # Proporcion de casos binarios\n",
    "        \n",
    "        if proporcion == previo:\n",
    "            print('Ninguna variable relajada cumple las condiciones')\n",
    "            break\n",
    "        \n",
    "        if proporcion == 1:\n",
    "            optimo_encontrado = True\n",
    "            print('Optimo encontrado')\n",
    "            break\n",
    "        \n",
    "        previo = proporcion\n",
    "        \n",
    "        df_bins = df_Y_aux[(df_Y_aux['BINARIOS'] == 1) & (df_Y_aux['NUEVO'])].reset_index(drop = True)\n",
    "        \n",
    "        # Agregar las restricciones\n",
    "        for i in range(len(df_bins)):\n",
    "            R = (df_bins['X'][i] == df_bins['Y'][i])\n",
    "            model += R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit('Revisar en txt por que está unbounded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "if optimo_encontrado:\n",
    "    sys.exit('Óptimo encontrado en Fix & Optimize. Ver soluciones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "model.max_gap = 0.1 # 0.01 revienta...estuvo unas 48 hrs intentando resolver mx\n",
    "status = model.optimize() #(max_seconds = 40 * 60) # Luego MIP\n",
    "\n",
    "print(f'Tiempo en modelo MIP [MIN]: {(time.time() - t1) / 60}')\n",
    "print('MIP')\n",
    "display(status)\n",
    "display(model.objective_value)\n",
    "df_times, t0 = medir_tiempo(t0, 'SOL MIP', df_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3204,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write('model.lp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VN_USD.x, INV_USD.x, INV_USD.x / VN_USD.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tiempo hasta ahora [MIN]')\n",
    "print((time.time() - t_0) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_times, t0 = medir_tiempo(t0, 'M Ejecución', df_times)\n",
    "df_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit('Otros de aqui hacia abajo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_parametros.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dic_parametros['RELACION_VISITAS']\n",
    "df.M.min(), df.M.max()\n",
    "df[df['M'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mip import Model, BINARY\n",
    "\n",
    "# Crear un modelo\n",
    "model = Model()\n",
    "\n",
    "# Definir variables\n",
    "x = model.add_var(name=\"x\", var_type=BINARY)\n",
    "y = model.add_var(name=\"y\", var_type=BINARY)\n",
    "\n",
    "# Agregar restricciones\n",
    "model += x + y <= float('inf'), \"Restriccion_1\"\n",
    "model += x - y >= 0, \"Restriccion_2\"\n",
    "\n",
    "# Listar restricciones\n",
    "print(\"Lista de restricciones:\")\n",
    "for constr in model.constrs:\n",
    "    print(f\"Nombre: {constr.name}, Expresión: {constr.expr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if status == mip.OptimizationStatus.OPTIMAL:\n",
    "    print('optimal solution cost {} found'.format(model.objective_value))\n",
    "elif status == mip.OptimizationStatus.FEASIBLE:\n",
    "    print('sol.cost {} found, best possible: {}'.format(model.objective_value, model.objective_bound))\n",
    "elif status == mip.OptimizationStatus.NO_SOLUTION_FOUND:\n",
    "    print('no feasible solution found, lower bound is: {}'.format(model.objective_bound))\n",
    "if status == mip.OptimizationStatus.OPTIMAL or status == mip.OptimizationStatus.FEASIBLE:\n",
    "    print('solution:')\n",
    "    for v in model.vars:\n",
    "        if abs(v.x) > 1e-6: # only printing non-zeros\n",
    "            #if 'H' == v.name[:1]:\n",
    "            print('{} : {}'.format(v.name, v.x))\n",
    "            #if ('Inv' in v.name) and not ('Inv_rango' in v.name):\n",
    "            #    print('{} : {}'.format(v.name, v.x))\n",
    "            #if 'Y_' in v.name:\n",
    "            #    print('{} : {}'.format(v.name, v.x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit('Solo revs abajo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R1c.FAMILIA.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfk = df_R1c[df_R1c.FAMILIA == familia_sm].reset_index(drop = True)\n",
    "dfk['X'] = [dfk['X'][i].x for i in range(len(dfk))]\n",
    "#dfk['X'].sum()\n",
    "dfk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R3c[df_R3c.FAMILIA == \"17 - Muebles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit('Abajo solo revisiones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(df_R3b['X_Inv'][i] >= df_R3b['MIN_VIS_IN'][i] * df_R3b['X_Y'][i] - df_R3b['H3b1'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = df_R3b[df_R3b['FAMILIA'] == '25 - Promociones - Soporte Tecnico']\n",
    "dfr = dfr[dfr['TIPO_MEDIO'] == 'SEM - Non Brand']\n",
    "dfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit('Continuar acá')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "[Extender a otras restricciones > Ocupar en R5] R3b3 Tiene un enfoque con un groupby sum que puede bajar bastante los tiempos en la creación de otras restricciones\n",
    "\n",
    "Variables sensibilizables: Restricciones R8 asociada a metas\n",
    "\n",
    "Integrar variable de inversión para distintos niveles de duplicidad (ya creada)\n",
    "contar pars, vars y restricciones: len(model.vars), len(model.constrs)\n",
    "Guardar modelo base con pkl\n",
    "Weighted RLM que permita estimar el tiempo de generación & resolución de un modelo según el tamaño de los conjuntos (cant de días, de paises, etc)\n",
    "\n",
    "Variables pasadas (hasta ayer) deben tomar si o si los números reales (Vent, Ord, Vis, Inv)\n",
    "\n",
    "[OK] Variables de Holgura?\n",
    "\n",
    "# En MIP: Infactible + otras restricciones puede ser factible (infactible a veces puede ser no acotado)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit('Falta restricción asociada a:')\n",
    "dic_variables['Inversion']['Inv_all']#.DUPLICACION.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit('Continuar restricciones acá')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \"\"\"\n",
    "    Antigua parte final de R5 (lenta)\n",
    "    \n",
    "    for j in tqdm.tqdm(range(len(df_relaciones_duplicidad_i))):\n",
    "        factor = df_relaciones_duplicidad_i[f'f_{metrica_name}'][j]\n",
    "        \n",
    "        df_relaciones_duplicidad_i_j = df_relaciones_duplicidad_i.iloc[j: j + 1]\n",
    "        df_relaciones_duplicidad_i_j['IN'] = True\n",
    "        \n",
    "        if activar_holguras:\n",
    "            hgp, hgn = df_relaciones_duplicidad_i_j[f'H5_p_{m}'][j], df_relaciones_duplicidad_i_j[f'H5_n_{m}'][j]\n",
    "        df_relaciones_duplicidad_i_j = df_relaciones_duplicidad_i_j[campos_beta + campos_extra + ['IN']]\n",
    "        #display(df_relaciones_duplicidad_i_j)\n",
    "        \n",
    "        display(df_m)\n",
    "        sys.exit()\n",
    "        LHS = df_m[df_m['DUPLICACION'] == agr].reset_index(drop = True)\n",
    "        LHS = LHS.merge(df_relaciones_duplicidad_i_j, on = campos_beta + campos_extra, how = 'left')\n",
    "        LHS['IN'] = LHS['IN'].fillna(False)\n",
    "        LHS = LHS[LHS['IN']].reset_index(drop = True)\n",
    "        \n",
    "        RHS = df_m[df_m['DUPLICACION'] == desagr].reset_index(drop = True)\n",
    "        RHS = RHS.merge(df_relaciones_duplicidad_i_j, on = campos_beta + campos_extra, how = 'left')\n",
    "        RHS['IN'] = RHS['IN'].fillna(False)\n",
    "        RHS = RHS[RHS['IN']].reset_index(drop = True)\n",
    "        \n",
    "        name = m_s + '_' + agr + '_' + desagr + '_' + '_'.join(LHS['NAME'][0].split('_')[1:])\n",
    "        \n",
    "        #display(LHS)\n",
    "        #display(RHS)\n",
    "        \n",
    "        #print(factor)\n",
    "        #print(name)\n",
    "        \n",
    "        rhs = 0\n",
    "        for k in range(len(RHS)):\n",
    "            rhs += RHS[f'X_{m_s}T'][k]\n",
    "        \n",
    "        if activar_holguras:\n",
    "            R5 = (LHS[f'X_{m_s}T'][0] == factor * rhs + hgp - hgn)\n",
    "        else:\n",
    "            R5 = (LHS[f'X_{m_s}T'][0] == factor * rhs)\n",
    "            \n",
    "        model += R5\n",
    "\n",
    "        if j < escribir_modelo[1]:\n",
    "            escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R5({name}): {R5}  \\n')\n",
    "    \"\"\"\n",
    "#sys.exit('Solo revision')\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Antigua parte final de R6\n",
    "    \n",
    "    \n",
    "    # ELIMINAR BLOQUE\n",
    "\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df_visitas_pago))):\n",
    "    df_visitas_pago_i = df_visitas_pago.iloc[i: i + 1]\n",
    "    df_visitas_pago_i_tupla = df_visitas_pago_i[campos_beta + campos_grano]\n",
    "    \n",
    "    name = df_visitas_pago_i[\"NAME\"][i]\n",
    "    \n",
    "    for c in campos_clacom:\n",
    "        df_visitas_pago_i_tupla = df_visitas_pago_i_tupla.rename(columns = {c: c + '_FINAL'})\n",
    "    df_visitas_pago_i_tupla['IN'] = True\n",
    "    \n",
    "    display(df_visitas_pago_i_tupla)\n",
    "    sys.exit()\n",
    "    #print('A')\n",
    "    #display(df_visitas_pago_i)\n",
    "    \n",
    "    df_relacion_visitas_i = df_relacion_visitas.merge(df_visitas_pago_i_tupla, on = campos_beta + campos_clacom_final + [campo_last_touch], how = 'left')\n",
    "    df_relacion_visitas_i['IN'] = df_relacion_visitas_i['IN'].fillna(False)\n",
    "    df_relacion_visitas_i = df_relacion_visitas_i[df_relacion_visitas_i['IN']].reset_index(drop = True)\n",
    "    #display(df_relacion_visitas_i)\n",
    "    \n",
    "    df_relacion_visitas_i_tupla = df_relacion_visitas_i[campos_beta + campos_clacom_inicial + [campo_last_touch] + ['M', 'IN']]\n",
    "\n",
    "    for c in campos_clacom:\n",
    "        df_relacion_visitas_i_tupla = df_relacion_visitas_i_tupla.rename(columns = {c + '_INICIAL': c})\n",
    "    \n",
    "    df_visitas_pago_in_i = df_visitas_pago_in.merge(df_relacion_visitas_i_tupla, on = campos_beta + campos_grano, how = 'left')\n",
    "    df_visitas_pago_in_i['IN'] = df_visitas_pago_in_i['IN'].fillna(False)\n",
    "    df_visitas_pago_in_i = df_visitas_pago_in_i[df_visitas_pago_in_i['IN']].reset_index(drop = True)\n",
    "\n",
    "    df_visitas_pago_in_i['M_V_in'] = df_visitas_pago_in_i['M'] * df_visitas_pago_in_i['X_VP_in']\n",
    "    S = df_visitas_pago_in_i['M_V_in'].sum()\n",
    "    \n",
    "    if activar_holguras:\n",
    "        R6 = (df_visitas_pago_i['X_VP'][i] == S + df_visitas_pago_i['H6_p'][i] - df_visitas_pago_i['H6_n'][i])\n",
    "    else:\n",
    "        R6 = (df_visitas_pago_i['X_VP'][i] == S)\n",
    "    model += R6\n",
    "    \n",
    "    if i <= escribir_modelo[1]:\n",
    "        escribir_modelo_opt(activar = escribir_modelo[0], archivo = f'Modelo de optimizacion.txt', modo = 'a', linea = f'R6({name}): {R6}  \\n')\n",
    "        \n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy import Symbol, integrate #symbols, Q, assume, integrate\n",
    "\n",
    "def solucion_segmento_representativo():\n",
    "    \n",
    "    def func(D, n, i):\n",
    "        # D = a + c * t\n",
    "        return D * (i ** n)\n",
    "\n",
    "\n",
    "    i, A, B, i_0, i_f, D = sp.symbols('i A B i_0 i_f D')\n",
    "    n = Symbol('n', positive = True, real = True) \n",
    "\n",
    "    # D = a + c * t\n",
    "    # define la funcion\n",
    "    f = D * (i ** n) \n",
    "    g = A * i + B\n",
    "\n",
    "    # Calcula la integral de (f(i) - g(i)) ** 2\n",
    "\n",
    "    integral = integrate((f - g) ** 2, (i, i_0, i_f))\n",
    "    integral.simplify()\n",
    "    \n",
    "    # derivar la integral respecto a A y a B\n",
    "    derivativeA = sp.diff(integral, A)\n",
    "    derivativeB = sp.diff(integral, B)\n",
    "\n",
    "    # Resuelve el sistema de ecuaciones\n",
    "    solution = sp.solve([derivativeA, derivativeB], (A, B))\n",
    "    \n",
    "    return solution\n",
    "\n",
    "def solucion_segmento_representativo_subs(solution, i_0v, i_fv, Dv, nv ):\n",
    "\n",
    "    i, A, B, i_0, i_f, D = sp.symbols('i A B i_0 i_f D')\n",
    "    n = Symbol('n', positive = True, real = True) \n",
    "    \n",
    "    A_ = solution[A].subs({i_0: i_0v, i_f: i_fv, D: Dv, n: nv})\n",
    "    B_ = solution[B].subs({i_0: i_0v, i_f: i_fv, D: Dv, n: nv})\n",
    "    \n",
    "    return A_, B_\n",
    "\n",
    "dict_valores = {'i_0': 20, 'i_f': 40, 'D': 4, 'n': 0.5}\n",
    "solution = solucion_segmento_representativo()\n",
    "A, B = solucion_segmento_representativo_subs(solution, dict_valores)\n",
    "A, B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knapsack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mip import Model, xsum, BINARY, MAXIMIZE, INTEGER\n",
    "\n",
    "# Datos\n",
    "values = [10, 15, 7]\n",
    "weights = [2, 3, 1]\n",
    "capacity = 4\n",
    "\n",
    "# Modelo\n",
    "model = Model(\"Mochila\", sense = MAXIMIZE) # Para maximizar FO\n",
    "\n",
    "# Variables binarias\n",
    "x = [model.add_var(var_type=BINARY) for _ in range(len(values))]\n",
    "\n",
    "# Restricción: Capacidad de la mochila\n",
    "model += xsum(weights[i] * x[i] for i in range(len(values))) <= capacity\n",
    "\n",
    "# Función objetivo: Maximizar el valor total\n",
    "model.objective = xsum(values[i] * x[i] for i in range(len(values)))\n",
    "\n",
    "# Resolver\n",
    "model.optimize()\n",
    "\n",
    "# Resultados\n",
    "print(\"Elementos seleccionados:\")\n",
    "for i in range(len(values)):\n",
    "    if x[i].x >= 0.99:  # Ajuste para precisión numérica\n",
    "        print(f\" - Objeto {i+1} (Valor: {values[i]}, Peso: {weights[i]})\")\n",
    "print(f\"Valor total: {model.objective_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema de la Dieta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mip import Model, xsum, CONTINUOUS\n",
    "\n",
    "# Datos\n",
    "costs = [2, 3]\n",
    "protein = [3, 2]\n",
    "fat = [2, 4]\n",
    "req_protein = 10\n",
    "req_fat = 18\n",
    "\n",
    "# Modelo\n",
    "model = Model(\"Dieta\")\n",
    "\n",
    "# Variables continuas (cantidades de alimentos)\n",
    "x = [model.add_var(var_type=CONTINUOUS) for _ in range(len(costs))]\n",
    "\n",
    "# Restricciones nutricionales\n",
    "model += xsum(protein[i] * x[i] for i in range(len(costs))) >= req_protein\n",
    "model += xsum(fat[i] * x[i] for i in range(len(costs))) >= req_fat\n",
    "\n",
    "# Función objetivo: Minimizar el costo\n",
    "model.objective = xsum(costs[i] * x[i] for i in range(len(costs)))\n",
    "\n",
    "# Resolver\n",
    "model.optimize()\n",
    "\n",
    "# Resultados\n",
    "print(\"Cantidades óptimas de alimentos:\")\n",
    "for i in range(len(costs)):\n",
    "    print(f\" - Alimento {chr(65+i)}: {x[i].x:.2f} unidades\")\n",
    "print(f\"Costo total: {model.objective_value:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
